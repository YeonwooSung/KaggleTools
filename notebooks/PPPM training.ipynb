{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4wtXQc5eILks"},"outputs":[],"source":["# !pip install kaggle\n","# from google.colab import files\n","# files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ybT-kPrIRRF"},"outputs":[],"source":["# !ls -1ha kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21153,"status":"ok","timestamp":1653102353129,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"ngruL3LzILgG","outputId":"5f7cb9c3-586a-4631-f42d-43152431fff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-pSk1HrImpC"},"outputs":[],"source":["# !mkdir -p ~/.kaggle\n","# !cp kaggle.json ~/.kaggle/\n","# # Permission Warning 이 일어나지 않도록 \n","# !chmod 600 ~/.kaggle/kaggle.json\n","# # 본인이 참가한 모든 대회 보기 \n","# !kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8081,"status":"ok","timestamp":1653102361209,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"93TLgBfpILdp","outputId":"2eb9b8fc-5795-456c-84b3-8d8bfd906e1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tokenizers\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 4.1 MB/s \n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 78.9 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 79.7 MB/s \n","\u001b[?25hRequirement already satisfied: six\u003e=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Collecting sentry-sdk\u003e=1.0.0\n","  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 85.4 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil\u003e=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting GitPython\u003e=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 89.0 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Collecting shortuuid\u003e=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: promise\u003c3,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: protobuf\u003e=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: Click!=8.0.0,\u003e=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: requests\u003c3,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting docker-pycreds\u003e=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: psutil\u003e=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython\u003e=1.0.0-\u003ewandb) (4.2.0)\n","Collecting gitdb\u003c5,\u003e=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hCollecting smmap\u003c6,\u003e=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2021.10.8)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.0.0-\u003ewandb) (3.0.4)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8ee19231907bab4c179b90d02dfb48d5ee825b60af5b05220753638faf836955\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, tokenizers, sentencepiece\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentencepiece-0.1.96 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 tokenizers-0.12.1 wandb-0.12.16\n"]}],"source":["!pip3 install tokenizers wandb sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6457,"status":"ok","timestamp":1653102367659,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"dTDhaP31LevK","outputId":"8906c77d-d6de-49ba-dac2-c1526a6621d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.9 MB/s \n","\u001b[?25hCollecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.8.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Installing collected packages: pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 transformers-4.19.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABCV5MzcILYt"},"outputs":[],"source":["import os\n","os.chdir(\"drive/\")\n","os.chdir('My Drive')\n","os.chdir('Kaggle')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe14zhChIld4"},"outputs":[],"source":["# !kaggle competitions download -c us-patent-phrase-to-phrase-matching\n","# !unzip us-patent-phrase-to-phrase-matching.zip\n","# !ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enX70dDOIlbH"},"outputs":[],"source":["# debert_v3_tokenizer_path = 'deberta-v2-v3-fast-tokenizer'\n","# %env TOKENIZERS_PARALLELISM=true\n","\n","# import shutil\n","# from pathlib import Path\n","\n","# transformers_path = Path('/usr/local/lib/python3.7/dist-packages/transformers')\n","# input_dir = Path('./deberta-v2-v3-fast-tokenizer')\n","\n","# convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","# conversion_path = transformers_path / convert_file.name\n","\n","# if conversion_path.exists():\n","#     conversion_path.unlink()\n","\n","# shutil.copy(convert_file, transformers_path)\n","# deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","# for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","#     filepath = deberta_v2_path/filename\n","    \n","#     if filepath.exists():\n","#         filepath.unlink()\n","#     shutil.copy(input_dir/filename, filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8uVyeNzILWZ"},"outputs":[],"source":["OUTPUT_DIR = './pppm-deberta-v3-outputs/'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1653102367660,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"B9XJVEp-ILTm","outputId":"04162e17-f5b3-4cae-d143-1a9d82b3f651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat May 21 03:06:06 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') \u003e= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad4bqKUJILRr"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    wandb=True\n","    competition='PPPM'\n","    _wandb_kernel='bluehills'\n","    debug=False\n","    apex=True\n","    print_freq=100\n","    num_workers=4\n","    model=\"microsoft/deberta-v3-large\"\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=5\n","    encoder_lr=1.5e-5 #2e-5\n","    decoder_lr=1.5e-5 #2e-5\n","    min_lr=5e-7\n","    eps=5e-7\n","    betas=(0.9, 0.999)\n","    batch_size=14\n","    fc_dropout=0.1\n","    target_size=1\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    train_all_index=25\n","    n_fold=20\n","    trn_fold=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 ,15, 16, 17, 18, 19]\n","    train=True\n","    \n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"elapsed":10608,"status":"ok","timestamp":1653102378264,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"qW8but_GILO1","outputId":"1afd4e45-72b1-454e-b923-906236b1a1d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["login to wandb\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) =\u003e {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() =\u003e {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() =\u003e reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data =\u003e {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbluehills\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.16"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/drive/My Drive/Kaggle/wandb/run-20220521_030613-ffslevnp\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/bluehills/PPPM-Training/runs/ffslevnp\" target=\"_blank\"\u003emicrosoft/deberta-v3-large\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/bluehills/PPPM-Training\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","    import wandb\n","    try:\n","        # from kaggle_secrets import UserSecretsClient\n","        # user_secrets = UserSecretsClient()\n","        # secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        # wandb.login(key=secret_value_0)\n","        print('login to wandb')\n","        wandb.login()\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W\u0026B account, go to Add-ons -\u003e Secrets and provide your W\u0026B access token. Use the Label name as wandb_api. \\nGet your W\u0026B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","    run = wandb.init(project='PPPM-Training', \n","                     name=CFG.model,\n","                     config=class2dict(CFG),\n","                     group=CFG.model,\n","                     job_type=\"train\",\n","                     anonymous=anony)"]},{"cell_type":"markdown","metadata":{"id":"g_kIjiCGLHsk"},"source":["# Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6308,"status":"ok","timestamp":1653102384565,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"F3Ud6NtXILMj","outputId":"a6b8a118-1645-4b24-928c-d9ba27e5d13f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.__version__: 1.11.0+cu113\n","tokenizers.__version__: 0.12.1\n","transformers.__version__: 4.19.2\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import shutil\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedGroupKFold\n","\n","import torch\n","print(f\"torch.__version__: {torch.__version__}\")\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","import torch.cuda.amp as amp\n","\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n","\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"0N-UkOUGLMTx"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO_u0OIhILJo"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def get_score(y_true, y_pred):\n","    score = sp.stats.pearsonr(y_true, y_pred)[0]\n","    return score\n","\n","\n","def get_logger(filename=OUTPUT_DIR+'train'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{"id":"6epV68-8Lrk7"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"executionInfo":{"elapsed":1919,"status":"ok","timestamp":1653102386481,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"-15eijrbILHp","outputId":"9b2a2e3e-8b97-44ae-e1a2-69ba9456b0ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["train.shape: (36473, 5)\n","test.shape: (36, 4)\n","submission.shape: (36, 2)\n"]},{"data":{"text/html":["\n","  \u003cdiv id=\"df-d9c6a9a7-1712-4106-b77c-219dffb5fd75\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003escore\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e37d61fd2272659b1\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eabatement of pollution\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e7b9652b17b68b7a4\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eact of abating\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.75\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36d72442aefd8232\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eactive catalyst\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.25\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e5296b0c19e1ce60e\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eeliminating process\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e54c1e3b9184cb5b6\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eforest region\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.00\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9c6a9a7-1712-4106-b77c-219dffb5fd75')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d9c6a9a7-1712-4106-b77c-219dffb5fd75 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d9c6a9a7-1712-4106-b77c-219dffb5fd75');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id     anchor                  target context  score\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-5eb63520-c90d-4722-89e3-02c37e945087\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e4112d61851461f60\u003c/td\u003e\n","      \u003ctd\u003eopc drum\u003c/td\u003e\n","      \u003ctd\u003einorganic photoconductor drum\u003c/td\u003e\n","      \u003ctd\u003eG02\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e09e418c93a776564\u003c/td\u003e\n","      \u003ctd\u003eadjust gas flow\u003c/td\u003e\n","      \u003ctd\u003ealtering gas flow\u003c/td\u003e\n","      \u003ctd\u003eF23\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36baf228038e314b\u003c/td\u003e\n","      \u003ctd\u003elower trunnion\u003c/td\u003e\n","      \u003ctd\u003elower locating\u003c/td\u003e\n","      \u003ctd\u003eB60\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1f37ead645e7f0c8\u003c/td\u003e\n","      \u003ctd\u003ecap component\u003c/td\u003e\n","      \u003ctd\u003eupper portion\u003c/td\u003e\n","      \u003ctd\u003eD06\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e71a5b6ad068d531f\u003c/td\u003e\n","      \u003ctd\u003eneural stimulation\u003c/td\u003e\n","      \u003ctd\u003eartificial neural network\u003c/td\u003e\n","      \u003ctd\u003eH04\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5eb63520-c90d-4722-89e3-02c37e945087')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-5eb63520-c90d-4722-89e3-02c37e945087 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5eb63520-c90d-4722-89e3-02c37e945087');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id              anchor                         target context\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n","2  36baf228038e314b      lower trunnion                 lower locating     B60\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-063c7a51-2725-4d30-8f41-d1f9ee97f461\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003escore\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e4112d61851461f60\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e09e418c93a776564\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36baf228038e314b\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1f37ead645e7f0c8\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e71a5b6ad068d531f\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-063c7a51-2725-4d30-8f41-d1f9ee97f461')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-063c7a51-2725-4d30-8f41-d1f9ee97f461 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-063c7a51-2725-4d30-8f41-d1f9ee97f461');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id  score\n","0  4112d61851461f60      0\n","1  09e418c93a776564      0\n","2  36baf228038e314b      0\n","3  1f37ead645e7f0c8      0\n","4  71a5b6ad068d531f      0"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","submission = pd.read_csv('sample_submission.csv')\n","print(f\"train.shape: {train.shape}\")\n","print(f\"test.shape: {test.shape}\")\n","print(f\"submission.shape: {submission.shape}\")\n","display(train.head())\n","display(test.head())\n","display(submission.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":5878,"status":"ok","timestamp":1653102392353,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"6T4B0z0yILFW","outputId":"04a5750e-92e3-4e71-8c22-9b75d7281b3f"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-dbfb6811-6703-47f0-b309-7b6eb24a6ed5\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003escore\u003c/th\u003e\n","      \u003cth\u003econtext_text\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e37d61fd2272659b1\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eabatement of pollution\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e7b9652b17b68b7a4\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eact of abating\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.75\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36d72442aefd8232\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eactive catalyst\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.25\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e5296b0c19e1ce60e\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eeliminating process\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e54c1e3b9184cb5b6\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eforest region\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.00\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbfb6811-6703-47f0-b309-7b6eb24a6ed5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-dbfb6811-6703-47f0-b309-7b6eb24a6ed5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dbfb6811-6703-47f0-b309-7b6eb24a6ed5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id     anchor                  target context  score                                       context_text\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-8c08ec1d-dac9-4af4-95d3-429cec411719\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003econtext_text\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e4112d61851461f60\u003c/td\u003e\n","      \u003ctd\u003eopc drum\u003c/td\u003e\n","      \u003ctd\u003einorganic photoconductor drum\u003c/td\u003e\n","      \u003ctd\u003eG02\u003c/td\u003e\n","      \u003ctd\u003ePHYSICS. OPTICS\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e09e418c93a776564\u003c/td\u003e\n","      \u003ctd\u003eadjust gas flow\u003c/td\u003e\n","      \u003ctd\u003ealtering gas flow\u003c/td\u003e\n","      \u003ctd\u003eF23\u003c/td\u003e\n","      \u003ctd\u003eMECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36baf228038e314b\u003c/td\u003e\n","      \u003ctd\u003elower trunnion\u003c/td\u003e\n","      \u003ctd\u003elower locating\u003c/td\u003e\n","      \u003ctd\u003eB60\u003c/td\u003e\n","      \u003ctd\u003ePERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1f37ead645e7f0c8\u003c/td\u003e\n","      \u003ctd\u003ecap component\u003c/td\u003e\n","      \u003ctd\u003eupper portion\u003c/td\u003e\n","      \u003ctd\u003eD06\u003c/td\u003e\n","      \u003ctd\u003eTEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e71a5b6ad068d531f\u003c/td\u003e\n","      \u003ctd\u003eneural stimulation\u003c/td\u003e\n","      \u003ctd\u003eartificial neural network\u003c/td\u003e\n","      \u003ctd\u003eH04\u003c/td\u003e\n","      \u003ctd\u003eELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c08ec1d-dac9-4af4-95d3-429cec411719')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-8c08ec1d-dac9-4af4-95d3-429cec411719 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8c08ec1d-dac9-4af4-95d3-429cec411719');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id              anchor                         target context                                       context_text\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# CPC Data\n","# ====================================================\n","def get_cpc_texts():\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir('./CPCSchemeXML202105'):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(f'./CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        pattern = \"^\"+pattern[:-2]\n","        cpc_result = re.sub(pattern, \"\", result[0])\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            pattern = \"^\"+pattern[:-2]\n","            results[context] = cpc_result + \". \" + re.sub(pattern, \"\", result[0])\n","    return results\n","\n","\n","def get_cpc_texts_nakama():\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir('./CPCSchemeXML202105'):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(f'./CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        cpc_result = result[0].lstrip(pattern)\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n","    return results\n","\n","\n","cpc_texts = get_cpc_texts()\n","# cpc_texts = get_cpc_texts_nakama()\n","torch.save(cpc_texts, OUTPUT_DIR+\"cpc_texts.pth\")\n","train['context_text'] = train['context'].map(cpc_texts)\n","test['context_text'] = test['context'].map(cpc_texts)\n","display(train.head())\n","display(test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1653102392353,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"46m7DZuOILDE","outputId":"4f5dffe4-5a63-4ed0-e17e-853ad8f0702c"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-ecbe50d8-f993-4923-8a87-e01153d0c4b7\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003escore\u003c/th\u003e\n","      \u003cth\u003econtext_text\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e37d61fd2272659b1\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eabatement of pollution\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","      \u003ctd\u003eabatement[SEP]abatement of pollution[SEP]HUMAN...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e7b9652b17b68b7a4\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eact of abating\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.75\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","      \u003ctd\u003eabatement[SEP]act of abating[SEP]HUMAN NECESSI...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36d72442aefd8232\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eactive catalyst\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.25\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","      \u003ctd\u003eabatement[SEP]active catalyst[SEP]HUMAN NECESS...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e5296b0c19e1ce60e\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eeliminating process\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.50\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","      \u003ctd\u003eabatement[SEP]eliminating process[SEP]HUMAN NE...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e54c1e3b9184cb5b6\u003c/td\u003e\n","      \u003ctd\u003eabatement\u003c/td\u003e\n","      \u003ctd\u003eforest region\u003c/td\u003e\n","      \u003ctd\u003eA47\u003c/td\u003e\n","      \u003ctd\u003e0.00\u003c/td\u003e\n","      \u003ctd\u003eHUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\u003c/td\u003e\n","      \u003ctd\u003eabatement[SEP]forest region[SEP]HUMAN NECESSIT...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecbe50d8-f993-4923-8a87-e01153d0c4b7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-ecbe50d8-f993-4923-8a87-e01153d0c4b7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ecbe50d8-f993-4923-8a87-e01153d0c4b7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id     anchor                  target context  score                                       context_text                                               text\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]abatement of pollution[SEP]HUMAN...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]act of abating[SEP]HUMAN NECESSI...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]active catalyst[SEP]HUMAN NECESS...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]eliminating process[SEP]HUMAN NE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]forest region[SEP]HUMAN NECESSIT..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-e849ff36-388f-40be-af33-10b4ffc56afe\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003eanchor\u003c/th\u003e\n","      \u003cth\u003etarget\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003econtext_text\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e4112d61851461f60\u003c/td\u003e\n","      \u003ctd\u003eopc drum\u003c/td\u003e\n","      \u003ctd\u003einorganic photoconductor drum\u003c/td\u003e\n","      \u003ctd\u003eG02\u003c/td\u003e\n","      \u003ctd\u003ePHYSICS. OPTICS\u003c/td\u003e\n","      \u003ctd\u003eopc drum[SEP]inorganic photoconductor drum[SEP...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e09e418c93a776564\u003c/td\u003e\n","      \u003ctd\u003eadjust gas flow\u003c/td\u003e\n","      \u003ctd\u003ealtering gas flow\u003c/td\u003e\n","      \u003ctd\u003eF23\u003c/td\u003e\n","      \u003ctd\u003eMECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\u003c/td\u003e\n","      \u003ctd\u003eadjust gas flow[SEP]altering gas flow[SEP]MECH...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e36baf228038e314b\u003c/td\u003e\n","      \u003ctd\u003elower trunnion\u003c/td\u003e\n","      \u003ctd\u003elower locating\u003c/td\u003e\n","      \u003ctd\u003eB60\u003c/td\u003e\n","      \u003ctd\u003ePERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\u003c/td\u003e\n","      \u003ctd\u003elower trunnion[SEP]lower locating[SEP]PERFORMI...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1f37ead645e7f0c8\u003c/td\u003e\n","      \u003ctd\u003ecap component\u003c/td\u003e\n","      \u003ctd\u003eupper portion\u003c/td\u003e\n","      \u003ctd\u003eD06\u003c/td\u003e\n","      \u003ctd\u003eTEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\u003c/td\u003e\n","      \u003ctd\u003ecap component[SEP]upper portion[SEP]TEXTILES; ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e71a5b6ad068d531f\u003c/td\u003e\n","      \u003ctd\u003eneural stimulation\u003c/td\u003e\n","      \u003ctd\u003eartificial neural network\u003c/td\u003e\n","      \u003ctd\u003eH04\u003c/td\u003e\n","      \u003ctd\u003eELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE\u003c/td\u003e\n","      \u003ctd\u003eneural stimulation[SEP]artificial neural netwo...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e849ff36-388f-40be-af33-10b4ffc56afe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-e849ff36-388f-40be-af33-10b4ffc56afe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e849ff36-388f-40be-af33-10b4ffc56afe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                 id              anchor                         target context                                       context_text                                               text\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS  opc drum[SEP]inorganic photoconductor drum[SEP...\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...  adjust gas flow[SEP]altering gas flow[SEP]MECH...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...  lower trunnion[SEP]lower locating[SEP]PERFORMI...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...  cap component[SEP]upper portion[SEP]TEXTILES; ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE  neural stimulation[SEP]artificial neural netwo..."]},"metadata":{},"output_type":"display_data"}],"source":["train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n","test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n","display(train.head())\n","display(test.head())"]},{"cell_type":"markdown","metadata":{"id":"isTSVEuINl2S"},"source":["# EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1653102392353,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"3rsR5QFLILAq","outputId":"efd4b2b8-f158-4510-b8d8-bb1f0e98f824"},"outputs":[{"data":{"text/plain":["\u003cmatplotlib.axes._subplots.AxesSubplot at 0x7f0452ff1350\u003e"]},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["train['score'].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1653102392353,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"qco8TIYeIK-W","outputId":"6962d629-7caf-4fc1-d7c6-32ab59602b5e"},"outputs":[{"data":{"text/plain":["B    8019\n","H    6195\n","G    6013\n","C    5288\n","A    4094\n","F    4054\n","E    1531\n","D    1279\n","Name: context, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["display(train['context'].apply(lambda x: x[0]).value_counts())"]},{"cell_type":"markdown","metadata":{"id":"0X9jmLp9NrEE"},"source":["# CV Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnm4sSJdIK73"},"outputs":[],"source":["# ====================================================\n","# CV split\n","# ====================================================\n","# train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","# Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n","#     train.loc[val_index, 'fold'] = int(n)\n","# train['fold'] = train['fold'].astype(int)\n","# display(train.groupby('fold').size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkVs3kLeKPx3"},"outputs":[],"source":["# train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","\n","# encoder = LabelEncoder()\n","# train['anchor_map'] = encoder.fit_transform(train['anchor'])\n","\n","# kf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (_, valid_index) in enumerate(kf.split(train, train['score_map'], groups=train['anchor_map'])):\n","#     train.loc[valid_index, 'fold'] = int(n)\n","\n","# train['fold'] = train['fold'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4451,"status":"ok","timestamp":1653102396795,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"Gd7lcdeGiEZU","outputId":"0754f8f7-68b7-47d4-e8cb-2898170690d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["696 37\n","696 37\n","696 37\n","696 37\n","696 37\n","697 36\n","697 36\n","696 37\n","696 37\n","696 37\n","697 36\n","697 36\n","696 37\n","696 37\n","696 37\n","696 37\n","696 37\n","697 36\n","697 36\n","697 36\n","7     2026\n","15    2008\n","14    1989\n","9     1977\n","5     1976\n","0     1930\n","1     1917\n","16    1883\n","3     1820\n","2     1817\n","8     1809\n","18    1804\n","10    1788\n","4     1755\n","19    1750\n","12    1733\n","11    1728\n","17    1666\n","6     1655\n","13    1442\n","Name: fold, dtype: int64\n"]}],"source":["!pip3 install -q iterative-stratification\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","dfx = pd.get_dummies(train, columns=[\"score\"]).groupby([\"anchor\"], as_index=False).sum()\n","cols = [c for c in dfx.columns if c.startswith(\"score_\") or c == \"anchor\"]\n","# dfx = pd.get_dummies(train, columns=[\"score\"]).groupby([\"target\"], as_index=False).sum()\n","# cols = [c for c in dfx.columns if c.startswith(\"score_\") or c == \"target\"]\n","dfx = dfx[cols]\n","\n","mskf = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=42)\n","labels = [c for c in dfx.columns if c != \"anchor\"]\n","# labels = [c for c in dfx.columns if c != \"target\"]\n","dfx_labels = dfx[labels]\n","dfx[\"fold\"] = -1\n","\n","for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n","    print(len(trn_), len(val_))\n","    dfx.loc[val_, \"fold\"] = fold\n","\n","train = train.merge(dfx[[\"anchor\", \"fold\"]], on=\"anchor\", how=\"left\")\n","# train = train.merge(dfx[[\"target\", \"fold\"]], on=\"target\", how=\"left\")\n","print(train.fold.value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67wpP1q7IK5L"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())"]},{"cell_type":"markdown","metadata":{"id":"bIqxWfHqNzR2"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148},"executionInfo":{"elapsed":9732,"status":"ok","timestamp":1653102406525,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"0LIvoPmoIFUv","outputId":"ad288b94-0f50-4f72-876b-174e4f1b659f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21de0591b92b46d086603b6b93d4d3fb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61cec5e1d9024a3698a8c7ff234f7558","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/580 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d1548af1a8448aab2d0bf9b2c8cd2c6","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"BvhCQypyN2nU"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"executionInfo":{"elapsed":5911,"status":"ok","timestamp":1653102412420,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"O9KbGQdeN195","outputId":"6074660f-bfa7-435e-b456-d4476edd8eb6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78aad57533894f4d8af389605ccc1fad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/136 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"085986b038804169aea7d2fed219e269","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/36473 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a7425c4857a4baebe7b41a8323f99cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/36473 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["max_len: 133\n"]}],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","lengths_dict = {}\n","\n","lengths = []\n","tk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","    lengths.append(length)\n","lengths_dict['context_text'] = lengths\n","\n","for text_col in ['anchor', 'target']:\n","    lengths = []\n","    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n","    for text in tk0:\n","        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","        lengths.append(length)\n","    lengths_dict[text_col] = lengths\n","    \n","CFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n","                + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twc1qFyRN17n"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text):\n","    inputs = cfg.tokenizer(text,\n","                           add_special_tokens=True,\n","                           max_length=cfg.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].values\n","        self.labels = df['score'].values\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1653102412420,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"5iFhyETfN15V","outputId":"fa420183-3aba-4b35-d654-5d7648a6c1c4"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntrain_dataset = TrainDataset(CFG, train)\\ninputs, label = train_dataset[0]\\nprint(inputs)\\nprint(label)\\n'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","train_dataset = TrainDataset(CFG, train)\n","inputs, label = train_dataset[0]\n","print(inputs)\n","print(label)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"m1x8L7BQOKr2"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPMtgF_NbjBe"},"outputs":[],"source":["# ====================================================\n","# Model\n","# ====================================================\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","            # self.model = AutoModelForSequenceClassification.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","            # self.model = AutoModelForSequenceClassification.from_config(self.config)\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n","        # self.fc = nn.Linear(self.config.num_labels, self.cfg.target_size)\n","        self._init_weights(self.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self._init_weights(self.attention)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        # feature = torch.mean(last_hidden_states, 1)\n","        weights = self.attention(last_hidden_states)\n","        feature = torch.sum(weights * last_hidden_states, dim=1)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(self.fc_dropout(feature))\n","        return output\n","    \n","    # def forward(self, inputs):\n","    #     return self.model(**inputs)\n"]},{"cell_type":"markdown","metadata":{"id":"N80Z0ZF9OcjW"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjzXaGwxN10f"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        if CFG.gradient_accumulation_steps \u003e 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    predictions = np.concatenate(predictions)\n","    return losses.avg, predictions\n","\n","\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9fNekOSN1yT"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","    \n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    if fold != CFG.train_all_index:\n","        train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    else:\n","        train_folds = folds\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds['score'].values\n","    \n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model \u0026 optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    \n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","    # criterion = nn.MSELoss()\n","    # criterion = FocalLossV1().cuda()\n","    \n","    best_score = 0.\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","        \n","        # scoring\n","        score = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n","                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","        \n","        if best_score \u003c score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds['pred'] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return valid_folds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"GIQnVGWmN1vf"},"outputs":[{"name":"stderr","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fc5680c8a5e4334a8d186c5880fe37c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/833M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2467] Elapsed 0m 1s (remain 43m 8s) Loss: 0.6819(0.6819) Grad: 188990.7031  LR: 0.00001500  \n","Epoch: [1][100/2467] Elapsed 0m 26s (remain 10m 16s) Loss: 0.6880(0.6358) Grad: 82440.0781  LR: 0.00001500  \n","Epoch: [1][200/2467] Elapsed 0m 51s (remain 9m 39s) Loss: 0.5939(0.6143) Grad: 107244.1797  LR: 0.00001499  \n","Epoch: [1][300/2467] Elapsed 1m 16s (remain 9m 9s) Loss: 0.6499(0.5988) Grad: 70917.7422  LR: 0.00001498  \n","Epoch: [1][400/2467] Elapsed 1m 41s (remain 8m 42s) Loss: 0.5300(0.5905) Grad: 73409.7891  LR: 0.00001496  \n","Epoch: [1][500/2467] Elapsed 2m 6s (remain 8m 15s) Loss: 0.5761(0.5836) Grad: 89761.4375  LR: 0.00001494  \n","Epoch: [1][600/2467] Elapsed 2m 31s (remain 7m 50s) Loss: 0.7102(0.5781) Grad: 143936.9219  LR: 0.00001491  \n","Epoch: [1][700/2467] Elapsed 2m 56s (remain 7m 24s) Loss: 0.5257(0.5763) Grad: 30932.1484  LR: 0.00001488  \n","Epoch: [1][800/2467] Elapsed 3m 21s (remain 6m 59s) Loss: 0.5764(0.5748) Grad: 35723.0508  LR: 0.00001484  \n","Epoch: [1][900/2467] Elapsed 3m 46s (remain 6m 33s) Loss: 0.5617(0.5735) Grad: 69462.3438  LR: 0.00001480  \n","Epoch: [1][1000/2467] Elapsed 4m 11s (remain 6m 8s) Loss: 0.6267(0.5719) Grad: 91832.2109  LR: 0.00001476  \n","Epoch: [1][1100/2467] Elapsed 4m 36s (remain 5m 42s) Loss: 0.6461(0.5694) Grad: 32163.5020  LR: 0.00001471  \n","Epoch: [1][1200/2467] Elapsed 5m 1s (remain 5m 17s) Loss: 0.5310(0.5683) Grad: 16544.0684  LR: 0.00001465  \n","Epoch: [1][1300/2467] Elapsed 5m 26s (remain 4m 52s) Loss: 0.5305(0.5667) Grad: 47051.7266  LR: 0.00001459  \n","Epoch: [1][1400/2467] Elapsed 5m 51s (remain 4m 27s) Loss: 0.4431(0.5647) Grad: 28433.5664  LR: 0.00001453  \n","Epoch: [1][1500/2467] Elapsed 6m 15s (remain 4m 1s) Loss: 0.4990(0.5631) Grad: 16147.0068  LR: 0.00001446  \n","Epoch: [1][1600/2467] Elapsed 6m 40s (remain 3m 36s) Loss: 0.6505(0.5606) Grad: 50784.9883  LR: 0.00001439  \n","Epoch: [1][1700/2467] Elapsed 7m 5s (remain 3m 11s) Loss: 0.5988(0.5592) Grad: 52306.1094  LR: 0.00001431  \n","Epoch: [1][1800/2467] Elapsed 7m 30s (remain 2m 46s) Loss: 0.6306(0.5588) Grad: 24685.0156  LR: 0.00001422  \n","Epoch: [1][1900/2467] Elapsed 7m 55s (remain 2m 21s) Loss: 0.5217(0.5578) Grad: 33554.4570  LR: 0.00001414  \n","Epoch: [1][2000/2467] Elapsed 8m 20s (remain 1m 56s) Loss: 0.3901(0.5577) Grad: 153194.5000  LR: 0.00001405  \n","Epoch: [1][2100/2467] Elapsed 8m 45s (remain 1m 31s) Loss: 0.7165(0.5576) Grad: 49009.2070  LR: 0.00001395  \n","Epoch: [1][2200/2467] Elapsed 9m 10s (remain 1m 6s) Loss: 0.4920(0.5572) Grad: 15302.4297  LR: 0.00001385  \n","Epoch: [1][2300/2467] Elapsed 9m 35s (remain 0m 41s) Loss: 0.5425(0.5564) Grad: 13390.0625  LR: 0.00001375  \n","Epoch: [1][2400/2467] Elapsed 10m 0s (remain 0m 16s) Loss: 0.5794(0.5560) Grad: 11923.8027  LR: 0.00001364  \n","Epoch: [1][2466/2467] Elapsed 10m 16s (remain 0m 0s) Loss: 0.4412(0.5554) Grad: 23965.0645  LR: 0.00001357  \n","EVAL: [0/138] Elapsed 0m 0s (remain 1m 8s) Loss: 0.3751(0.3751) \n","EVAL: [100/138] Elapsed 0m 13s (remain 0m 4s) Loss: 0.6793(0.5694) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5554  avg_val_loss: 0.5581  time: 635s\n","Epoch 1 - Score: 0.8200\n","Epoch 1 - Save Best Score: 0.8200 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [137/138] Elapsed 0m 18s (remain 0m 0s) Loss: 0.4968(0.5581) \n","Epoch: [2][0/2467] Elapsed 0m 0s (remain 29m 18s) Loss: 0.5026(0.5026) Grad: 82954.6953  LR: 0.00001357  \n","Epoch: [2][100/2467] Elapsed 0m 25s (remain 10m 6s) Loss: 0.3759(0.5131) Grad: 97540.3281  LR: 0.00001345  \n","Epoch: [2][200/2467] Elapsed 0m 51s (remain 9m 35s) Loss: 0.5873(0.5243) Grad: 53112.4375  LR: 0.00001333  \n","Epoch: [2][300/2467] Elapsed 1m 15s (remain 9m 6s) Loss: 0.3222(0.5226) Grad: 80254.5469  LR: 0.00001321  \n","Epoch: [2][400/2467] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5748(0.5208) Grad: 70213.8516  LR: 0.00001309  \n","Epoch: [2][500/2467] Elapsed 2m 5s (remain 8m 12s) Loss: 0.4535(0.5200) Grad: 192630.7500  LR: 0.00001296  \n","Epoch: [2][600/2467] Elapsed 2m 30s (remain 7m 46s) Loss: 0.5096(0.5220) Grad: 55994.7578  LR: 0.00001282  \n","Epoch: [2][700/2467] Elapsed 2m 55s (remain 7m 21s) Loss: 0.5836(0.5213) Grad: 189115.0000  LR: 0.00001269  \n","Epoch: [2][800/2467] Elapsed 3m 19s (remain 6m 55s) Loss: 0.4904(0.5214) Grad: 21886.9082  LR: 0.00001255  \n","Epoch: [2][900/2467] Elapsed 3m 44s (remain 6m 30s) Loss: 0.5184(0.5198) Grad: 28218.1738  LR: 0.00001241  \n","Epoch: [2][1000/2467] Elapsed 4m 9s (remain 6m 5s) Loss: 0.4169(0.5179) Grad: 16891.0566  LR: 0.00001226  \n","Epoch: [2][1100/2467] Elapsed 4m 34s (remain 5m 40s) Loss: 0.5064(0.5175) Grad: 45089.4023  LR: 0.00001211  \n","Epoch: [2][1200/2467] Elapsed 4m 59s (remain 5m 15s) Loss: 0.5143(0.5182) Grad: 64553.2109  LR: 0.00001196  \n","Epoch: [2][1300/2467] Elapsed 5m 24s (remain 4m 50s) Loss: 0.5630(0.5179) Grad: 39384.9570  LR: 0.00001180  \n","Epoch: [2][1400/2467] Elapsed 5m 48s (remain 4m 25s) Loss: 0.5616(0.5178) Grad: 29347.3203  LR: 0.00001165  \n","Epoch: [2][1500/2467] Elapsed 6m 13s (remain 4m 0s) Loss: 0.5433(0.5188) Grad: 38899.8047  LR: 0.00001149  \n","Epoch: [2][1600/2467] Elapsed 6m 38s (remain 3m 35s) Loss: 0.5663(0.5191) Grad: 38519.7656  LR: 0.00001132  \n","Epoch: [2][1700/2467] Elapsed 7m 3s (remain 3m 10s) Loss: 0.6098(0.5194) Grad: 37660.5000  LR: 0.00001116  \n","Epoch: [2][1800/2467] Elapsed 7m 28s (remain 2m 45s) Loss: 0.4093(0.5191) Grad: 164739.7812  LR: 0.00001099  \n","Epoch: [2][1900/2467] Elapsed 7m 52s (remain 2m 20s) Loss: 0.3530(0.5192) Grad: 21107.1953  LR: 0.00001082  \n","Epoch: [2][2000/2467] Elapsed 8m 17s (remain 1m 55s) Loss: 0.5918(0.5183) Grad: 24222.6543  LR: 0.00001065  \n","Epoch: [2][2100/2467] Elapsed 8m 42s (remain 1m 31s) Loss: 0.5245(0.5182) Grad: 97971.3047  LR: 0.00001047  \n","Epoch: [2][2200/2467] Elapsed 9m 7s (remain 1m 6s) Loss: 0.4578(0.5185) Grad: 28243.0938  LR: 0.00001030  \n","Epoch: [2][2300/2467] Elapsed 9m 32s (remain 0m 41s) Loss: 0.5868(0.5184) Grad: 98955.8516  LR: 0.00001012  \n","Epoch: [2][2400/2467] Elapsed 9m 57s (remain 0m 16s) Loss: 0.4919(0.5185) Grad: 29890.6621  LR: 0.00000994  \n","Epoch: [2][2466/2467] Elapsed 10m 13s (remain 0m 0s) Loss: 0.5535(0.5187) Grad: 11045.2549  LR: 0.00000982  \n","EVAL: [0/138] Elapsed 0m 0s (remain 1m 2s) Loss: 0.3735(0.3735) \n","EVAL: [100/138] Elapsed 0m 13s (remain 0m 4s) Loss: 0.6749(0.5646) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5187  avg_val_loss: 0.5522  time: 632s\n","Epoch 2 - Score: 0.8231\n","Epoch 2 - Save Best Score: 0.8231 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [137/138] Elapsed 0m 18s (remain 0m 0s) Loss: 0.4990(0.5522) \n","Epoch: [3][0/2467] Elapsed 0m 0s (remain 30m 4s) Loss: 0.4334(0.4334) Grad: 45671.9297  LR: 0.00000982  \n","Epoch: [3][100/2467] Elapsed 0m 25s (remain 10m 6s) Loss: 0.4904(0.5115) Grad: 112970.4453  LR: 0.00000963  \n","Epoch: [3][200/2467] Elapsed 0m 50s (remain 9m 34s) Loss: 0.6135(0.5078) Grad: 122559.9531  LR: 0.00000945  \n","Epoch: [3][300/2467] Elapsed 1m 15s (remain 9m 5s) Loss: 0.5019(0.5064) Grad: 155776.5781  LR: 0.00000927  \n","Epoch: [3][400/2467] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5464(0.5076) Grad: 30386.0801  LR: 0.00000908  \n","Epoch: [3][500/2467] Elapsed 2m 5s (remain 8m 11s) Loss: 0.4876(0.5077) Grad: 76156.0938  LR: 0.00000889  \n","Epoch: [3][600/2467] Elapsed 2m 30s (remain 7m 46s) Loss: 0.4338(0.5066) Grad: 58378.0781  LR: 0.00000870  \n","Epoch: [3][700/2467] Elapsed 2m 54s (remain 7m 20s) Loss: 0.4300(0.5057) Grad: 50551.7852  LR: 0.00000851  \n","Epoch: [3][800/2467] Elapsed 3m 19s (remain 6m 55s) Loss: 0.5324(0.5071) Grad: 206170.4844  LR: 0.00000833  \n","Epoch: [3][900/2467] Elapsed 3m 44s (remain 6m 30s) Loss: 0.5378(0.5067) Grad: 79714.3672  LR: 0.00000814  \n","Epoch: [3][1000/2467] Elapsed 4m 9s (remain 6m 4s) Loss: 0.4002(0.5089) Grad: 57399.3750  LR: 0.00000794  \n","Epoch: [3][1100/2467] Elapsed 4m 34s (remain 5m 39s) Loss: 0.3634(0.5079) Grad: 71228.8672  LR: 0.00000775  \n","Epoch: [3][1200/2467] Elapsed 4m 58s (remain 5m 15s) Loss: 0.4398(0.5070) Grad: 162737.8438  LR: 0.00000756  \n","Epoch: [3][1300/2467] Elapsed 5m 23s (remain 4m 50s) Loss: 0.5061(0.5076) Grad: 153607.7500  LR: 0.00000737  \n","Epoch: [3][1400/2467] Elapsed 5m 48s (remain 4m 25s) Loss: 0.5935(0.5080) Grad: 31409.7227  LR: 0.00000718  \n","Epoch: [3][1500/2467] Elapsed 6m 13s (remain 4m 0s) Loss: 0.6227(0.5078) Grad: 61792.3281  LR: 0.00000699  \n","Epoch: [3][1600/2467] Elapsed 6m 38s (remain 3m 35s) Loss: 0.6090(0.5069) Grad: 85605.7500  LR: 0.00000680  \n","Epoch: [3][1700/2467] Elapsed 7m 3s (remain 3m 10s) Loss: 0.2884(0.5063) Grad: 62787.8750  LR: 0.00000661  \n","Epoch: [3][1800/2467] Elapsed 7m 27s (remain 2m 45s) Loss: 0.4653(0.5060) Grad: 60216.6328  LR: 0.00000642  \n","Epoch: [3][1900/2467] Elapsed 7m 52s (remain 2m 20s) Loss: 0.5381(0.5061) Grad: 50513.5078  LR: 0.00000623  \n","Epoch: [3][2000/2467] Elapsed 8m 17s (remain 1m 55s) Loss: 0.5016(0.5058) Grad: 119320.2578  LR: 0.00000604  \n","Epoch: [3][2100/2467] Elapsed 8m 42s (remain 1m 30s) Loss: 0.5742(0.5053) Grad: 85983.9922  LR: 0.00000586  \n","Epoch: [3][2200/2467] Elapsed 9m 6s (remain 1m 6s) Loss: 0.5642(0.5055) Grad: 44301.6406  LR: 0.00000567  \n","Epoch: [3][2300/2467] Elapsed 9m 31s (remain 0m 41s) Loss: 0.4540(0.5054) Grad: 405176.8438  LR: 0.00000549  \n","Epoch: [3][2400/2467] Elapsed 9m 56s (remain 0m 16s) Loss: 0.5661(0.5051) Grad: 31480.0488  LR: 0.00000530  \n","Epoch: [3][2466/2467] Elapsed 10m 12s (remain 0m 0s) Loss: 0.5677(0.5053) Grad: 23111.5645  LR: 0.00000518  \n","EVAL: [0/138] Elapsed 0m 0s (remain 1m 3s) Loss: 0.3778(0.3778) \n","EVAL: [100/138] Elapsed 0m 13s (remain 0m 4s) Loss: 0.6763(0.5805) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5053  avg_val_loss: 0.5662  time: 631s\n","Epoch 3 - Score: 0.8326\n","Epoch 3 - Save Best Score: 0.8326 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [137/138] Elapsed 0m 18s (remain 0m 0s) Loss: 0.4940(0.5662) \n","Epoch: [4][0/2467] Elapsed 0m 0s (remain 29m 38s) Loss: 0.5945(0.5945) Grad: 107613.3750  LR: 0.00000518  \n","Epoch: [4][100/2467] Elapsed 0m 25s (remain 10m 2s) Loss: 0.3935(0.4965) Grad: 38656.1445  LR: 0.00000500  \n","Epoch: [4][200/2467] Elapsed 0m 50s (remain 9m 33s) Loss: 0.6851(0.5014) Grad: 209777.8750  LR: 0.00000482  \n","Epoch: [4][300/2467] Elapsed 1m 15s (remain 9m 4s) Loss: 0.5752(0.5057) Grad: 40943.1797  LR: 0.00000464  \n","Epoch: [4][400/2467] Elapsed 1m 40s (remain 8m 37s) Loss: 0.6131(0.5050) Grad: 57883.6953  LR: 0.00000447  \n","Epoch: [4][500/2467] Elapsed 2m 5s (remain 8m 11s) Loss: 0.4362(0.5020) Grad: 53183.0039  LR: 0.00000429  \n","Epoch: [4][600/2467] Elapsed 2m 29s (remain 7m 45s) Loss: 0.5006(0.5023) Grad: 27919.6738  LR: 0.00000412  \n","Epoch: [4][700/2467] Elapsed 2m 54s (remain 7m 19s) Loss: 0.4055(0.5005) Grad: 31535.8555  LR: 0.00000395  \n","Epoch: [4][800/2467] Elapsed 3m 19s (remain 6m 54s) Loss: 0.5044(0.5001) Grad: 33469.8555  LR: 0.00000379  \n","Epoch: [4][900/2467] Elapsed 3m 44s (remain 6m 29s) Loss: 0.5409(0.4986) Grad: 58336.9727  LR: 0.00000362  \n","Epoch: [4][1000/2467] Elapsed 4m 8s (remain 6m 4s) Loss: 0.5070(0.4992) Grad: 40865.4258  LR: 0.00000346  \n","Epoch: [4][1100/2467] Elapsed 4m 33s (remain 5m 39s) Loss: 0.5760(0.4984) Grad: 352102.8125  LR: 0.00000330  \n","Epoch: [4][1200/2467] Elapsed 4m 58s (remain 5m 14s) Loss: 0.6005(0.4979) Grad: 43834.9531  LR: 0.00000314  \n","Epoch: [4][1300/2467] Elapsed 5m 23s (remain 4m 49s) Loss: 0.5151(0.4972) Grad: 43942.7500  LR: 0.00000299  \n","Epoch: [4][1400/2467] Elapsed 5m 48s (remain 4m 24s) Loss: 0.4084(0.4974) Grad: 66668.1094  LR: 0.00000284  \n","Epoch: [4][1500/2467] Elapsed 6m 12s (remain 3m 59s) Loss: 0.5228(0.4977) Grad: 35135.4258  LR: 0.00000269  \n","Epoch: [4][1600/2467] Elapsed 6m 37s (remain 3m 35s) Loss: 0.4613(0.4991) Grad: 7397.5562  LR: 0.00000254  \n","Epoch: [4][1700/2467] Elapsed 7m 2s (remain 3m 10s) Loss: 0.5961(0.4991) Grad: 9366.2061  LR: 0.00000240  \n","Epoch: [4][1800/2467] Elapsed 7m 27s (remain 2m 45s) Loss: 0.5988(0.4981) Grad: 7096.1611  LR: 0.00000226  \n","Epoch: [4][1900/2467] Elapsed 7m 51s (remain 2m 20s) Loss: 0.6665(0.4981) Grad: 11471.0107  LR: 0.00000213  \n","Epoch: [4][2000/2467] Elapsed 8m 16s (remain 1m 55s) Loss: 0.5283(0.4980) Grad: 15103.5039  LR: 0.00000200  \n","Epoch: [4][2100/2467] Elapsed 8m 41s (remain 1m 30s) Loss: 0.4675(0.4984) Grad: 26878.7344  LR: 0.00000187  \n","Epoch: [4][2200/2467] Elapsed 9m 6s (remain 1m 5s) Loss: 0.4938(0.4981) Grad: 18906.6465  LR: 0.00000175  \n","Epoch: [4][2300/2467] Elapsed 9m 30s (remain 0m 41s) Loss: 0.5607(0.4979) Grad: 12211.8369  LR: 0.00000163  \n","Epoch: [4][2400/2467] Elapsed 9m 55s (remain 0m 16s) Loss: 0.3777(0.4982) Grad: 16189.7148  LR: 0.00000151  \n","Epoch: [4][2466/2467] Elapsed 10m 11s (remain 0m 0s) Loss: 0.4154(0.4979) Grad: 34538.5234  LR: 0.00000143  \n","EVAL: [0/138] Elapsed 0m 0s (remain 1m 5s) Loss: 0.3832(0.3832) \n","EVAL: [100/138] Elapsed 0m 13s (remain 0m 4s) Loss: 0.6799(0.5952) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4979  avg_val_loss: 0.5777  time: 630s\n","Epoch 4 - Score: 0.8309\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [137/138] Elapsed 0m 18s (remain 0m 0s) Loss: 0.4952(0.5777) \n","Epoch: [5][0/2467] Elapsed 0m 0s (remain 29m 34s) Loss: 0.6456(0.6456) Grad: 151154.2656  LR: 0.00000143  \n","Epoch: [5][100/2467] Elapsed 0m 25s (remain 9m 57s) Loss: 0.4862(0.4986) Grad: 37674.7305  LR: 0.00000132  \n","Epoch: [5][200/2467] Elapsed 0m 50s (remain 9m 26s) Loss: 0.5378(0.4947) Grad: 70602.7266  LR: 0.00000122  \n","Epoch: [5][300/2467] Elapsed 1m 15s (remain 9m 0s) Loss: 0.3666(0.4947) Grad: 41200.9219  LR: 0.00000111  \n","Epoch: [5][400/2467] Elapsed 1m 39s (remain 8m 34s) Loss: 0.5913(0.4999) Grad: 56197.3438  LR: 0.00000102  \n","Epoch: [5][500/2467] Elapsed 2m 4s (remain 8m 9s) Loss: 0.3825(0.4974) Grad: 40545.7812  LR: 0.00000092  \n","Epoch: [5][600/2467] Elapsed 2m 29s (remain 7m 44s) Loss: 0.6098(0.4978) Grad: 107375.4062  LR: 0.00000083  \n","Epoch: [5][700/2467] Elapsed 2m 54s (remain 7m 19s) Loss: 0.5256(0.4969) Grad: 33428.7734  LR: 0.00000075  \n","Epoch: [5][800/2467] Elapsed 3m 19s (remain 6m 54s) Loss: 0.6184(0.4961) Grad: 70394.1562  LR: 0.00000067  \n","Epoch: [5][900/2467] Elapsed 3m 43s (remain 6m 29s) Loss: 0.4277(0.4951) Grad: 51091.1055  LR: 0.00000059  \n","Epoch: [5][1000/2467] Elapsed 4m 8s (remain 6m 4s) Loss: 0.5024(0.4947) Grad: 219980.9375  LR: 0.00000052  \n","Epoch: [5][1100/2467] Elapsed 4m 33s (remain 5m 39s) Loss: 0.4707(0.4947) Grad: 32608.9102  LR: 0.00000045  \n","Epoch: [5][1200/2467] Elapsed 4m 58s (remain 5m 14s) Loss: 0.5447(0.4944) Grad: 27785.1660  LR: 0.00000039  \n","Epoch: [5][1300/2467] Elapsed 5m 23s (remain 4m 49s) Loss: 0.4993(0.4942) Grad: 37150.8047  LR: 0.00000033  \n","Epoch: [5][1400/2467] Elapsed 5m 48s (remain 4m 24s) Loss: 0.6200(0.4945) Grad: 47759.0234  LR: 0.00000028  \n","Epoch: [5][1500/2467] Elapsed 6m 12s (remain 3m 59s) Loss: 0.4504(0.4944) Grad: 12764.9004  LR: 0.00000023  \n","Epoch: [5][1600/2467] Elapsed 6m 37s (remain 3m 35s) Loss: 0.5428(0.4943) Grad: 30423.2676  LR: 0.00000018  \n","Epoch: [5][1700/2467] Elapsed 7m 2s (remain 3m 10s) Loss: 0.4210(0.4940) Grad: 216915.3125  LR: 0.00000014  \n","Epoch: [5][1800/2467] Elapsed 7m 27s (remain 2m 45s) Loss: 0.4676(0.4939) Grad: 77044.7344  LR: 0.00000011  \n","Epoch: [5][1900/2467] Elapsed 7m 52s (remain 2m 20s) Loss: 0.5479(0.4933) Grad: 29644.4297  LR: 0.00000008  \n","Epoch: [5][2000/2467] Elapsed 8m 16s (remain 1m 55s) Loss: 0.4099(0.4929) Grad: 21524.1426  LR: 0.00000005  \n","Epoch: [5][2100/2467] Elapsed 8m 41s (remain 1m 30s) Loss: 0.4690(0.4931) Grad: 13008.5342  LR: 0.00000003  \n","Epoch: [5][2200/2467] Elapsed 9m 6s (remain 1m 6s) Loss: 0.5796(0.4930) Grad: 25239.8652  LR: 0.00000002  \n","Epoch: [5][2300/2467] Elapsed 9m 31s (remain 0m 41s) Loss: 0.5436(0.4931) Grad: 49553.9219  LR: 0.00000001  \n","Epoch: [5][2400/2467] Elapsed 9m 55s (remain 0m 16s) Loss: 0.4763(0.4925) Grad: 18107.9570  LR: 0.00000000  \n","Epoch: [5][2466/2467] Elapsed 10m 12s (remain 0m 0s) Loss: 0.4028(0.4924) Grad: 14531.0742  LR: 0.00000000  \n","EVAL: [0/138] Elapsed 0m 0s (remain 1m 3s) Loss: 0.3826(0.3826) \n","EVAL: [100/138] Elapsed 0m 13s (remain 0m 4s) Loss: 0.6797(0.5982) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4924  avg_val_loss: 0.5804  time: 631s\n","Epoch 5 - Score: 0.8299\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [137/138] Elapsed 0m 18s (remain 0m 0s) Loss: 0.4946(0.5804) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 0 result ==========\n","Score: 0.8326\n","========== fold: 1 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2468] Elapsed 0m 0s (remain 18m 28s) Loss: 0.9333(0.9333) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2468] Elapsed 0m 25s (remain 9m 48s) Loss: 0.5976(0.6477) Grad: 31511.0898  LR: 0.00001500  \n","Epoch: [1][200/2468] Elapsed 0m 49s (remain 9m 20s) Loss: 0.5076(0.6167) Grad: 40493.2031  LR: 0.00001499  \n","Epoch: [1][300/2468] Elapsed 1m 14s (remain 8m 54s) Loss: 0.5507(0.6045) Grad: 17735.1719  LR: 0.00001498  \n","Epoch: [1][400/2468] Elapsed 1m 38s (remain 8m 29s) Loss: 0.5133(0.5977) Grad: 12822.2236  LR: 0.00001496  \n","Epoch: [1][500/2468] Elapsed 2m 3s (remain 8m 4s) Loss: 0.5647(0.5892) Grad: 3095.9209  LR: 0.00001494  \n","Epoch: [1][600/2468] Elapsed 2m 28s (remain 7m 40s) Loss: 0.5876(0.5848) Grad: 8523.9346  LR: 0.00001491  \n","Epoch: [1][700/2468] Elapsed 2m 52s (remain 7m 15s) Loss: 0.5656(0.5806) Grad: 8302.9658  LR: 0.00001488  \n","Epoch: [1][800/2468] Elapsed 3m 17s (remain 6m 51s) Loss: 0.4974(0.5765) Grad: 6342.5132  LR: 0.00001484  \n","Epoch: [1][900/2468] Elapsed 3m 42s (remain 6m 26s) Loss: 0.4900(0.5737) Grad: 5448.6084  LR: 0.00001480  \n","Epoch: [1][1000/2468] Elapsed 4m 6s (remain 6m 1s) Loss: 0.5057(0.5707) Grad: 9922.9463  LR: 0.00001476  \n","Epoch: [1][1100/2468] Elapsed 4m 31s (remain 5m 37s) Loss: 0.7166(0.5693) Grad: 24816.9238  LR: 0.00001471  \n","Epoch: [1][1200/2468] Elapsed 4m 56s (remain 5m 12s) Loss: 0.5314(0.5682) Grad: 3701.8665  LR: 0.00001465  \n","Epoch: [1][1300/2468] Elapsed 5m 20s (remain 4m 47s) Loss: 0.6059(0.5670) Grad: 6118.5215  LR: 0.00001459  \n","Epoch: [1][1400/2468] Elapsed 5m 45s (remain 4m 23s) Loss: 0.5844(0.5654) Grad: 9279.8428  LR: 0.00001453  \n","Epoch: [1][1500/2468] Elapsed 6m 10s (remain 3m 58s) Loss: 0.5556(0.5638) Grad: 16694.2305  LR: 0.00001446  \n","Epoch: [1][1600/2468] Elapsed 6m 34s (remain 3m 33s) Loss: 0.4174(0.5632) Grad: 8148.8936  LR: 0.00001439  \n","Epoch: [1][1700/2468] Elapsed 6m 59s (remain 3m 9s) Loss: 0.3871(0.5623) Grad: 10085.7773  LR: 0.00001431  \n","Epoch: [1][1800/2468] Elapsed 7m 23s (remain 2m 44s) Loss: 0.5889(0.5617) Grad: 4480.8989  LR: 0.00001423  \n","Epoch: [1][1900/2468] Elapsed 7m 48s (remain 2m 19s) Loss: 0.6415(0.5609) Grad: 11677.4814  LR: 0.00001414  \n","Epoch: [1][2000/2468] Elapsed 8m 13s (remain 1m 55s) Loss: 0.6417(0.5600) Grad: 10945.0469  LR: 0.00001405  \n","Epoch: [1][2100/2468] Elapsed 8m 37s (remain 1m 30s) Loss: 0.4740(0.5594) Grad: 1563.1228  LR: 0.00001395  \n","Epoch: [1][2200/2468] Elapsed 9m 2s (remain 1m 5s) Loss: 0.5138(0.5587) Grad: 5231.7930  LR: 0.00001385  \n","Epoch: [1][2300/2468] Elapsed 9m 27s (remain 0m 41s) Loss: 0.5051(0.5579) Grad: 5418.2407  LR: 0.00001375  \n","Epoch: [1][2400/2468] Elapsed 9m 51s (remain 0m 16s) Loss: 0.3269(0.5569) Grad: 2879.3982  LR: 0.00001364  \n","Epoch: [1][2467/2468] Elapsed 10m 8s (remain 0m 0s) Loss: 0.6160(0.5566) Grad: 8586.4238  LR: 0.00001357  \n","EVAL: [0/137] Elapsed 0m 0s (remain 0m 47s) Loss: 0.4581(0.4581) \n","EVAL: [100/137] Elapsed 0m 13s (remain 0m 4s) Loss: 0.4855(0.5590) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5566  avg_val_loss: 0.5602  time: 626s\n","Epoch 1 - Score: 0.8182\n","Epoch 1 - Save Best Score: 0.8182 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [136/137] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5527(0.5602) \n","Epoch: [2][0/2468] Elapsed 0m 0s (remain 24m 2s) Loss: 0.4834(0.4834) Grad: 200353.1562  LR: 0.00001357  \n","Epoch: [2][100/2468] Elapsed 0m 25s (remain 9m 55s) Loss: 0.3891(0.5315) Grad: 111608.6406  LR: 0.00001345  \n","Epoch: [2][200/2468] Elapsed 0m 50s (remain 9m 28s) Loss: 0.4818(0.5202) Grad: 53734.5742  LR: 0.00001333  \n","Epoch: [2][300/2468] Elapsed 1m 14s (remain 8m 59s) Loss: 0.5107(0.5205) Grad: 47252.3281  LR: 0.00001321  \n","Epoch: [2][400/2468] Elapsed 1m 39s (remain 8m 33s) Loss: 0.5327(0.5202) Grad: 28894.9629  LR: 0.00001309  \n","Epoch: [2][500/2468] Elapsed 2m 4s (remain 8m 8s) Loss: 0.5083(0.5192) Grad: 67513.3359  LR: 0.00001296  \n","Epoch: [2][600/2468] Elapsed 2m 29s (remain 7m 43s) Loss: 0.5720(0.5182) Grad: 45145.5469  LR: 0.00001283  \n","Epoch: [2][700/2468] Elapsed 2m 53s (remain 7m 17s) Loss: 0.4961(0.5187) Grad: 71147.4141  LR: 0.00001269  \n","Epoch: [2][800/2468] Elapsed 3m 18s (remain 6m 52s) Loss: 0.4189(0.5181) Grad: 16510.4141  LR: 0.00001255  \n","Epoch: [2][900/2468] Elapsed 3m 43s (remain 6m 27s) Loss: 0.6047(0.5169) Grad: 24981.7812  LR: 0.00001241  \n","Epoch: [2][1000/2468] Elapsed 4m 7s (remain 6m 2s) Loss: 0.6097(0.5179) Grad: 127787.3516  LR: 0.00001226  \n","Epoch: [2][1100/2468] Elapsed 4m 32s (remain 5m 38s) Loss: 0.4845(0.5177) Grad: 27240.8633  LR: 0.00001211  \n","Epoch: [2][1200/2468] Elapsed 4m 56s (remain 5m 13s) Loss: 0.3783(0.5174) Grad: 38303.4844  LR: 0.00001196  \n","Epoch: [2][1300/2468] Elapsed 5m 21s (remain 4m 48s) Loss: 0.4879(0.5167) Grad: 53310.0312  LR: 0.00001180  \n","Epoch: [2][1400/2468] Elapsed 5m 46s (remain 4m 23s) Loss: 0.4154(0.5161) Grad: 41760.7617  LR: 0.00001165  \n","Epoch: [2][1500/2468] Elapsed 6m 11s (remain 3m 59s) Loss: 0.5342(0.5167) Grad: 20549.7129  LR: 0.00001149  \n","Epoch: [2][1600/2468] Elapsed 6m 35s (remain 3m 34s) Loss: 0.5522(0.5170) Grad: 22655.7637  LR: 0.00001132  \n","Epoch: [2][1700/2468] Elapsed 7m 0s (remain 3m 9s) Loss: 0.5455(0.5175) Grad: 17096.4785  LR: 0.00001116  \n","Epoch: [2][1800/2468] Elapsed 7m 25s (remain 2m 44s) Loss: 0.5487(0.5178) Grad: 23865.9375  LR: 0.00001099  \n","Epoch: [2][1900/2468] Elapsed 7m 49s (remain 2m 20s) Loss: 0.4819(0.5174) Grad: 176936.1406  LR: 0.00001082  \n","Epoch: [2][2000/2468] Elapsed 8m 14s (remain 1m 55s) Loss: 0.4911(0.5165) Grad: 35080.7578  LR: 0.00001065  \n","Epoch: [2][2100/2468] Elapsed 8m 38s (remain 1m 30s) Loss: 0.5115(0.5166) Grad: 28353.3027  LR: 0.00001047  \n","Epoch: [2][2200/2468] Elapsed 9m 3s (remain 1m 5s) Loss: 0.6236(0.5167) Grad: 35840.2422  LR: 0.00001030  \n","Epoch: [2][2300/2468] Elapsed 9m 28s (remain 0m 41s) Loss: 0.5406(0.5169) Grad: 103014.8906  LR: 0.00001012  \n","Epoch: [2][2400/2468] Elapsed 9m 52s (remain 0m 16s) Loss: 0.4981(0.5166) Grad: 290831.0938  LR: 0.00000994  \n","Epoch: [2][2467/2468] Elapsed 10m 9s (remain 0m 0s) Loss: 0.5038(0.5166) Grad: 188865.4219  LR: 0.00000982  \n","EVAL: [0/137] Elapsed 0m 0s (remain 0m 48s) Loss: 0.4538(0.4538) \n","EVAL: [100/137] Elapsed 0m 13s (remain 0m 4s) Loss: 0.4658(0.5566) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5166  avg_val_loss: 0.5532  time: 628s\n","Epoch 2 - Score: 0.8309\n","Epoch 2 - Save Best Score: 0.8309 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [136/137] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5399(0.5532) \n","Epoch: [3][0/2468] Elapsed 0m 0s (remain 24m 24s) Loss: 0.4056(0.4056) Grad: 81685.6172  LR: 0.00000982  \n","Epoch: [3][100/2468] Elapsed 0m 25s (remain 9m 55s) Loss: 0.4205(0.5192) Grad: 33285.1406  LR: 0.00000963  \n","Epoch: [3][200/2468] Elapsed 0m 50s (remain 9m 29s) Loss: 0.5430(0.5107) Grad: 77434.1797  LR: 0.00000945  \n","Epoch: [3][300/2468] Elapsed 1m 15s (remain 9m 0s) Loss: 0.5059(0.5123) Grad: 73241.6172  LR: 0.00000927  \n","Epoch: [3][400/2468] Elapsed 1m 39s (remain 8m 34s) Loss: 0.4020(0.5121) Grad: 60124.9414  LR: 0.00000908  \n","Epoch: [3][500/2468] Elapsed 2m 4s (remain 8m 8s) Loss: 0.5200(0.5114) Grad: 30451.4141  LR: 0.00000889  \n","Epoch: [3][600/2468] Elapsed 2m 29s (remain 7m 43s) Loss: 0.5419(0.5122) Grad: 52160.2969  LR: 0.00000870  \n","Epoch: [3][700/2468] Elapsed 2m 53s (remain 7m 17s) Loss: 0.5520(0.5111) Grad: 49231.8711  LR: 0.00000852  \n","Epoch: [3][800/2468] Elapsed 3m 18s (remain 6m 52s) Loss: 0.4103(0.5091) Grad: 108012.4844  LR: 0.00000833  \n","Epoch: [3][900/2468] Elapsed 3m 42s (remain 6m 27s) Loss: 0.4295(0.5088) Grad: 68707.1094  LR: 0.00000814  \n","Epoch: [3][1000/2468] Elapsed 4m 7s (remain 6m 2s) Loss: 0.3701(0.5092) Grad: 51983.6562  LR: 0.00000795  \n","Epoch: [3][1100/2468] Elapsed 4m 32s (remain 5m 37s) Loss: 0.4959(0.5094) Grad: 44440.6133  LR: 0.00000775  \n","Epoch: [3][1200/2468] Elapsed 4m 56s (remain 5m 13s) Loss: 0.5507(0.5087) Grad: 44000.7695  LR: 0.00000756  \n","Epoch: [3][1300/2468] Elapsed 5m 21s (remain 4m 48s) Loss: 0.5488(0.5072) Grad: 89278.2656  LR: 0.00000737  \n","Epoch: [3][1400/2468] Elapsed 5m 46s (remain 4m 23s) Loss: 0.5898(0.5076) Grad: 74745.1797  LR: 0.00000718  \n","Epoch: [3][1500/2468] Elapsed 6m 10s (remain 3m 58s) Loss: 0.5428(0.5073) Grad: 24595.9688  LR: 0.00000699  \n","Epoch: [3][1600/2468] Elapsed 6m 35s (remain 3m 34s) Loss: 0.5882(0.5071) Grad: 22480.1758  LR: 0.00000680  \n","Epoch: [3][1700/2468] Elapsed 7m 0s (remain 3m 9s) Loss: 0.5382(0.5070) Grad: 46579.8320  LR: 0.00000661  \n","Epoch: [3][1800/2468] Elapsed 7m 24s (remain 2m 44s) Loss: 0.4705(0.5068) Grad: 16772.7969  LR: 0.00000642  \n","Epoch: [3][1900/2468] Elapsed 7m 49s (remain 2m 20s) Loss: 0.5896(0.5071) Grad: 30641.9004  LR: 0.00000623  \n","Epoch: [3][2000/2468] Elapsed 8m 14s (remain 1m 55s) Loss: 0.4684(0.5067) Grad: 128591.0312  LR: 0.00000605  \n","Epoch: [3][2100/2468] Elapsed 8m 38s (remain 1m 30s) Loss: 0.6222(0.5070) Grad: 630890.6250  LR: 0.00000586  \n","Epoch: [3][2200/2468] Elapsed 9m 3s (remain 1m 5s) Loss: 0.3063(0.5068) Grad: 37604.8320  LR: 0.00000567  \n","Epoch: [3][2300/2468] Elapsed 9m 27s (remain 0m 41s) Loss: 0.4254(0.5068) Grad: 25405.4336  LR: 0.00000549  \n","Epoch: [3][2400/2468] Elapsed 9m 52s (remain 0m 16s) Loss: 0.5727(0.5069) Grad: 80021.3516  LR: 0.00000531  \n","Epoch: [3][2467/2468] Elapsed 10m 8s (remain 0m 0s) Loss: 0.5444(0.5067) Grad: 27994.8262  LR: 0.00000518  \n","EVAL: [0/137] Elapsed 0m 0s (remain 0m 48s) Loss: 0.4536(0.4536) \n","EVAL: [100/137] Elapsed 0m 13s (remain 0m 4s) Loss: 0.4833(0.5583) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5067  avg_val_loss: 0.5536  time: 627s\n","Epoch 3 - Score: 0.8354\n","Epoch 3 - Save Best Score: 0.8354 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [136/137] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5314(0.5536) \n","Epoch: [4][0/2468] Elapsed 0m 0s (remain 24m 1s) Loss: 0.4857(0.4857) Grad: 58962.6562  LR: 0.00000518  \n","Epoch: [4][100/2468] Elapsed 0m 25s (remain 9m 57s) Loss: 0.3948(0.4962) Grad: 105061.9141  LR: 0.00000500  \n","Epoch: [4][200/2468] Elapsed 0m 50s (remain 9m 30s) Loss: 0.5058(0.4921) Grad: 108171.1406  LR: 0.00000482  \n","Epoch: [4][300/2468] Elapsed 1m 15s (remain 9m 1s) Loss: 0.6265(0.4967) Grad: 171605.0469  LR: 0.00000464  \n","Epoch: [4][400/2468] Elapsed 1m 39s (remain 8m 34s) Loss: 0.5009(0.4958) Grad: 278924.7500  LR: 0.00000447  \n","Epoch: [4][500/2468] Elapsed 2m 4s (remain 8m 8s) Loss: 0.5586(0.4937) Grad: 46240.1055  LR: 0.00000429  \n","Epoch: [4][600/2468] Elapsed 2m 29s (remain 7m 43s) Loss: 0.3842(0.4957) Grad: 47426.4297  LR: 0.00000412  \n","Epoch: [4][700/2468] Elapsed 2m 53s (remain 7m 18s) Loss: 0.4787(0.4955) Grad: 69677.2812  LR: 0.00000395  \n","Epoch: [4][800/2468] Elapsed 3m 18s (remain 6m 53s) Loss: 0.5961(0.4958) Grad: 118903.0000  LR: 0.00000379  \n","Epoch: [4][900/2468] Elapsed 3m 43s (remain 6m 27s) Loss: 0.4588(0.4970) Grad: 85616.4844  LR: 0.00000362  \n","Epoch: [4][1000/2468] Elapsed 4m 7s (remain 6m 3s) Loss: 0.3974(0.4974) Grad: 32886.1602  LR: 0.00000346  \n","Epoch: [4][1100/2468] Elapsed 4m 32s (remain 5m 38s) Loss: 0.5784(0.4987) Grad: 259754.2812  LR: 0.00000330  \n","Epoch: [4][1200/2468] Elapsed 4m 57s (remain 5m 13s) Loss: 0.4810(0.4986) Grad: 79100.3125  LR: 0.00000314  \n","Epoch: [4][1300/2468] Elapsed 5m 21s (remain 4m 48s) Loss: 0.5441(0.4997) Grad: 28904.7676  LR: 0.00000299  \n","Epoch: [4][1400/2468] Elapsed 5m 46s (remain 4m 23s) Loss: 0.4806(0.4995) Grad: 41332.6211  LR: 0.00000284  \n","Epoch: [4][1500/2468] Elapsed 6m 11s (remain 3m 59s) Loss: 0.5974(0.5005) Grad: 107508.5781  LR: 0.00000269  \n","Epoch: [4][1600/2468] Elapsed 6m 35s (remain 3m 34s) Loss: 0.4753(0.5004) Grad: 48963.1602  LR: 0.00000255  \n","Epoch: [4][1700/2468] Elapsed 7m 0s (remain 3m 9s) Loss: 0.5922(0.5000) Grad: 52863.2461  LR: 0.00000240  \n","Epoch: [4][1800/2468] Elapsed 7m 25s (remain 2m 44s) Loss: 0.3320(0.5002) Grad: 25731.0254  LR: 0.00000227  \n","Epoch: [4][1900/2468] Elapsed 7m 49s (remain 2m 20s) Loss: 0.5487(0.4997) Grad: 57843.5430  LR: 0.00000213  \n","Epoch: [4][2000/2468] Elapsed 8m 14s (remain 1m 55s) Loss: 0.5755(0.5001) Grad: 137397.0625  LR: 0.00000200  \n","Epoch: [4][2100/2468] Elapsed 8m 39s (remain 1m 30s) Loss: 0.5554(0.5003) Grad: 59956.2812  LR: 0.00000187  \n","Epoch: [4][2200/2468] Elapsed 9m 3s (remain 1m 5s) Loss: 0.4464(0.5008) Grad: 184129.7812  LR: 0.00000175  \n","Epoch: [4][2300/2468] Elapsed 9m 28s (remain 0m 41s) Loss: 0.4613(0.5005) Grad: 51669.9883  LR: 0.00000163  \n","Epoch: [4][2400/2468] Elapsed 9m 53s (remain 0m 16s) Loss: 0.5395(0.5005) Grad: 75804.5391  LR: 0.00000151  \n","Epoch: [4][2467/2468] Elapsed 10m 9s (remain 0m 0s) Loss: 0.5232(0.5005) Grad: 76658.9688  LR: 0.00000143  \n","EVAL: [0/137] Elapsed 0m 0s (remain 0m 49s) Loss: 0.4533(0.4533) \n","EVAL: [100/137] Elapsed 0m 13s (remain 0m 4s) Loss: 0.4908(0.5796) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.5005  avg_val_loss: 0.5754  time: 628s\n","Epoch 4 - Score: 0.8261\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [136/137] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5297(0.5754) \n","Epoch: [5][0/2468] Elapsed 0m 0s (remain 24m 28s) Loss: 0.5451(0.5451) Grad: 71977.9688  LR: 0.00000143  \n","Epoch: [5][100/2468] Elapsed 0m 25s (remain 9m 56s) Loss: 0.4533(0.4926) Grad: 78424.7109  LR: 0.00000132  \n","Epoch: [5][200/2468] Elapsed 0m 50s (remain 9m 26s) Loss: 0.4913(0.4928) Grad: 153244.1562  LR: 0.00000122  \n","Epoch: [5][300/2468] Elapsed 1m 15s (remain 9m 0s) Loss: 0.4164(0.4899) Grad: 44473.6562  LR: 0.00000111  \n","Epoch: [5][400/2468] Elapsed 1m 39s (remain 8m 34s) Loss: 0.3780(0.4918) Grad: 32560.5273  LR: 0.00000102  \n","Epoch: [5][500/2468] Elapsed 2m 4s (remain 8m 9s) Loss: 0.4104(0.4958) Grad: 44647.1211  LR: 0.00000092  \n","Epoch: [5][600/2468] Elapsed 2m 29s (remain 7m 44s) Loss: 0.4537(0.4949) Grad: 34776.3984  LR: 0.00000083  \n","Epoch: [5][700/2468] Elapsed 2m 54s (remain 7m 20s) Loss: 0.6208(0.4964) Grad: 38968.9844  LR: 0.00000075  \n","Epoch: [5][800/2468] Elapsed 3m 19s (remain 6m 55s) Loss: 0.3337(0.4968) Grad: 51019.0820  LR: 0.00000067  \n","Epoch: [5][900/2468] Elapsed 3m 44s (remain 6m 30s) Loss: 0.4719(0.4973) Grad: 77220.8906  LR: 0.00000059  \n","Epoch: [5][1000/2468] Elapsed 4m 9s (remain 6m 5s) Loss: 0.4043(0.4983) Grad: 242859.8281  LR: 0.00000052  \n","Epoch: [5][1100/2468] Elapsed 4m 34s (remain 5m 40s) Loss: 0.5100(0.4980) Grad: 43251.4922  LR: 0.00000045  \n","Epoch: [5][1200/2468] Elapsed 4m 59s (remain 5m 16s) Loss: 0.5809(0.4976) Grad: 31613.3672  LR: 0.00000039  \n","Epoch: [5][1300/2468] Elapsed 5m 24s (remain 4m 51s) Loss: 0.3605(0.4967) Grad: 21367.7227  LR: 0.00000033  \n","Epoch: [5][1400/2468] Elapsed 5m 49s (remain 4m 26s) Loss: 0.3253(0.4959) Grad: 28449.9883  LR: 0.00000028  \n","Epoch: [5][1500/2468] Elapsed 6m 14s (remain 4m 1s) Loss: 0.5175(0.4961) Grad: 35399.7305  LR: 0.00000023  \n","Epoch: [5][1600/2468] Elapsed 6m 39s (remain 3m 36s) Loss: 0.4661(0.4957) Grad: 60485.3398  LR: 0.00000018  \n","Epoch: [5][1700/2468] Elapsed 7m 4s (remain 3m 11s) Loss: 0.4797(0.4964) Grad: 51099.5820  LR: 0.00000014  \n","Epoch: [5][1800/2468] Elapsed 7m 29s (remain 2m 46s) Loss: 0.5294(0.4966) Grad: 52062.4375  LR: 0.00000011  \n","Epoch: [5][1900/2468] Elapsed 7m 54s (remain 2m 21s) Loss: 0.5399(0.4974) Grad: 49818.5898  LR: 0.00000008  \n","Epoch: [5][2000/2468] Elapsed 8m 19s (remain 1m 56s) Loss: 0.4019(0.4974) Grad: 85952.1094  LR: 0.00000005  \n","Epoch: [5][2100/2468] Elapsed 8m 44s (remain 1m 31s) Loss: 0.5484(0.4972) Grad: 61685.9180  LR: 0.00000003  \n","Epoch: [5][2200/2468] Elapsed 9m 9s (remain 1m 6s) Loss: 0.5750(0.4973) Grad: 695074.1875  LR: 0.00000002  \n","Epoch: [5][2300/2468] Elapsed 9m 34s (remain 0m 41s) Loss: 0.5441(0.4967) Grad: 78332.7344  LR: 0.00000001  \n","Epoch: [5][2400/2468] Elapsed 9m 59s (remain 0m 16s) Loss: 0.5374(0.4966) Grad: 209584.7031  LR: 0.00000000  \n","Epoch: [5][2467/2468] Elapsed 10m 15s (remain 0m 0s) Loss: 0.3898(0.4963) Grad: 41019.5742  LR: 0.00000000  \n","EVAL: [0/137] Elapsed 0m 0s (remain 0m 48s) Loss: 0.4574(0.4574) \n","EVAL: [100/137] Elapsed 0m 13s (remain 0m 4s) Loss: 0.4773(0.5783) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4963  avg_val_loss: 0.5746  time: 634s\n","Epoch 5 - Score: 0.8295\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [136/137] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5252(0.5746) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 1 result ==========\n","Score: 0.8354\n","========== fold: 2 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2475] Elapsed 0m 0s (remain 21m 40s) Loss: 0.6769(0.6769) Grad: 149115.9219  LR: 0.00001500  \n","Epoch: [1][100/2475] Elapsed 0m 25s (remain 9m 52s) Loss: 0.6363(0.6420) Grad: 37808.3281  LR: 0.00001500  \n","Epoch: [1][200/2475] Elapsed 0m 49s (remain 9m 25s) Loss: 0.6672(0.6092) Grad: 67859.6328  LR: 0.00001499  \n","Epoch: [1][300/2475] Elapsed 1m 14s (remain 8m 59s) Loss: 0.5964(0.5957) Grad: 7512.8794  LR: 0.00001498  \n","Epoch: [1][400/2475] Elapsed 1m 39s (remain 8m 34s) Loss: 0.5827(0.5872) Grad: 12298.1045  LR: 0.00001496  \n","Epoch: [1][500/2475] Elapsed 2m 4s (remain 8m 9s) Loss: 0.5988(0.5822) Grad: 18734.4082  LR: 0.00001494  \n","Epoch: [1][600/2475] Elapsed 2m 28s (remain 7m 44s) Loss: 0.5774(0.5780) Grad: 16923.2754  LR: 0.00001491  \n","Epoch: [1][700/2475] Elapsed 2m 53s (remain 7m 19s) Loss: 0.7121(0.5742) Grad: 15088.6172  LR: 0.00001488  \n","Epoch: [1][800/2475] Elapsed 3m 18s (remain 6m 54s) Loss: 0.6426(0.5716) Grad: 6358.5195  LR: 0.00001485  \n","Epoch: [1][900/2475] Elapsed 3m 43s (remain 6m 29s) Loss: 0.5098(0.5688) Grad: 19261.6816  LR: 0.00001480  \n","Epoch: [1][1000/2475] Elapsed 4m 7s (remain 6m 5s) Loss: 0.4620(0.5662) Grad: 30335.3750  LR: 0.00001476  \n","Epoch: [1][1100/2475] Elapsed 4m 32s (remain 5m 40s) Loss: 0.5247(0.5642) Grad: 18528.6406  LR: 0.00001471  \n","Epoch: [1][1200/2475] Elapsed 4m 57s (remain 5m 15s) Loss: 0.4774(0.5629) Grad: 13678.9580  LR: 0.00001465  \n","Epoch: [1][1300/2475] Elapsed 5m 22s (remain 4m 50s) Loss: 0.4170(0.5614) Grad: 14213.4355  LR: 0.00001459  \n","Epoch: [1][1400/2475] Elapsed 5m 47s (remain 4m 26s) Loss: 0.5594(0.5608) Grad: 8849.4912  LR: 0.00001453  \n","Epoch: [1][1500/2475] Elapsed 6m 11s (remain 4m 1s) Loss: 0.5971(0.5598) Grad: 12974.9014  LR: 0.00001446  \n","Epoch: [1][1600/2475] Elapsed 6m 36s (remain 3m 36s) Loss: 0.5979(0.5586) Grad: 18227.9941  LR: 0.00001439  \n","Epoch: [1][1700/2475] Elapsed 7m 1s (remain 3m 11s) Loss: 0.5200(0.5575) Grad: 4783.4868  LR: 0.00001431  \n","Epoch: [1][1800/2475] Elapsed 7m 26s (remain 2m 46s) Loss: 0.7035(0.5573) Grad: 13475.3135  LR: 0.00001423  \n","Epoch: [1][1900/2475] Elapsed 7m 50s (remain 2m 22s) Loss: 0.5903(0.5566) Grad: 65557.9766  LR: 0.00001414  \n","Epoch: [1][2000/2475] Elapsed 8m 15s (remain 1m 57s) Loss: 0.7138(0.5562) Grad: 52425.3008  LR: 0.00001405  \n","Epoch: [1][2100/2475] Elapsed 8m 40s (remain 1m 32s) Loss: 0.5301(0.5556) Grad: 8356.2041  LR: 0.00001396  \n","Epoch: [1][2200/2475] Elapsed 9m 4s (remain 1m 7s) Loss: 0.3204(0.5545) Grad: 11656.7148  LR: 0.00001386  \n","Epoch: [1][2300/2475] Elapsed 9m 29s (remain 0m 43s) Loss: 0.5686(0.5541) Grad: 17950.8594  LR: 0.00001376  \n","Epoch: [1][2400/2475] Elapsed 9m 54s (remain 0m 18s) Loss: 0.4527(0.5539) Grad: 13398.9414  LR: 0.00001365  \n","Epoch: [1][2474/2475] Elapsed 10m 12s (remain 0m 0s) Loss: 0.5811(0.5536) Grad: 10265.3828  LR: 0.00001357  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 56s) Loss: 0.3515(0.3515) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4232(0.5248) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5536  avg_val_loss: 0.5304  time: 630s\n","Epoch 1 - Score: 0.8453\n","Epoch 1 - Save Best Score: 0.8453 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4237(0.5304) \n","Epoch: [2][0/2475] Elapsed 0m 0s (remain 26m 49s) Loss: 0.4259(0.4259) Grad: 102871.5000  LR: 0.00001357  \n","Epoch: [2][100/2475] Elapsed 0m 25s (remain 10m 5s) Loss: 0.4825(0.5278) Grad: 73725.4453  LR: 0.00001345  \n","Epoch: [2][200/2475] Elapsed 0m 50s (remain 9m 34s) Loss: 0.5649(0.5178) Grad: 127996.7031  LR: 0.00001334  \n","Epoch: [2][300/2475] Elapsed 1m 15s (remain 9m 5s) Loss: 0.5003(0.5142) Grad: 36827.8867  LR: 0.00001321  \n","Epoch: [2][400/2475] Elapsed 1m 40s (remain 8m 39s) Loss: 0.4560(0.5166) Grad: 33406.3398  LR: 0.00001309  \n","Epoch: [2][500/2475] Elapsed 2m 5s (remain 8m 13s) Loss: 0.5306(0.5151) Grad: 30239.5020  LR: 0.00001296  \n","Epoch: [2][600/2475] Elapsed 2m 30s (remain 7m 47s) Loss: 0.4100(0.5156) Grad: 42274.0547  LR: 0.00001283  \n","Epoch: [2][700/2475] Elapsed 2m 54s (remain 7m 22s) Loss: 0.6023(0.5173) Grad: 152133.3125  LR: 0.00001269  \n","Epoch: [2][800/2475] Elapsed 3m 19s (remain 6m 57s) Loss: 0.5316(0.5171) Grad: 48738.7695  LR: 0.00001255  \n","Epoch: [2][900/2475] Elapsed 3m 44s (remain 6m 31s) Loss: 0.5909(0.5169) Grad: 63704.0781  LR: 0.00001241  \n","Epoch: [2][1000/2475] Elapsed 4m 9s (remain 6m 6s) Loss: 0.6298(0.5166) Grad: 114146.7109  LR: 0.00001227  \n","Epoch: [2][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.4369(0.5165) Grad: 8369.9600  LR: 0.00001212  \n","Epoch: [2][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.5987(0.5170) Grad: 9240.2197  LR: 0.00001197  \n","Epoch: [2][1300/2475] Elapsed 5m 23s (remain 4m 51s) Loss: 0.5694(0.5182) Grad: 6893.5366  LR: 0.00001181  \n","Epoch: [2][1400/2475] Elapsed 5m 48s (remain 4m 27s) Loss: 0.4339(0.5193) Grad: 20740.3691  LR: 0.00001165  \n","Epoch: [2][1500/2475] Elapsed 6m 13s (remain 4m 2s) Loss: 0.5335(0.5195) Grad: 24811.8477  LR: 0.00001149  \n","Epoch: [2][1600/2475] Elapsed 6m 38s (remain 3m 37s) Loss: 0.5274(0.5188) Grad: 38793.3281  LR: 0.00001133  \n","Epoch: [2][1700/2475] Elapsed 7m 2s (remain 3m 12s) Loss: 0.3649(0.5193) Grad: 15374.6074  LR: 0.00001117  \n","Epoch: [2][1800/2475] Elapsed 7m 27s (remain 2m 47s) Loss: 0.4871(0.5194) Grad: 8655.4424  LR: 0.00001100  \n","Epoch: [2][1900/2475] Elapsed 7m 52s (remain 2m 22s) Loss: 0.4742(0.5190) Grad: 8711.0811  LR: 0.00001083  \n","Epoch: [2][2000/2475] Elapsed 8m 17s (remain 1m 57s) Loss: 0.5257(0.5193) Grad: 13246.6025  LR: 0.00001066  \n","Epoch: [2][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.5553(0.5192) Grad: 6185.7480  LR: 0.00001048  \n","Epoch: [2][2200/2475] Elapsed 9m 6s (remain 1m 8s) Loss: 0.5508(0.5189) Grad: 10511.9951  LR: 0.00001031  \n","Epoch: [2][2300/2475] Elapsed 9m 31s (remain 0m 43s) Loss: 0.5746(0.5182) Grad: 11348.6396  LR: 0.00001013  \n","Epoch: [2][2400/2475] Elapsed 9m 56s (remain 0m 18s) Loss: 0.4509(0.5186) Grad: 6896.2139  LR: 0.00000995  \n","Epoch: [2][2474/2475] Elapsed 10m 14s (remain 0m 0s) Loss: 0.5062(0.5187) Grad: 12036.9111  LR: 0.00000982  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 55s) Loss: 0.3427(0.3427) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4338(0.5320) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5187  avg_val_loss: 0.5369  time: 632s\n","Epoch 2 - Score: 0.8436\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4117(0.5369) \n","Epoch: [3][0/2475] Elapsed 0m 0s (remain 26m 49s) Loss: 0.4743(0.4743) Grad: 31399.0996  LR: 0.00000982  \n","Epoch: [3][100/2475] Elapsed 0m 25s (remain 9m 57s) Loss: 0.4974(0.5134) Grad: 96151.8906  LR: 0.00000964  \n","Epoch: [3][200/2475] Elapsed 0m 50s (remain 9m 27s) Loss: 0.4640(0.5096) Grad: 53082.4688  LR: 0.00000945  \n","Epoch: [3][300/2475] Elapsed 1m 14s (remain 9m 1s) Loss: 0.5868(0.5098) Grad: 95101.8359  LR: 0.00000927  \n","Epoch: [3][400/2475] Elapsed 1m 39s (remain 8m 35s) Loss: 0.5454(0.5105) Grad: 85715.3906  LR: 0.00000908  \n","Epoch: [3][500/2475] Elapsed 2m 4s (remain 8m 10s) Loss: 0.5668(0.5081) Grad: 51370.6523  LR: 0.00000890  \n","Epoch: [3][600/2475] Elapsed 2m 29s (remain 7m 45s) Loss: 0.4202(0.5054) Grad: 83891.3281  LR: 0.00000871  \n","Epoch: [3][700/2475] Elapsed 2m 54s (remain 7m 20s) Loss: 0.5692(0.5060) Grad: 228217.3281  LR: 0.00000852  \n","Epoch: [3][800/2475] Elapsed 3m 18s (remain 6m 55s) Loss: 0.4852(0.5048) Grad: 26460.7344  LR: 0.00000833  \n","Epoch: [3][900/2475] Elapsed 3m 43s (remain 6m 30s) Loss: 0.6353(0.5037) Grad: 48171.7891  LR: 0.00000814  \n","Epoch: [3][1000/2475] Elapsed 4m 8s (remain 6m 5s) Loss: 0.4502(0.5039) Grad: 100545.4844  LR: 0.00000795  \n","Epoch: [3][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.4861(0.5047) Grad: 48893.8789  LR: 0.00000776  \n","Epoch: [3][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.4759(0.5046) Grad: 349733.9375  LR: 0.00000757  \n","Epoch: [3][1300/2475] Elapsed 5m 23s (remain 4m 51s) Loss: 0.6214(0.5046) Grad: 113219.6562  LR: 0.00000738  \n","Epoch: [3][1400/2475] Elapsed 5m 48s (remain 4m 26s) Loss: 0.5896(0.5058) Grad: 750662.3125  LR: 0.00000719  \n","Epoch: [3][1500/2475] Elapsed 6m 12s (remain 4m 1s) Loss: 0.4429(0.5062) Grad: 19283.8105  LR: 0.00000700  \n","Epoch: [3][1600/2475] Elapsed 6m 37s (remain 3m 37s) Loss: 0.6389(0.5063) Grad: 103210.5859  LR: 0.00000681  \n","Epoch: [3][1700/2475] Elapsed 7m 2s (remain 3m 12s) Loss: 0.4556(0.5061) Grad: 46377.9688  LR: 0.00000662  \n","Epoch: [3][1800/2475] Elapsed 7m 27s (remain 2m 47s) Loss: 0.5814(0.5058) Grad: 48301.3750  LR: 0.00000643  \n","Epoch: [3][1900/2475] Elapsed 7m 51s (remain 2m 22s) Loss: 0.3978(0.5051) Grad: 23946.9258  LR: 0.00000624  \n","Epoch: [3][2000/2475] Elapsed 8m 16s (remain 1m 57s) Loss: 0.4236(0.5049) Grad: 297061.5312  LR: 0.00000606  \n","Epoch: [3][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.4426(0.5042) Grad: 126795.7266  LR: 0.00000587  \n","Epoch: [3][2200/2475] Elapsed 9m 6s (remain 1m 8s) Loss: 0.4756(0.5045) Grad: 57826.4648  LR: 0.00000569  \n","Epoch: [3][2300/2475] Elapsed 9m 31s (remain 0m 43s) Loss: 0.5752(0.5050) Grad: 75131.5625  LR: 0.00000550  \n","Epoch: [3][2400/2475] Elapsed 9m 55s (remain 0m 18s) Loss: 0.4652(0.5052) Grad: 66935.9922  LR: 0.00000532  \n","Epoch: [3][2474/2475] Elapsed 10m 14s (remain 0m 0s) Loss: 0.5022(0.5049) Grad: 13717.6211  LR: 0.00000518  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 57s) Loss: 0.3430(0.3430) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4177(0.5368) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5049  avg_val_loss: 0.5409  time: 632s\n","Epoch 3 - Score: 0.8493\n","Epoch 3 - Save Best Score: 0.8493 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4206(0.5409) \n","Epoch: [4][0/2475] Elapsed 0m 0s (remain 27m 57s) Loss: 0.4179(0.4179) Grad: 209211.8281  LR: 0.00000518  \n","Epoch: [4][100/2475] Elapsed 0m 25s (remain 10m 4s) Loss: 0.4940(0.5022) Grad: 13921.5830  LR: 0.00000500  \n","Epoch: [4][200/2475] Elapsed 0m 51s (remain 9m 37s) Loss: 0.5000(0.5000) Grad: 30532.8125  LR: 0.00000482  \n","Epoch: [4][300/2475] Elapsed 1m 16s (remain 9m 9s) Loss: 0.5370(0.5012) Grad: 13716.9648  LR: 0.00000465  \n","Epoch: [4][400/2475] Elapsed 1m 40s (remain 8m 41s) Loss: 0.5375(0.5008) Grad: 35233.4023  LR: 0.00000447  \n","Epoch: [4][500/2475] Elapsed 2m 5s (remain 8m 14s) Loss: 0.5351(0.4999) Grad: 16069.5889  LR: 0.00000430  \n","Epoch: [4][600/2475] Elapsed 2m 30s (remain 7m 48s) Loss: 0.5061(0.4994) Grad: 94348.8438  LR: 0.00000413  \n","Epoch: [4][700/2475] Elapsed 2m 55s (remain 7m 23s) Loss: 0.4415(0.4990) Grad: 49525.2891  LR: 0.00000396  \n","Epoch: [4][800/2475] Elapsed 3m 19s (remain 6m 57s) Loss: 0.3645(0.4994) Grad: 124443.1016  LR: 0.00000379  \n","Epoch: [4][900/2475] Elapsed 3m 44s (remain 6m 32s) Loss: 0.5414(0.4999) Grad: 23863.6895  LR: 0.00000363  \n","Epoch: [4][1000/2475] Elapsed 4m 9s (remain 6m 7s) Loss: 0.6612(0.4987) Grad: 23923.4277  LR: 0.00000347  \n","Epoch: [4][1100/2475] Elapsed 4m 34s (remain 5m 42s) Loss: 0.5110(0.4985) Grad: 24569.3086  LR: 0.00000331  \n","Epoch: [4][1200/2475] Elapsed 4m 59s (remain 5m 17s) Loss: 0.5597(0.4981) Grad: 321513.8750  LR: 0.00000315  \n","Epoch: [4][1300/2475] Elapsed 5m 24s (remain 4m 52s) Loss: 0.4048(0.4985) Grad: 9101.0928  LR: 0.00000300  \n","Epoch: [4][1400/2475] Elapsed 5m 49s (remain 4m 27s) Loss: 0.4865(0.4985) Grad: 18242.1758  LR: 0.00000285  \n","Epoch: [4][1500/2475] Elapsed 6m 14s (remain 4m 2s) Loss: 0.5593(0.4987) Grad: 30572.7871  LR: 0.00000270  \n","Epoch: [4][1600/2475] Elapsed 6m 38s (remain 3m 37s) Loss: 0.4620(0.4987) Grad: 55974.2227  LR: 0.00000255  \n","Epoch: [4][1700/2475] Elapsed 7m 3s (remain 3m 12s) Loss: 0.4317(0.4984) Grad: 12598.3525  LR: 0.00000241  \n","Epoch: [4][1800/2475] Elapsed 7m 28s (remain 2m 47s) Loss: 0.5268(0.4986) Grad: 47877.4023  LR: 0.00000227  \n","Epoch: [4][1900/2475] Elapsed 7m 52s (remain 2m 22s) Loss: 0.5188(0.4991) Grad: 12134.9160  LR: 0.00000214  \n","Epoch: [4][2000/2475] Elapsed 8m 17s (remain 1m 57s) Loss: 0.5027(0.4985) Grad: 6177.0063  LR: 0.00000201  \n","Epoch: [4][2100/2475] Elapsed 8m 42s (remain 1m 32s) Loss: 0.5482(0.4986) Grad: 8172.8350  LR: 0.00000188  \n","Epoch: [4][2200/2475] Elapsed 9m 7s (remain 1m 8s) Loss: 0.4562(0.4986) Grad: 6332.0410  LR: 0.00000176  \n","Epoch: [4][2300/2475] Elapsed 9m 31s (remain 0m 43s) Loss: 0.6149(0.4991) Grad: 8754.1299  LR: 0.00000163  \n","Epoch: [4][2400/2475] Elapsed 9m 56s (remain 0m 18s) Loss: 0.4812(0.4990) Grad: 11352.8174  LR: 0.00000152  \n","Epoch: [4][2474/2475] Elapsed 10m 14s (remain 0m 0s) Loss: 0.5874(0.4992) Grad: 6526.1030  LR: 0.00000143  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 52s) Loss: 0.3419(0.3419) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4008(0.5409) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4992  avg_val_loss: 0.5442  time: 632s\n","Epoch 4 - Score: 0.8482\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4193(0.5442) \n","Epoch: [5][0/2475] Elapsed 0m 0s (remain 26m 52s) Loss: 0.3937(0.3937) Grad: 78775.7266  LR: 0.00000143  \n","Epoch: [5][100/2475] Elapsed 0m 25s (remain 9m 58s) Loss: 0.3847(0.4998) Grad: 45509.7969  LR: 0.00000132  \n","Epoch: [5][200/2475] Elapsed 0m 50s (remain 9m 28s) Loss: 0.5044(0.4922) Grad: 43914.3125  LR: 0.00000122  \n","Epoch: [5][300/2475] Elapsed 1m 15s (remain 9m 2s) Loss: 0.4836(0.4948) Grad: 41223.1367  LR: 0.00000112  \n","Epoch: [5][400/2475] Elapsed 1m 39s (remain 8m 36s) Loss: 0.4986(0.4963) Grad: 42987.7305  LR: 0.00000102  \n","Epoch: [5][500/2475] Elapsed 2m 4s (remain 8m 11s) Loss: 0.5539(0.4982) Grad: 51497.8477  LR: 0.00000092  \n","Epoch: [5][600/2475] Elapsed 2m 29s (remain 7m 46s) Loss: 0.3387(0.4974) Grad: 31793.8828  LR: 0.00000083  \n","Epoch: [5][700/2475] Elapsed 2m 54s (remain 7m 20s) Loss: 0.4113(0.4977) Grad: 15768.5762  LR: 0.00000075  \n","Epoch: [5][800/2475] Elapsed 3m 18s (remain 6m 55s) Loss: 0.5606(0.4970) Grad: 50999.5391  LR: 0.00000067  \n","Epoch: [5][900/2475] Elapsed 3m 43s (remain 6m 30s) Loss: 0.4259(0.4969) Grad: 38575.2148  LR: 0.00000059  \n","Epoch: [5][1000/2475] Elapsed 4m 8s (remain 6m 6s) Loss: 0.3156(0.4953) Grad: 83095.9844  LR: 0.00000052  \n","Epoch: [5][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.4173(0.4956) Grad: 30903.8418  LR: 0.00000045  \n","Epoch: [5][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.4910(0.4953) Grad: 89351.3359  LR: 0.00000039  \n","Epoch: [5][1300/2475] Elapsed 5m 22s (remain 4m 51s) Loss: 0.5952(0.4955) Grad: 9208.9814  LR: 0.00000033  \n","Epoch: [5][1400/2475] Elapsed 5m 47s (remain 4m 26s) Loss: 0.5237(0.4948) Grad: 16896.8691  LR: 0.00000028  \n","Epoch: [5][1500/2475] Elapsed 6m 12s (remain 4m 1s) Loss: 0.4932(0.4945) Grad: 12024.3965  LR: 0.00000023  \n","Epoch: [5][1600/2475] Elapsed 6m 37s (remain 3m 36s) Loss: 0.6173(0.4943) Grad: 23111.5234  LR: 0.00000018  \n","Epoch: [5][1700/2475] Elapsed 7m 2s (remain 3m 12s) Loss: 0.5146(0.4935) Grad: 20282.2793  LR: 0.00000015  \n","Epoch: [5][1800/2475] Elapsed 7m 26s (remain 2m 47s) Loss: 0.5286(0.4935) Grad: 41070.6562  LR: 0.00000011  \n","Epoch: [5][1900/2475] Elapsed 7m 51s (remain 2m 22s) Loss: 0.5331(0.4933) Grad: 15999.8379  LR: 0.00000008  \n","Epoch: [5][2000/2475] Elapsed 8m 16s (remain 1m 57s) Loss: 0.4834(0.4938) Grad: 22103.2500  LR: 0.00000005  \n","Epoch: [5][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.4743(0.4935) Grad: 12358.8701  LR: 0.00000003  \n","Epoch: [5][2200/2475] Elapsed 9m 5s (remain 1m 7s) Loss: 0.5436(0.4936) Grad: 13317.8457  LR: 0.00000002  \n","Epoch: [5][2300/2475] Elapsed 9m 30s (remain 0m 43s) Loss: 0.5212(0.4937) Grad: 18214.0137  LR: 0.00000001  \n","Epoch: [5][2400/2475] Elapsed 9m 55s (remain 0m 18s) Loss: 0.3718(0.4937) Grad: 21057.8457  LR: 0.00000000  \n","Epoch: [5][2474/2475] Elapsed 10m 13s (remain 0m 0s) Loss: 0.5583(0.4940) Grad: 36572.1562  LR: 0.00000000  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 55s) Loss: 0.3432(0.3432) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4087(0.5467) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4940  avg_val_loss: 0.5491  time: 631s\n","Epoch 5 - Score: 0.8466\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4187(0.5491) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 2 result ==========\n","Score: 0.8493\n","========== fold: 3 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2475] Elapsed 0m 0s (remain 18m 39s) Loss: 0.8930(0.8930) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2475] Elapsed 0m 25s (remain 9m 50s) Loss: 0.5230(0.6355) Grad: 27087.1172  LR: 0.00001500  \n","Epoch: [1][200/2475] Elapsed 0m 49s (remain 9m 23s) Loss: 0.6259(0.6123) Grad: 70715.7422  LR: 0.00001499  \n","Epoch: [1][300/2475] Elapsed 1m 14s (remain 8m 57s) Loss: 0.4755(0.5963) Grad: 49625.9727  LR: 0.00001498  \n","Epoch: [1][400/2475] Elapsed 1m 39s (remain 8m 32s) Loss: 0.5946(0.5868) Grad: 58870.2148  LR: 0.00001496  \n","Epoch: [1][500/2475] Elapsed 2m 3s (remain 8m 7s) Loss: 0.4993(0.5795) Grad: 27753.2109  LR: 0.00001494  \n","Epoch: [1][600/2475] Elapsed 2m 28s (remain 7m 42s) Loss: 0.6666(0.5759) Grad: 45210.2422  LR: 0.00001491  \n","Epoch: [1][700/2475] Elapsed 2m 53s (remain 7m 18s) Loss: 0.5475(0.5743) Grad: 14915.5732  LR: 0.00001488  \n","Epoch: [1][800/2475] Elapsed 3m 17s (remain 6m 53s) Loss: 0.4414(0.5729) Grad: 26958.3574  LR: 0.00001485  \n","Epoch: [1][900/2475] Elapsed 3m 42s (remain 6m 28s) Loss: 0.6103(0.5703) Grad: 34639.0703  LR: 0.00001480  \n","Epoch: [1][1000/2475] Elapsed 4m 7s (remain 6m 4s) Loss: 0.4605(0.5675) Grad: 30460.9199  LR: 0.00001476  \n","Epoch: [1][1100/2475] Elapsed 4m 32s (remain 5m 39s) Loss: 0.5126(0.5661) Grad: 23359.6211  LR: 0.00001471  \n","Epoch: [1][1200/2475] Elapsed 4m 57s (remain 5m 15s) Loss: 0.5135(0.5655) Grad: 29334.8164  LR: 0.00001465  \n","Epoch: [1][1300/2475] Elapsed 5m 22s (remain 4m 50s) Loss: 0.6429(0.5635) Grad: 82426.9219  LR: 0.00001459  \n","Epoch: [1][1400/2475] Elapsed 5m 47s (remain 4m 26s) Loss: 0.6137(0.5617) Grad: 32405.8203  LR: 0.00001453  \n","Epoch: [1][1500/2475] Elapsed 6m 11s (remain 4m 1s) Loss: 0.6273(0.5605) Grad: 83192.1406  LR: 0.00001446  \n","Epoch: [1][1600/2475] Elapsed 6m 36s (remain 3m 36s) Loss: 0.5911(0.5599) Grad: 26840.4297  LR: 0.00001439  \n","Epoch: [1][1700/2475] Elapsed 7m 1s (remain 3m 11s) Loss: 0.5475(0.5582) Grad: 22096.1367  LR: 0.00001431  \n","Epoch: [1][1800/2475] Elapsed 7m 26s (remain 2m 46s) Loss: 0.5836(0.5572) Grad: 55557.7109  LR: 0.00001423  \n","Epoch: [1][1900/2475] Elapsed 7m 50s (remain 2m 22s) Loss: 0.5108(0.5564) Grad: 32650.0312  LR: 0.00001414  \n","Epoch: [1][2000/2475] Elapsed 8m 15s (remain 1m 57s) Loss: 0.5203(0.5552) Grad: 22559.7949  LR: 0.00001405  \n","Epoch: [1][2100/2475] Elapsed 8m 40s (remain 1m 32s) Loss: 0.4046(0.5550) Grad: 49451.8711  LR: 0.00001396  \n","Epoch: [1][2200/2475] Elapsed 9m 4s (remain 1m 7s) Loss: 0.5962(0.5547) Grad: 46839.9688  LR: 0.00001386  \n","Epoch: [1][2300/2475] Elapsed 9m 29s (remain 0m 43s) Loss: 0.4389(0.5545) Grad: 127429.6797  LR: 0.00001376  \n","Epoch: [1][2400/2475] Elapsed 9m 54s (remain 0m 18s) Loss: 0.5205(0.5535) Grad: 41674.6367  LR: 0.00001365  \n","Epoch: [1][2474/2475] Elapsed 10m 12s (remain 0m 0s) Loss: 0.5410(0.5533) Grad: 68093.3203  LR: 0.00001357  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 45s) Loss: 0.4749(0.4749) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6110(0.5383) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5533  avg_val_loss: 0.5364  time: 630s\n","Epoch 1 - Score: 0.8381\n","Epoch 1 - Save Best Score: 0.8381 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6720(0.5364) \n","Epoch: [2][0/2475] Elapsed 0m 0s (remain 25m 14s) Loss: 0.4827(0.4827) Grad: 112336.9688  LR: 0.00001357  \n","Epoch: [2][100/2475] Elapsed 0m 25s (remain 10m 3s) Loss: 0.6457(0.5240) Grad: 341543.0312  LR: 0.00001345  \n","Epoch: [2][200/2475] Elapsed 0m 50s (remain 9m 34s) Loss: 0.5127(0.5150) Grad: 28869.8477  LR: 0.00001334  \n","Epoch: [2][300/2475] Elapsed 1m 15s (remain 9m 5s) Loss: 0.7477(0.5174) Grad: 608477.6875  LR: 0.00001321  \n","Epoch: [2][400/2475] Elapsed 1m 40s (remain 8m 37s) Loss: 0.5789(0.5171) Grad: 37969.0898  LR: 0.00001309  \n","Epoch: [2][500/2475] Elapsed 2m 4s (remain 8m 11s) Loss: 0.4536(0.5163) Grad: 211273.0312  LR: 0.00001296  \n","Epoch: [2][600/2475] Elapsed 2m 29s (remain 7m 46s) Loss: 0.8229(0.5166) Grad: 266664.4688  LR: 0.00001283  \n","Epoch: [2][700/2475] Elapsed 2m 54s (remain 7m 20s) Loss: 0.6256(0.5169) Grad: 115698.5859  LR: 0.00001269  \n","Epoch: [2][800/2475] Elapsed 3m 18s (remain 6m 55s) Loss: 0.4985(0.5165) Grad: 41862.5898  LR: 0.00001255  \n","Epoch: [2][900/2475] Elapsed 3m 43s (remain 6m 30s) Loss: 0.4796(0.5166) Grad: 76309.6875  LR: 0.00001241  \n","Epoch: [2][1000/2475] Elapsed 4m 8s (remain 6m 5s) Loss: 0.5590(0.5156) Grad: 57330.2734  LR: 0.00001226  \n","Epoch: [2][1100/2475] Elapsed 4m 32s (remain 5m 40s) Loss: 0.4333(0.5154) Grad: 46324.7539  LR: 0.00001212  \n","Epoch: [2][1200/2475] Elapsed 4m 57s (remain 5m 15s) Loss: 0.4139(0.5146) Grad: 60934.0625  LR: 0.00001196  \n","Epoch: [2][1300/2475] Elapsed 5m 22s (remain 4m 50s) Loss: 0.5530(0.5145) Grad: 125030.8359  LR: 0.00001181  \n","Epoch: [2][1400/2475] Elapsed 5m 46s (remain 4m 25s) Loss: 0.4822(0.5146) Grad: 475608.4062  LR: 0.00001165  \n","Epoch: [2][1500/2475] Elapsed 6m 11s (remain 4m 1s) Loss: 0.5339(0.5144) Grad: 42592.5586  LR: 0.00001149  \n","Epoch: [2][1600/2475] Elapsed 6m 36s (remain 3m 36s) Loss: 0.4728(0.5144) Grad: 156898.0000  LR: 0.00001133  \n","Epoch: [2][1700/2475] Elapsed 7m 0s (remain 3m 11s) Loss: 0.6181(0.5145) Grad: 70928.5312  LR: 0.00001117  \n","Epoch: [2][1800/2475] Elapsed 7m 25s (remain 2m 46s) Loss: 0.4370(0.5148) Grad: 60035.8594  LR: 0.00001100  \n","Epoch: [2][1900/2475] Elapsed 7m 50s (remain 2m 21s) Loss: 0.4296(0.5152) Grad: 64808.0859  LR: 0.00001083  \n","Epoch: [2][2000/2475] Elapsed 8m 14s (remain 1m 57s) Loss: 0.4870(0.5153) Grad: 56877.9062  LR: 0.00001066  \n","Epoch: [2][2100/2475] Elapsed 8m 39s (remain 1m 32s) Loss: 0.4369(0.5154) Grad: 211773.2188  LR: 0.00001048  \n","Epoch: [2][2200/2475] Elapsed 9m 4s (remain 1m 7s) Loss: 0.4465(0.5156) Grad: 96917.9141  LR: 0.00001031  \n","Epoch: [2][2300/2475] Elapsed 9m 28s (remain 0m 42s) Loss: 0.5225(0.5156) Grad: 332446.4062  LR: 0.00001013  \n","Epoch: [2][2400/2475] Elapsed 9m 53s (remain 0m 18s) Loss: 0.6516(0.5155) Grad: 559900.5625  LR: 0.00000995  \n","Epoch: [2][2474/2475] Elapsed 10m 11s (remain 0m 0s) Loss: 0.4146(0.5157) Grad: 99209.1641  LR: 0.00000982  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 45s) Loss: 0.4372(0.4372) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5926(0.5513) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5157  avg_val_loss: 0.5489  time: 629s\n","Epoch 2 - Score: 0.8382\n","Epoch 2 - Save Best Score: 0.8382 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6810(0.5489) \n","Epoch: [3][0/2475] Elapsed 0m 0s (remain 23m 35s) Loss: 0.4654(0.4654) Grad: 140035.4219  LR: 0.00000982  \n","Epoch: [3][100/2475] Elapsed 0m 25s (remain 9m 59s) Loss: 0.3992(0.4921) Grad: 92048.4922  LR: 0.00000963  \n","Epoch: [3][200/2475] Elapsed 0m 50s (remain 9m 33s) Loss: 0.4317(0.5033) Grad: 89616.7188  LR: 0.00000945  \n","Epoch: [3][300/2475] Elapsed 1m 15s (remain 9m 4s) Loss: 0.2693(0.4980) Grad: 282325.8438  LR: 0.00000927  \n","Epoch: [3][400/2475] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5607(0.5008) Grad: 132951.5625  LR: 0.00000908  \n","Epoch: [3][500/2475] Elapsed 2m 5s (remain 8m 12s) Loss: 0.5084(0.5017) Grad: 36625.7969  LR: 0.00000889  \n","Epoch: [3][600/2475] Elapsed 2m 29s (remain 7m 46s) Loss: 0.5609(0.5029) Grad: 82788.7812  LR: 0.00000871  \n","Epoch: [3][700/2475] Elapsed 2m 54s (remain 7m 21s) Loss: 0.5479(0.5039) Grad: 24065.8164  LR: 0.00000852  \n","Epoch: [3][800/2475] Elapsed 3m 19s (remain 6m 56s) Loss: 0.5681(0.5051) Grad: 32498.6035  LR: 0.00000833  \n","Epoch: [3][900/2475] Elapsed 3m 43s (remain 6m 31s) Loss: 0.4956(0.5049) Grad: 205782.9062  LR: 0.00000814  \n","Epoch: [3][1000/2475] Elapsed 4m 8s (remain 6m 6s) Loss: 0.5601(0.5044) Grad: 39854.3477  LR: 0.00000795  \n","Epoch: [3][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.5070(0.5047) Grad: 35355.6250  LR: 0.00000776  \n","Epoch: [3][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.5147(0.5053) Grad: 22021.2109  LR: 0.00000757  \n","Epoch: [3][1300/2475] Elapsed 5m 23s (remain 4m 51s) Loss: 0.5396(0.5042) Grad: 45348.6445  LR: 0.00000738  \n","Epoch: [3][1400/2475] Elapsed 5m 47s (remain 4m 26s) Loss: 0.4046(0.5041) Grad: 37453.1406  LR: 0.00000719  \n","Epoch: [3][1500/2475] Elapsed 6m 12s (remain 4m 1s) Loss: 0.5185(0.5043) Grad: 64395.1875  LR: 0.00000700  \n","Epoch: [3][1600/2475] Elapsed 6m 37s (remain 3m 36s) Loss: 0.4880(0.5042) Grad: 94115.3906  LR: 0.00000681  \n","Epoch: [3][1700/2475] Elapsed 7m 1s (remain 3m 12s) Loss: 0.5450(0.5047) Grad: 44773.3320  LR: 0.00000662  \n","Epoch: [3][1800/2475] Elapsed 7m 26s (remain 2m 47s) Loss: 0.5054(0.5055) Grad: 38834.3359  LR: 0.00000643  \n","Epoch: [3][1900/2475] Elapsed 7m 51s (remain 2m 22s) Loss: 0.5829(0.5057) Grad: 48464.9258  LR: 0.00000624  \n","Epoch: [3][2000/2475] Elapsed 8m 16s (remain 1m 57s) Loss: 0.5387(0.5054) Grad: 87579.5312  LR: 0.00000606  \n","Epoch: [3][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.4585(0.5054) Grad: 120957.9062  LR: 0.00000587  \n","Epoch: [3][2200/2475] Elapsed 9m 5s (remain 1m 7s) Loss: 0.5321(0.5054) Grad: 560932.5625  LR: 0.00000568  \n","Epoch: [3][2300/2475] Elapsed 9m 30s (remain 0m 43s) Loss: 0.5697(0.5054) Grad: 329806.6875  LR: 0.00000550  \n","Epoch: [3][2400/2475] Elapsed 9m 55s (remain 0m 18s) Loss: 0.5963(0.5057) Grad: 1259304.3750  LR: 0.00000532  \n","Epoch: [3][2474/2475] Elapsed 10m 13s (remain 0m 0s) Loss: 0.4080(0.5055) Grad: 47782.7656  LR: 0.00000518  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 46s) Loss: 0.4399(0.4399) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5925(0.5535) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5055  avg_val_loss: 0.5513  time: 631s\n","Epoch 3 - Score: 0.8387\n","Epoch 3 - Save Best Score: 0.8387 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6821(0.5513) \n","Epoch: [4][0/2475] Elapsed 0m 0s (remain 24m 5s) Loss: 0.4615(0.4615) Grad: 50346.4414  LR: 0.00000518  \n","Epoch: [4][100/2475] Elapsed 0m 25s (remain 10m 2s) Loss: 0.4313(0.5005) Grad: 65615.3203  LR: 0.00000500  \n","Epoch: [4][200/2475] Elapsed 0m 50s (remain 9m 34s) Loss: 0.4835(0.4977) Grad: 51794.7148  LR: 0.00000482  \n","Epoch: [4][300/2475] Elapsed 1m 15s (remain 9m 5s) Loss: 0.5499(0.4984) Grad: 54349.3984  LR: 0.00000465  \n","Epoch: [4][400/2475] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5469(0.4985) Grad: 746444.8750  LR: 0.00000447  \n","Epoch: [4][500/2475] Elapsed 2m 5s (remain 8m 12s) Loss: 0.5071(0.4968) Grad: 46409.6445  LR: 0.00000430  \n","Epoch: [4][600/2475] Elapsed 2m 29s (remain 7m 47s) Loss: 0.4357(0.4973) Grad: 46158.4570  LR: 0.00000413  \n","Epoch: [4][700/2475] Elapsed 2m 54s (remain 7m 21s) Loss: 0.3149(0.4972) Grad: 123685.2188  LR: 0.00000396  \n","Epoch: [4][800/2475] Elapsed 3m 19s (remain 6m 56s) Loss: 0.6001(0.4970) Grad: 37045.5586  LR: 0.00000379  \n","Epoch: [4][900/2475] Elapsed 3m 44s (remain 6m 31s) Loss: 0.5648(0.4979) Grad: 48372.5391  LR: 0.00000363  \n","Epoch: [4][1000/2475] Elapsed 4m 8s (remain 6m 6s) Loss: 0.3955(0.4988) Grad: 26770.9570  LR: 0.00000346  \n","Epoch: [4][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.4583(0.4988) Grad: 62601.6016  LR: 0.00000331  \n","Epoch: [4][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.5180(0.4988) Grad: 69795.2812  LR: 0.00000315  \n","Epoch: [4][1300/2475] Elapsed 5m 23s (remain 4m 51s) Loss: 0.6281(0.4983) Grad: 168432.9375  LR: 0.00000300  \n","Epoch: [4][1400/2475] Elapsed 5m 48s (remain 4m 26s) Loss: 0.4913(0.4984) Grad: 58882.1797  LR: 0.00000284  \n","Epoch: [4][1500/2475] Elapsed 6m 12s (remain 4m 1s) Loss: 0.4458(0.4975) Grad: 68168.3359  LR: 0.00000270  \n","Epoch: [4][1600/2475] Elapsed 6m 37s (remain 3m 37s) Loss: 0.4497(0.4972) Grad: 27685.3457  LR: 0.00000255  \n","Epoch: [4][1700/2475] Elapsed 7m 2s (remain 3m 12s) Loss: 0.4197(0.4971) Grad: 36007.5703  LR: 0.00000241  \n","Epoch: [4][1800/2475] Elapsed 7m 27s (remain 2m 47s) Loss: 0.5773(0.4963) Grad: 97402.5781  LR: 0.00000227  \n","Epoch: [4][1900/2475] Elapsed 7m 51s (remain 2m 22s) Loss: 0.6149(0.4969) Grad: 82439.3281  LR: 0.00000214  \n","Epoch: [4][2000/2475] Elapsed 8m 16s (remain 1m 57s) Loss: 0.5304(0.4969) Grad: 123869.1250  LR: 0.00000201  \n","Epoch: [4][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.5577(0.4967) Grad: 50187.9375  LR: 0.00000188  \n","Epoch: [4][2200/2475] Elapsed 9m 6s (remain 1m 8s) Loss: 0.4703(0.4972) Grad: 170219.8438  LR: 0.00000175  \n","Epoch: [4][2300/2475] Elapsed 9m 31s (remain 0m 43s) Loss: 0.5396(0.4972) Grad: 7799.5269  LR: 0.00000163  \n","Epoch: [4][2400/2475] Elapsed 9m 55s (remain 0m 18s) Loss: 0.5248(0.4971) Grad: 20885.2930  LR: 0.00000152  \n","Epoch: [4][2474/2475] Elapsed 10m 14s (remain 0m 0s) Loss: 0.5853(0.4972) Grad: 52230.2578  LR: 0.00000143  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 46s) Loss: 0.4359(0.4359) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5954(0.5677) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4972  avg_val_loss: 0.5637  time: 632s\n","Epoch 4 - Score: 0.8338\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6847(0.5637) \n","Epoch: [5][0/2475] Elapsed 0m 0s (remain 24m 6s) Loss: 0.5067(0.5067) Grad: 35241.3359  LR: 0.00000143  \n","Epoch: [5][100/2475] Elapsed 0m 25s (remain 9m 58s) Loss: 0.6268(0.4901) Grad: 75476.1406  LR: 0.00000132  \n","Epoch: [5][200/2475] Elapsed 0m 50s (remain 9m 29s) Loss: 0.6258(0.4950) Grad: 39773.8633  LR: 0.00000122  \n","Epoch: [5][300/2475] Elapsed 1m 15s (remain 9m 2s) Loss: 0.4447(0.4959) Grad: 228940.7344  LR: 0.00000111  \n","Epoch: [5][400/2475] Elapsed 1m 39s (remain 8m 36s) Loss: 0.5496(0.4953) Grad: 58220.9297  LR: 0.00000102  \n","Epoch: [5][500/2475] Elapsed 2m 4s (remain 8m 11s) Loss: 0.4715(0.4940) Grad: 49517.6172  LR: 0.00000092  \n","Epoch: [5][600/2475] Elapsed 2m 29s (remain 7m 46s) Loss: 0.4753(0.4947) Grad: 58325.1094  LR: 0.00000083  \n","Epoch: [5][700/2475] Elapsed 2m 54s (remain 7m 21s) Loss: 0.5674(0.4951) Grad: 26571.6074  LR: 0.00000075  \n","Epoch: [5][800/2475] Elapsed 3m 19s (remain 6m 56s) Loss: 0.4837(0.4961) Grad: 174355.2656  LR: 0.00000067  \n","Epoch: [5][900/2475] Elapsed 3m 43s (remain 6m 31s) Loss: 0.6045(0.4948) Grad: 33741.1562  LR: 0.00000059  \n","Epoch: [5][1000/2475] Elapsed 4m 8s (remain 6m 6s) Loss: 0.4777(0.4936) Grad: 12702.5693  LR: 0.00000052  \n","Epoch: [5][1100/2475] Elapsed 4m 33s (remain 5m 41s) Loss: 0.4408(0.4948) Grad: 22863.1562  LR: 0.00000045  \n","Epoch: [5][1200/2475] Elapsed 4m 58s (remain 5m 16s) Loss: 0.6216(0.4935) Grad: 18631.4922  LR: 0.00000039  \n","Epoch: [5][1300/2475] Elapsed 5m 23s (remain 4m 51s) Loss: 0.5983(0.4933) Grad: 17297.2012  LR: 0.00000033  \n","Epoch: [5][1400/2475] Elapsed 5m 47s (remain 4m 26s) Loss: 0.5586(0.4929) Grad: 31044.5938  LR: 0.00000028  \n","Epoch: [5][1500/2475] Elapsed 6m 12s (remain 4m 1s) Loss: 0.3973(0.4926) Grad: 24044.4258  LR: 0.00000023  \n","Epoch: [5][1600/2475] Elapsed 6m 37s (remain 3m 36s) Loss: 0.4589(0.4922) Grad: 27390.7891  LR: 0.00000018  \n","Epoch: [5][1700/2475] Elapsed 7m 2s (remain 3m 12s) Loss: 0.4548(0.4917) Grad: 26157.9258  LR: 0.00000014  \n","Epoch: [5][1800/2475] Elapsed 7m 26s (remain 2m 47s) Loss: 0.5587(0.4923) Grad: 118344.5859  LR: 0.00000011  \n","Epoch: [5][1900/2475] Elapsed 7m 51s (remain 2m 22s) Loss: 0.6261(0.4926) Grad: 26258.9121  LR: 0.00000008  \n","Epoch: [5][2000/2475] Elapsed 8m 16s (remain 1m 57s) Loss: 0.4168(0.4925) Grad: 12834.3799  LR: 0.00000005  \n","Epoch: [5][2100/2475] Elapsed 8m 41s (remain 1m 32s) Loss: 0.5464(0.4919) Grad: 18941.0996  LR: 0.00000003  \n","Epoch: [5][2200/2475] Elapsed 9m 5s (remain 1m 7s) Loss: 0.3970(0.4918) Grad: 27777.7246  LR: 0.00000002  \n","Epoch: [5][2300/2475] Elapsed 9m 30s (remain 0m 43s) Loss: 0.5335(0.4926) Grad: 11427.9180  LR: 0.00000001  \n","Epoch: [5][2400/2475] Elapsed 9m 55s (remain 0m 18s) Loss: 0.5464(0.4925) Grad: 24714.1582  LR: 0.00000000  \n","Epoch: [5][2474/2475] Elapsed 10m 13s (remain 0m 0s) Loss: 0.4060(0.4928) Grad: 20237.9141  LR: 0.00000000  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 46s) Loss: 0.4534(0.4534) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5892(0.5754) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4928  avg_val_loss: 0.5719  time: 631s\n","Epoch 5 - Score: 0.8312\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6849(0.5719) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 3 result ==========\n","Score: 0.8387\n","========== fold: 4 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2479] Elapsed 0m 0s (remain 21m 29s) Loss: 0.6785(0.6785) Grad: 60639.9766  LR: 0.00001500  \n","Epoch: [1][100/2479] Elapsed 0m 25s (remain 9m 56s) Loss: 0.6310(0.6461) Grad: 26467.1719  LR: 0.00001500  \n","Epoch: [1][200/2479] Elapsed 0m 50s (remain 9m 30s) Loss: 0.5160(0.6176) Grad: 47195.7461  LR: 0.00001499  \n","Epoch: [1][300/2479] Elapsed 1m 15s (remain 9m 4s) Loss: 0.6093(0.6012) Grad: 251451.3125  LR: 0.00001498  \n","Epoch: [1][400/2479] Elapsed 1m 40s (remain 8m 40s) Loss: 0.6722(0.5940) Grad: 30279.4277  LR: 0.00001496  \n","Epoch: [1][500/2479] Elapsed 2m 5s (remain 8m 15s) Loss: 0.6248(0.5870) Grad: 26912.4707  LR: 0.00001494  \n","Epoch: [1][600/2479] Elapsed 2m 30s (remain 7m 50s) Loss: 0.5577(0.5828) Grad: 42861.9766  LR: 0.00001491  \n","Epoch: [1][700/2479] Elapsed 2m 55s (remain 7m 25s) Loss: 0.5635(0.5783) Grad: 49957.4102  LR: 0.00001488  \n","Epoch: [1][800/2479] Elapsed 3m 20s (remain 7m 0s) Loss: 0.4495(0.5752) Grad: 41574.4180  LR: 0.00001485  \n","Epoch: [1][900/2479] Elapsed 3m 45s (remain 6m 35s) Loss: 0.4784(0.5725) Grad: 17120.4160  LR: 0.00001481  \n","Epoch: [1][1000/2479] Elapsed 4m 10s (remain 6m 9s) Loss: 0.5636(0.5707) Grad: 41068.8398  LR: 0.00001476  \n","Epoch: [1][1100/2479] Elapsed 4m 35s (remain 5m 44s) Loss: 0.5567(0.5680) Grad: 7175.3271  LR: 0.00001471  \n","Epoch: [1][1200/2479] Elapsed 5m 0s (remain 5m 20s) Loss: 0.5766(0.5656) Grad: 23784.5898  LR: 0.00001466  \n","Epoch: [1][1300/2479] Elapsed 5m 25s (remain 4m 54s) Loss: 0.5868(0.5649) Grad: 47545.8789  LR: 0.00001460  \n","Epoch: [1][1400/2479] Elapsed 5m 50s (remain 4m 29s) Loss: 0.6446(0.5635) Grad: 57892.6641  LR: 0.00001453  \n","Epoch: [1][1500/2479] Elapsed 6m 15s (remain 4m 4s) Loss: 0.5167(0.5620) Grad: 38081.3555  LR: 0.00001446  \n","Epoch: [1][1600/2479] Elapsed 6m 40s (remain 3m 39s) Loss: 0.5298(0.5608) Grad: 26908.8027  LR: 0.00001439  \n","Epoch: [1][1700/2479] Elapsed 7m 5s (remain 3m 14s) Loss: 0.5616(0.5595) Grad: 26253.6211  LR: 0.00001431  \n","Epoch: [1][1800/2479] Elapsed 7m 30s (remain 2m 49s) Loss: 0.5574(0.5588) Grad: 12056.7949  LR: 0.00001423  \n","Epoch: [1][1900/2479] Elapsed 7m 55s (remain 2m 24s) Loss: 0.5770(0.5576) Grad: 26706.5703  LR: 0.00001415  \n","Epoch: [1][2000/2479] Elapsed 8m 20s (remain 1m 59s) Loss: 0.5345(0.5569) Grad: 51605.9141  LR: 0.00001406  \n","Epoch: [1][2100/2479] Elapsed 8m 44s (remain 1m 34s) Loss: 0.5719(0.5558) Grad: 70683.1094  LR: 0.00001396  \n","Epoch: [1][2200/2479] Elapsed 9m 9s (remain 1m 9s) Loss: 0.4368(0.5545) Grad: 47289.1367  LR: 0.00001386  \n","Epoch: [1][2300/2479] Elapsed 9m 34s (remain 0m 44s) Loss: 0.6003(0.5541) Grad: 91538.8672  LR: 0.00001376  \n","Epoch: [1][2400/2479] Elapsed 9m 59s (remain 0m 19s) Loss: 0.4627(0.5534) Grad: 30351.1973  LR: 0.00001365  \n","Epoch: [1][2478/2479] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5289(0.5526) Grad: 47977.9531  LR: 0.00001357  \n","EVAL: [0/126] Elapsed 0m 0s (remain 0m 44s) Loss: 0.6300(0.6300) \n","EVAL: [100/126] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5645(0.5511) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5526  avg_val_loss: 0.5529  time: 636s\n","Epoch 1 - Score: 0.8226\n","Epoch 1 - Save Best Score: 0.8226 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [125/126] Elapsed 0m 16s (remain 0m 0s) Loss: 0.3707(0.5529) \n","Epoch: [2][0/2479] Elapsed 0m 0s (remain 24m 58s) Loss: 0.5850(0.5850) Grad: 101375.6250  LR: 0.00001357  \n","Epoch: [2][100/2479] Elapsed 0m 25s (remain 10m 5s) Loss: 0.4796(0.5339) Grad: 57776.7070  LR: 0.00001345  \n","Epoch: [2][200/2479] Elapsed 0m 50s (remain 9m 37s) Loss: 0.5152(0.5253) Grad: 38047.6992  LR: 0.00001334  \n","Epoch: [2][300/2479] Elapsed 1m 15s (remain 9m 9s) Loss: 0.5408(0.5193) Grad: 38020.3359  LR: 0.00001322  \n","Epoch: [2][400/2479] Elapsed 1m 40s (remain 8m 41s) Loss: 0.4957(0.5224) Grad: 194427.0938  LR: 0.00001309  \n","Epoch: [2][500/2479] Elapsed 2m 5s (remain 8m 15s) Loss: 0.3573(0.5205) Grad: 84456.0781  LR: 0.00001296  \n","Epoch: [2][600/2479] Elapsed 2m 30s (remain 7m 49s) Loss: 0.4945(0.5176) Grad: 113504.8203  LR: 0.00001283  \n","Epoch: [2][700/2479] Elapsed 2m 55s (remain 7m 24s) Loss: 0.5578(0.5164) Grad: 154002.7812  LR: 0.00001269  \n","Epoch: [2][800/2479] Elapsed 3m 19s (remain 6m 58s) Loss: 0.5023(0.5172) Grad: 33913.1992  LR: 0.00001256  \n","Epoch: [2][900/2479] Elapsed 3m 44s (remain 6m 33s) Loss: 0.4981(0.5175) Grad: 39011.2617  LR: 0.00001241  \n","Epoch: [2][1000/2479] Elapsed 4m 9s (remain 6m 8s) Loss: 0.4205(0.5182) Grad: 68274.1406  LR: 0.00001227  \n","Epoch: [2][1100/2479] Elapsed 4m 34s (remain 5m 43s) Loss: 0.6012(0.5174) Grad: 230471.2656  LR: 0.00001212  \n","Epoch: [2][1200/2479] Elapsed 4m 59s (remain 5m 18s) Loss: 0.4156(0.5181) Grad: 42398.5742  LR: 0.00001197  \n","Epoch: [2][1300/2479] Elapsed 5m 24s (remain 4m 53s) Loss: 0.5922(0.5175) Grad: 77212.5703  LR: 0.00001182  \n","Epoch: [2][1400/2479] Elapsed 5m 48s (remain 4m 28s) Loss: 0.5230(0.5174) Grad: 326133.3438  LR: 0.00001166  \n","Epoch: [2][1500/2479] Elapsed 6m 13s (remain 4m 3s) Loss: 0.5242(0.5171) Grad: 74379.8750  LR: 0.00001150  \n","Epoch: [2][1600/2479] Elapsed 6m 38s (remain 3m 38s) Loss: 0.3986(0.5169) Grad: 60250.4961  LR: 0.00001134  \n","Epoch: [2][1700/2479] Elapsed 7m 3s (remain 3m 13s) Loss: 0.2749(0.5165) Grad: 128583.3828  LR: 0.00001117  \n","Epoch: [2][1800/2479] Elapsed 7m 28s (remain 2m 48s) Loss: 0.6951(0.5163) Grad: 183389.5781  LR: 0.00001101  \n","Epoch: [2][1900/2479] Elapsed 7m 52s (remain 2m 23s) Loss: 0.4098(0.5161) Grad: 36698.0039  LR: 0.00001084  \n","Epoch: [2][2000/2479] Elapsed 8m 17s (remain 1m 58s) Loss: 0.5477(0.5157) Grad: 145640.3281  LR: 0.00001067  \n","Epoch: [2][2100/2479] Elapsed 8m 42s (remain 1m 33s) Loss: 0.4322(0.5158) Grad: 390671.0938  LR: 0.00001049  \n","Epoch: [2][2200/2479] Elapsed 9m 7s (remain 1m 9s) Loss: 0.4919(0.5155) Grad: 75308.9609  LR: 0.00001032  \n","Epoch: [2][2300/2479] Elapsed 9m 31s (remain 0m 44s) Loss: 0.4548(0.5154) Grad: 253298.3281  LR: 0.00001014  \n","Epoch: [2][2400/2479] Elapsed 9m 56s (remain 0m 19s) Loss: 0.4850(0.5152) Grad: 77395.4062  LR: 0.00000996  \n","Epoch: [2][2478/2479] Elapsed 10m 15s (remain 0m 0s) Loss: 0.5526(0.5152) Grad: 99421.7266  LR: 0.00000982  \n","EVAL: [0/126] Elapsed 0m 0s (remain 0m 45s) Loss: 0.6841(0.6841) \n","EVAL: [100/126] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5370(0.5662) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5152  avg_val_loss: 0.5703  time: 633s\n","Epoch 2 - Score: 0.8208\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [125/126] Elapsed 0m 16s (remain 0m 0s) Loss: 0.3790(0.5703) \n","Epoch: [3][0/2479] Elapsed 0m 0s (remain 24m 19s) Loss: 0.3571(0.3571) Grad: 31642.3457  LR: 0.00000982  \n","Epoch: [3][100/2479] Elapsed 0m 25s (remain 9m 57s) Loss: 0.3806(0.5043) Grad: 44667.6992  LR: 0.00000964  \n","Epoch: [3][200/2479] Elapsed 0m 50s (remain 9m 28s) Loss: 0.4456(0.5038) Grad: 33562.2461  LR: 0.00000945  \n","Epoch: [3][300/2479] Elapsed 1m 14s (remain 9m 1s) Loss: 0.5349(0.5055) Grad: 62604.5820  LR: 0.00000927  \n","Epoch: [3][400/2479] Elapsed 1m 39s (remain 8m 36s) Loss: 0.4205(0.5091) Grad: 52282.5586  LR: 0.00000909  \n","Epoch: [3][500/2479] Elapsed 2m 4s (remain 8m 11s) Loss: 0.3788(0.5080) Grad: 49435.1406  LR: 0.00000890  \n","Epoch: [3][600/2479] Elapsed 2m 29s (remain 7m 46s) Loss: 0.6564(0.5059) Grad: 24546.7422  LR: 0.00000871  \n","Epoch: [3][700/2479] Elapsed 2m 53s (remain 7m 21s) Loss: 0.4696(0.5047) Grad: 55366.0781  LR: 0.00000852  \n","Epoch: [3][800/2479] Elapsed 3m 18s (remain 6m 56s) Loss: 0.4620(0.5070) Grad: 62224.5117  LR: 0.00000834  \n","Epoch: [3][900/2479] Elapsed 3m 43s (remain 6m 31s) Loss: 0.5706(0.5060) Grad: 201185.2344  LR: 0.00000815  \n","Epoch: [3][1000/2479] Elapsed 4m 8s (remain 6m 6s) Loss: 0.5841(0.5069) Grad: 77876.0312  LR: 0.00000796  \n","Epoch: [3][1100/2479] Elapsed 4m 33s (remain 5m 41s) Loss: 0.5426(0.5077) Grad: 103544.6719  LR: 0.00000777  \n","Epoch: [3][1200/2479] Elapsed 4m 57s (remain 5m 16s) Loss: 0.4790(0.5064) Grad: 180394.9062  LR: 0.00000758  \n","Epoch: [3][1300/2479] Elapsed 5m 22s (remain 4m 52s) Loss: 0.5361(0.5060) Grad: 123975.7812  LR: 0.00000739  \n","Epoch: [3][1400/2479] Elapsed 5m 47s (remain 4m 27s) Loss: 0.5212(0.5056) Grad: 302512.0312  LR: 0.00000720  \n","Epoch: [3][1500/2479] Elapsed 6m 12s (remain 4m 2s) Loss: 0.5706(0.5061) Grad: 82067.9922  LR: 0.00000701  \n","Epoch: [3][1600/2479] Elapsed 6m 37s (remain 3m 37s) Loss: 0.5487(0.5060) Grad: 48506.0781  LR: 0.00000682  \n","Epoch: [3][1700/2479] Elapsed 7m 1s (remain 3m 12s) Loss: 0.3150(0.5070) Grad: 30148.6660  LR: 0.00000663  \n","Epoch: [3][1800/2479] Elapsed 7m 26s (remain 2m 48s) Loss: 0.5339(0.5073) Grad: 47584.0391  LR: 0.00000644  \n","Epoch: [3][1900/2479] Elapsed 7m 51s (remain 2m 23s) Loss: 0.5791(0.5072) Grad: 137449.0156  LR: 0.00000625  \n","Epoch: [3][2000/2479] Elapsed 8m 16s (remain 1m 58s) Loss: 0.4533(0.5069) Grad: 199827.5625  LR: 0.00000607  \n","Epoch: [3][2100/2479] Elapsed 8m 41s (remain 1m 33s) Loss: 0.4475(0.5067) Grad: 170450.2812  LR: 0.00000588  \n","Epoch: [3][2200/2479] Elapsed 9m 5s (remain 1m 8s) Loss: 0.5609(0.5063) Grad: 96875.5312  LR: 0.00000569  \n","Epoch: [3][2300/2479] Elapsed 9m 30s (remain 0m 44s) Loss: 0.5474(0.5059) Grad: 66741.3750  LR: 0.00000551  \n","Epoch: [3][2400/2479] Elapsed 9m 55s (remain 0m 19s) Loss: 0.4076(0.5053) Grad: 200225.2656  LR: 0.00000533  \n","Epoch: [3][2478/2479] Elapsed 10m 14s (remain 0m 0s) Loss: 0.5180(0.5048) Grad: 556008.2500  LR: 0.00000519  \n","EVAL: [0/126] Elapsed 0m 0s (remain 0m 44s) Loss: 0.7024(0.7024) \n","EVAL: [100/126] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5398(0.5903) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5048  avg_val_loss: 0.5977  time: 631s\n","Epoch 3 - Score: 0.8162\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [125/126] Elapsed 0m 16s (remain 0m 0s) Loss: 0.3668(0.5977) \n","Epoch: [4][0/2479] Elapsed 0m 0s (remain 23m 28s) Loss: 0.5218(0.5218) Grad: 101417.3672  LR: 0.00000518  \n","Epoch: [4][100/2479] Elapsed 0m 25s (remain 9m 57s) Loss: 0.4100(0.5006) Grad: 148791.0469  LR: 0.00000500  \n","Epoch: [4][200/2479] Elapsed 0m 50s (remain 9m 28s) Loss: 0.4622(0.5020) Grad: 38946.5820  LR: 0.00000483  \n","Epoch: [4][300/2479] Elapsed 1m 14s (remain 9m 2s) Loss: 0.4951(0.5014) Grad: 40517.8828  LR: 0.00000465  \n","Epoch: [4][400/2479] Elapsed 1m 39s (remain 8m 36s) Loss: 0.4719(0.4987) Grad: 101225.2578  LR: 0.00000448  \n","Epoch: [4][500/2479] Elapsed 2m 4s (remain 8m 11s) Loss: 0.4566(0.4975) Grad: 242544.6562  LR: 0.00000430  \n","Epoch: [4][600/2479] Elapsed 2m 29s (remain 7m 46s) Loss: 0.4259(0.4958) Grad: 70990.9297  LR: 0.00000413  \n","Epoch: [4][700/2479] Elapsed 2m 54s (remain 7m 21s) Loss: 0.5541(0.4964) Grad: 36054.5898  LR: 0.00000396  \n","Epoch: [4][800/2479] Elapsed 3m 18s (remain 6m 56s) Loss: 0.4203(0.4958) Grad: 94038.7812  LR: 0.00000380  \n","Epoch: [4][900/2479] Elapsed 3m 43s (remain 6m 31s) Loss: 0.4720(0.4948) Grad: 93950.8750  LR: 0.00000363  \n","Epoch: [4][1000/2479] Elapsed 4m 8s (remain 6m 6s) Loss: 0.4550(0.4940) Grad: 178327.0938  LR: 0.00000347  \n","Epoch: [4][1100/2479] Elapsed 4m 33s (remain 5m 42s) Loss: 0.5226(0.4938) Grad: 37226.9297  LR: 0.00000331  \n","Epoch: [4][1200/2479] Elapsed 4m 58s (remain 5m 17s) Loss: 0.4555(0.4940) Grad: 117148.8438  LR: 0.00000316  \n","Epoch: [4][1300/2479] Elapsed 5m 23s (remain 4m 52s) Loss: 0.4577(0.4953) Grad: 25717.0645  LR: 0.00000300  \n","Epoch: [4][1400/2479] Elapsed 5m 47s (remain 4m 27s) Loss: 0.5106(0.4952) Grad: 37312.5664  LR: 0.00000285  \n","Epoch: [4][1500/2479] Elapsed 6m 12s (remain 4m 2s) Loss: 0.5356(0.4947) Grad: 27407.5742  LR: 0.00000270  \n","Epoch: [4][1600/2479] Elapsed 6m 37s (remain 3m 37s) Loss: 0.3253(0.4948) Grad: 144731.1875  LR: 0.00000256  \n","Epoch: [4][1700/2479] Elapsed 7m 2s (remain 3m 13s) Loss: 0.5106(0.4959) Grad: 47947.0820  LR: 0.00000242  \n","Epoch: [4][1800/2479] Elapsed 7m 26s (remain 2m 48s) Loss: 0.5192(0.4959) Grad: 41961.3398  LR: 0.00000228  \n","Epoch: [4][1900/2479] Elapsed 7m 51s (remain 2m 23s) Loss: 0.4451(0.4961) Grad: 146280.1875  LR: 0.00000214  \n","Epoch: [4][2000/2479] Elapsed 8m 16s (remain 1m 58s) Loss: 0.5366(0.4962) Grad: 143244.3281  LR: 0.00000201  \n","Epoch: [4][2100/2479] Elapsed 8m 41s (remain 1m 33s) Loss: 0.4419(0.4964) Grad: 205131.5781  LR: 0.00000189  \n","Epoch: [4][2200/2479] Elapsed 9m 6s (remain 1m 8s) Loss: 0.6117(0.4967) Grad: 99615.9531  LR: 0.00000176  \n","Epoch: [4][2300/2479] Elapsed 9m 31s (remain 0m 44s) Loss: 0.4084(0.4967) Grad: 43015.0625  LR: 0.00000164  \n","Epoch: [4][2400/2479] Elapsed 9m 55s (remain 0m 19s) Loss: 0.5274(0.4971) Grad: 276484.6875  LR: 0.00000152  \n","Epoch: [4][2478/2479] Elapsed 10m 15s (remain 0m 0s) Loss: 0.5076(0.4970) Grad: 44258.8359  LR: 0.00000144  \n","EVAL: [0/126] Elapsed 0m 0s (remain 0m 47s) Loss: 0.7222(0.7222) \n","EVAL: [100/126] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5417(0.5862) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4970  avg_val_loss: 0.5904  time: 632s\n","Epoch 4 - Score: 0.8198\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [125/126] Elapsed 0m 16s (remain 0m 0s) Loss: 0.3676(0.5904) \n","Epoch: [5][0/2479] Elapsed 0m 0s (remain 24m 6s) Loss: 0.5160(0.5160) Grad: 61019.2109  LR: 0.00000143  \n","Epoch: [5][100/2479] Elapsed 0m 25s (remain 9m 59s) Loss: 0.3579(0.5027) Grad: 29371.2891  LR: 0.00000133  \n","Epoch: [5][200/2479] Elapsed 0m 50s (remain 9m 31s) Loss: 0.4824(0.4967) Grad: 23701.7031  LR: 0.00000122  \n","Epoch: [5][300/2479] Elapsed 1m 15s (remain 9m 4s) Loss: 0.5425(0.4958) Grad: 42201.4297  LR: 0.00000112  \n","Epoch: [5][400/2479] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5324(0.4907) Grad: 20459.2031  LR: 0.00000102  \n","Epoch: [5][500/2479] Elapsed 2m 4s (remain 8m 13s) Loss: 0.5614(0.4909) Grad: 50945.7383  LR: 0.00000093  \n","Epoch: [5][600/2479] Elapsed 2m 29s (remain 7m 48s) Loss: 0.4315(0.4915) Grad: 37627.6445  LR: 0.00000084  \n","Epoch: [5][700/2479] Elapsed 2m 54s (remain 7m 22s) Loss: 0.4892(0.4927) Grad: 77295.1875  LR: 0.00000075  \n","Epoch: [5][800/2479] Elapsed 3m 19s (remain 6m 57s) Loss: 0.5900(0.4939) Grad: 434345.2812  LR: 0.00000067  \n","Epoch: [5][900/2479] Elapsed 3m 44s (remain 6m 32s) Loss: 0.4295(0.4945) Grad: 54260.3164  LR: 0.00000059  \n","Epoch: [5][1000/2479] Elapsed 4m 9s (remain 6m 7s) Loss: 0.5425(0.4947) Grad: 137826.6406  LR: 0.00000052  \n","Epoch: [5][1100/2479] Elapsed 4m 33s (remain 5m 42s) Loss: 0.4394(0.4933) Grad: 79825.7656  LR: 0.00000046  \n","Epoch: [5][1200/2479] Elapsed 4m 58s (remain 5m 18s) Loss: 0.5942(0.4928) Grad: 115039.7109  LR: 0.00000039  \n","Epoch: [5][1300/2479] Elapsed 5m 24s (remain 4m 53s) Loss: 0.6059(0.4930) Grad: 33798.9727  LR: 0.00000033  \n","Epoch: [5][1400/2479] Elapsed 5m 48s (remain 4m 28s) Loss: 0.5246(0.4928) Grad: 37450.6797  LR: 0.00000028  \n","Epoch: [5][1500/2479] Elapsed 6m 13s (remain 4m 3s) Loss: 0.4408(0.4929) Grad: 14639.9795  LR: 0.00000023  \n","Epoch: [5][1600/2479] Elapsed 6m 38s (remain 3m 38s) Loss: 0.4225(0.4926) Grad: 30388.8633  LR: 0.00000019  \n","Epoch: [5][1700/2479] Elapsed 7m 3s (remain 3m 13s) Loss: 0.5321(0.4919) Grad: 15495.8115  LR: 0.00000015  \n","Epoch: [5][1800/2479] Elapsed 7m 28s (remain 2m 48s) Loss: 0.5396(0.4923) Grad: 44948.9062  LR: 0.00000011  \n","Epoch: [5][1900/2479] Elapsed 7m 53s (remain 2m 23s) Loss: 0.5867(0.4919) Grad: 47738.5625  LR: 0.00000008  \n","Epoch: [5][2000/2479] Elapsed 8m 18s (remain 1m 58s) Loss: 0.3803(0.4922) Grad: 27659.0234  LR: 0.00000006  \n","Epoch: [5][2100/2479] Elapsed 8m 42s (remain 1m 34s) Loss: 0.3850(0.4924) Grad: 105825.5469  LR: 0.00000004  \n","Epoch: [5][2200/2479] Elapsed 9m 7s (remain 1m 9s) Loss: 0.4669(0.4924) Grad: 127359.1172  LR: 0.00000002  \n","Epoch: [5][2300/2479] Elapsed 9m 32s (remain 0m 44s) Loss: 0.4473(0.4925) Grad: 95641.2266  LR: 0.00000001  \n","Epoch: [5][2400/2479] Elapsed 9m 57s (remain 0m 19s) Loss: 0.5645(0.4926) Grad: 82370.0000  LR: 0.00000000  \n","Epoch: [5][2478/2479] Elapsed 10m 16s (remain 0m 0s) Loss: 0.4616(0.4929) Grad: 197848.5000  LR: 0.00000000  \n","EVAL: [0/126] Elapsed 0m 0s (remain 0m 44s) Loss: 0.7155(0.7155) \n","EVAL: [100/126] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5453(0.5900) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4929  avg_val_loss: 0.5967  time: 633s\n","Epoch 5 - Score: 0.8167\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [125/126] Elapsed 0m 16s (remain 0m 0s) Loss: 0.3649(0.5967) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 4 result ==========\n","Score: 0.8226\n","========== fold: 5 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2464] Elapsed 0m 0s (remain 18m 42s) Loss: 0.7006(0.7006) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2464] Elapsed 0m 25s (remain 9m 50s) Loss: 0.5609(0.6415) Grad: 39167.8203  LR: 0.00001500  \n","Epoch: [1][200/2464] Elapsed 0m 49s (remain 9m 22s) Loss: 0.5976(0.6187) Grad: 47091.1250  LR: 0.00001499  \n","Epoch: [1][300/2464] Elapsed 1m 14s (remain 8m 56s) Loss: 0.6473(0.6047) Grad: 43269.7539  LR: 0.00001498  \n","Epoch: [1][400/2464] Elapsed 1m 39s (remain 8m 31s) Loss: 0.5968(0.5924) Grad: 37902.3633  LR: 0.00001496  \n","Epoch: [1][500/2464] Elapsed 2m 4s (remain 8m 6s) Loss: 0.5864(0.5837) Grad: 23366.5488  LR: 0.00001494  \n","Epoch: [1][600/2464] Elapsed 2m 28s (remain 7m 41s) Loss: 0.5603(0.5794) Grad: 39598.6836  LR: 0.00001491  \n","Epoch: [1][700/2464] Elapsed 2m 53s (remain 7m 16s) Loss: 0.4909(0.5745) Grad: 24713.6270  LR: 0.00001488  \n","Epoch: [1][800/2464] Elapsed 3m 18s (remain 6m 51s) Loss: 0.5892(0.5720) Grad: 40065.4805  LR: 0.00001484  \n","Epoch: [1][900/2464] Elapsed 3m 43s (remain 6m 27s) Loss: 0.5201(0.5700) Grad: 52803.6797  LR: 0.00001480  \n","Epoch: [1][1000/2464] Elapsed 4m 7s (remain 6m 2s) Loss: 0.5170(0.5694) Grad: 35940.3867  LR: 0.00001476  \n","Epoch: [1][1100/2464] Elapsed 4m 32s (remain 5m 37s) Loss: 0.5512(0.5680) Grad: 35986.8320  LR: 0.00001471  \n","Epoch: [1][1200/2464] Elapsed 4m 57s (remain 5m 13s) Loss: 0.6897(0.5666) Grad: 56152.1836  LR: 0.00001465  \n","Epoch: [1][1300/2464] Elapsed 5m 22s (remain 4m 48s) Loss: 0.5947(0.5640) Grad: 26495.0059  LR: 0.00001459  \n","Epoch: [1][1400/2464] Elapsed 5m 47s (remain 4m 23s) Loss: 0.5291(0.5641) Grad: 29565.9238  LR: 0.00001453  \n","Epoch: [1][1500/2464] Elapsed 6m 11s (remain 3m 58s) Loss: 0.5055(0.5629) Grad: 32254.8711  LR: 0.00001446  \n","Epoch: [1][1600/2464] Elapsed 6m 36s (remain 3m 33s) Loss: 0.5178(0.5613) Grad: 18145.2559  LR: 0.00001438  \n","Epoch: [1][1700/2464] Elapsed 7m 1s (remain 3m 8s) Loss: 0.5981(0.5601) Grad: 11704.1084  LR: 0.00001431  \n","Epoch: [1][1800/2464] Elapsed 7m 25s (remain 2m 44s) Loss: 0.4587(0.5591) Grad: 67084.6484  LR: 0.00001422  \n","Epoch: [1][1900/2464] Elapsed 7m 50s (remain 2m 19s) Loss: 0.4211(0.5590) Grad: 25708.0762  LR: 0.00001414  \n","Epoch: [1][2000/2464] Elapsed 8m 15s (remain 1m 54s) Loss: 0.6209(0.5595) Grad: 29528.9551  LR: 0.00001404  \n","Epoch: [1][2100/2464] Elapsed 8m 40s (remain 1m 29s) Loss: 0.5936(0.5587) Grad: 79779.1094  LR: 0.00001395  \n","Epoch: [1][2200/2464] Elapsed 9m 4s (remain 1m 5s) Loss: 0.5387(0.5583) Grad: 29287.1758  LR: 0.00001385  \n","Epoch: [1][2300/2464] Elapsed 9m 29s (remain 0m 40s) Loss: 0.6086(0.5579) Grad: 18892.1270  LR: 0.00001375  \n","Epoch: [1][2400/2464] Elapsed 9m 54s (remain 0m 15s) Loss: 0.5252(0.5580) Grad: 14766.9189  LR: 0.00001364  \n","Epoch: [1][2463/2464] Elapsed 10m 9s (remain 0m 0s) Loss: 0.5252(0.5574) Grad: 9050.0107  LR: 0.00001357  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 50s) Loss: 0.5027(0.5027) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5850(0.5482) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5574  avg_val_loss: 0.5496  time: 629s\n","Epoch 1 - Score: 0.8147\n","Epoch 1 - Save Best Score: 0.8147 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3209(0.5496) \n","Epoch: [2][0/2464] Elapsed 0m 0s (remain 24m 40s) Loss: 0.6345(0.6345) Grad: 84784.2109  LR: 0.00001357  \n","Epoch: [2][100/2464] Elapsed 0m 25s (remain 10m 1s) Loss: 0.4863(0.5288) Grad: 65978.3750  LR: 0.00001345  \n","Epoch: [2][200/2464] Elapsed 0m 50s (remain 9m 31s) Loss: 0.5936(0.5311) Grad: 175948.8594  LR: 0.00001333  \n","Epoch: [2][300/2464] Elapsed 1m 15s (remain 9m 3s) Loss: 0.6212(0.5286) Grad: 190706.0000  LR: 0.00001321  \n","Epoch: [2][400/2464] Elapsed 1m 40s (remain 8m 36s) Loss: 0.5802(0.5311) Grad: 68120.5781  LR: 0.00001309  \n","Epoch: [2][500/2464] Elapsed 2m 5s (remain 8m 10s) Loss: 0.6246(0.5302) Grad: 43591.0938  LR: 0.00001296  \n","Epoch: [2][600/2464] Elapsed 2m 29s (remain 7m 44s) Loss: 0.4275(0.5270) Grad: 11328.7129  LR: 0.00001282  \n","Epoch: [2][700/2464] Elapsed 2m 54s (remain 7m 19s) Loss: 0.4243(0.5265) Grad: 44495.0312  LR: 0.00001269  \n","Epoch: [2][800/2464] Elapsed 3m 19s (remain 6m 53s) Loss: 0.5700(0.5246) Grad: 28566.4961  LR: 0.00001255  \n","Epoch: [2][900/2464] Elapsed 3m 44s (remain 6m 28s) Loss: 0.6214(0.5244) Grad: 45340.6719  LR: 0.00001240  \n","Epoch: [2][1000/2464] Elapsed 4m 8s (remain 6m 3s) Loss: 0.4872(0.5232) Grad: 19431.7031  LR: 0.00001226  \n","Epoch: [2][1100/2464] Elapsed 4m 33s (remain 5m 38s) Loss: 0.5036(0.5223) Grad: 34047.7344  LR: 0.00001211  \n","Epoch: [2][1200/2464] Elapsed 4m 58s (remain 5m 13s) Loss: 0.5401(0.5210) Grad: 36294.1602  LR: 0.00001196  \n","Epoch: [2][1300/2464] Elapsed 5m 22s (remain 4m 48s) Loss: 0.5701(0.5210) Grad: 5953.8311  LR: 0.00001180  \n","Epoch: [2][1400/2464] Elapsed 5m 47s (remain 4m 23s) Loss: 0.4727(0.5218) Grad: 62540.8594  LR: 0.00001164  \n","Epoch: [2][1500/2464] Elapsed 6m 12s (remain 3m 58s) Loss: 0.5694(0.5220) Grad: 17002.5723  LR: 0.00001148  \n","Epoch: [2][1600/2464] Elapsed 6m 37s (remain 3m 34s) Loss: 0.5427(0.5218) Grad: 8063.8647  LR: 0.00001132  \n","Epoch: [2][1700/2464] Elapsed 7m 1s (remain 3m 9s) Loss: 0.4547(0.5220) Grad: 7517.3970  LR: 0.00001115  \n","Epoch: [2][1800/2464] Elapsed 7m 26s (remain 2m 44s) Loss: 0.4657(0.5222) Grad: 9692.7021  LR: 0.00001098  \n","Epoch: [2][1900/2464] Elapsed 7m 51s (remain 2m 19s) Loss: 0.5347(0.5228) Grad: 13822.2861  LR: 0.00001081  \n","Epoch: [2][2000/2464] Elapsed 8m 16s (remain 1m 54s) Loss: 0.5022(0.5231) Grad: 13819.7520  LR: 0.00001064  \n","Epoch: [2][2100/2464] Elapsed 8m 40s (remain 1m 29s) Loss: 0.3480(0.5226) Grad: 16958.6973  LR: 0.00001047  \n","Epoch: [2][2200/2464] Elapsed 9m 5s (remain 1m 5s) Loss: 0.4640(0.5226) Grad: 2279.7346  LR: 0.00001029  \n","Epoch: [2][2300/2464] Elapsed 9m 30s (remain 0m 40s) Loss: 0.5792(0.5224) Grad: 43121.9336  LR: 0.00001011  \n","Epoch: [2][2400/2464] Elapsed 9m 55s (remain 0m 15s) Loss: 0.6764(0.5229) Grad: 35925.5586  LR: 0.00000993  \n","Epoch: [2][2463/2464] Elapsed 10m 10s (remain 0m 0s) Loss: 0.4991(0.5231) Grad: 35759.5586  LR: 0.00000982  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 49s) Loss: 0.5245(0.5245) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5662(0.5591) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5231  avg_val_loss: 0.5557  time: 629s\n","Epoch 2 - Score: 0.8085\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1079(0.5557) \n","Epoch: [3][0/2464] Elapsed 0m 0s (remain 24m 4s) Loss: 0.6319(0.6319) Grad: 145651.3281  LR: 0.00000982  \n","Epoch: [3][100/2464] Elapsed 0m 25s (remain 9m 51s) Loss: 0.3614(0.5180) Grad: 16880.9707  LR: 0.00000963  \n","Epoch: [3][200/2464] Elapsed 0m 50s (remain 9m 24s) Loss: 0.4413(0.5066) Grad: 11535.5098  LR: 0.00000945  \n","Epoch: [3][300/2464] Elapsed 1m 14s (remain 8m 58s) Loss: 0.5392(0.5066) Grad: 6778.5776  LR: 0.00000926  \n","Epoch: [3][400/2464] Elapsed 1m 39s (remain 8m 33s) Loss: 0.5343(0.5087) Grad: 14718.2734  LR: 0.00000908  \n","Epoch: [3][500/2464] Elapsed 2m 4s (remain 8m 8s) Loss: 0.4626(0.5106) Grad: 5719.0361  LR: 0.00000889  \n","Epoch: [3][600/2464] Elapsed 2m 29s (remain 7m 43s) Loss: 0.5528(0.5114) Grad: 33806.4727  LR: 0.00000870  \n","Epoch: [3][700/2464] Elapsed 2m 54s (remain 7m 18s) Loss: 0.5230(0.5100) Grad: 10495.2256  LR: 0.00000851  \n","Epoch: [3][800/2464] Elapsed 3m 19s (remain 6m 53s) Loss: 0.5562(0.5097) Grad: 6155.2837  LR: 0.00000832  \n","Epoch: [3][900/2464] Elapsed 3m 43s (remain 6m 28s) Loss: 0.4488(0.5097) Grad: 10573.2969  LR: 0.00000813  \n","Epoch: [3][1000/2464] Elapsed 4m 8s (remain 6m 3s) Loss: 0.4796(0.5092) Grad: 29841.5020  LR: 0.00000794  \n","Epoch: [3][1100/2464] Elapsed 4m 33s (remain 5m 38s) Loss: 0.5529(0.5084) Grad: 13264.4121  LR: 0.00000775  \n","Epoch: [3][1200/2464] Elapsed 4m 58s (remain 5m 13s) Loss: 0.3704(0.5087) Grad: 2535.6770  LR: 0.00000756  \n","Epoch: [3][1300/2464] Elapsed 5m 23s (remain 4m 48s) Loss: 0.5793(0.5078) Grad: 20386.0156  LR: 0.00000737  \n","Epoch: [3][1400/2464] Elapsed 5m 48s (remain 4m 24s) Loss: 0.4713(0.5080) Grad: 19787.8477  LR: 0.00000718  \n","Epoch: [3][1500/2464] Elapsed 6m 12s (remain 3m 59s) Loss: 0.5423(0.5069) Grad: 13715.8027  LR: 0.00000699  \n","Epoch: [3][1600/2464] Elapsed 6m 37s (remain 3m 34s) Loss: 0.5489(0.5073) Grad: 92377.6172  LR: 0.00000680  \n","Epoch: [3][1700/2464] Elapsed 7m 2s (remain 3m 9s) Loss: 0.4771(0.5075) Grad: 4415.8828  LR: 0.00000661  \n","Epoch: [3][1800/2464] Elapsed 7m 27s (remain 2m 44s) Loss: 0.5716(0.5071) Grad: 16417.0000  LR: 0.00000642  \n","Epoch: [3][1900/2464] Elapsed 7m 51s (remain 2m 19s) Loss: 0.5386(0.5079) Grad: 7092.1470  LR: 0.00000623  \n","Epoch: [3][2000/2464] Elapsed 8m 16s (remain 1m 54s) Loss: 0.6051(0.5080) Grad: 26262.3535  LR: 0.00000604  \n","Epoch: [3][2100/2464] Elapsed 8m 41s (remain 1m 30s) Loss: 0.6030(0.5074) Grad: 28141.9004  LR: 0.00000585  \n","Epoch: [3][2200/2464] Elapsed 9m 6s (remain 1m 5s) Loss: 0.6545(0.5071) Grad: 65779.0781  LR: 0.00000567  \n","Epoch: [3][2300/2464] Elapsed 9m 31s (remain 0m 40s) Loss: 0.4345(0.5074) Grad: 74312.8672  LR: 0.00000548  \n","Epoch: [3][2400/2464] Elapsed 9m 55s (remain 0m 15s) Loss: 0.4677(0.5077) Grad: 18966.0469  LR: 0.00000530  \n","Epoch: [3][2463/2464] Elapsed 10m 11s (remain 0m 0s) Loss: 0.4486(0.5074) Grad: 17672.1973  LR: 0.00000518  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 50s) Loss: 0.5003(0.5003) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5624(0.5599) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5074  avg_val_loss: 0.5623  time: 630s\n","Epoch 3 - Score: 0.8216\n","Epoch 3 - Save Best Score: 0.8216 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.0067(0.5623) \n","Epoch: [4][0/2464] Elapsed 0m 0s (remain 23m 51s) Loss: 0.5051(0.5051) Grad: 45844.1016  LR: 0.00000518  \n","Epoch: [4][100/2464] Elapsed 0m 25s (remain 10m 1s) Loss: 0.3987(0.4979) Grad: 82595.2500  LR: 0.00000500  \n","Epoch: [4][200/2464] Elapsed 0m 50s (remain 9m 32s) Loss: 0.4739(0.5063) Grad: 27951.8691  LR: 0.00000482  \n","Epoch: [4][300/2464] Elapsed 1m 15s (remain 9m 3s) Loss: 0.4746(0.5038) Grad: 40662.4180  LR: 0.00000464  \n","Epoch: [4][400/2464] Elapsed 1m 40s (remain 8m 36s) Loss: 0.5350(0.5026) Grad: 19594.0234  LR: 0.00000447  \n","Epoch: [4][500/2464] Elapsed 2m 5s (remain 8m 10s) Loss: 0.4765(0.5007) Grad: 44820.8359  LR: 0.00000429  \n","Epoch: [4][600/2464] Elapsed 2m 29s (remain 7m 44s) Loss: 0.5519(0.4990) Grad: 12964.9951  LR: 0.00000412  \n","Epoch: [4][700/2464] Elapsed 2m 54s (remain 7m 19s) Loss: 0.4118(0.4978) Grad: 9502.9199  LR: 0.00000395  \n","Epoch: [4][800/2464] Elapsed 3m 19s (remain 6m 54s) Loss: 0.4872(0.4979) Grad: 13554.3398  LR: 0.00000378  \n","Epoch: [4][900/2464] Elapsed 3m 44s (remain 6m 28s) Loss: 0.4978(0.4992) Grad: 20398.7090  LR: 0.00000362  \n","Epoch: [4][1000/2464] Elapsed 4m 8s (remain 6m 3s) Loss: 0.4708(0.4997) Grad: 85089.4531  LR: 0.00000346  \n","Epoch: [4][1100/2464] Elapsed 4m 33s (remain 5m 38s) Loss: 0.4190(0.4988) Grad: 63414.8242  LR: 0.00000330  \n","Epoch: [4][1200/2464] Elapsed 4m 58s (remain 5m 13s) Loss: 0.5453(0.4990) Grad: 29377.0430  LR: 0.00000314  \n","Epoch: [4][1300/2464] Elapsed 5m 23s (remain 4m 49s) Loss: 0.4933(0.4984) Grad: 12336.4219  LR: 0.00000299  \n","Epoch: [4][1400/2464] Elapsed 5m 48s (remain 4m 24s) Loss: 0.4203(0.4977) Grad: 20352.3984  LR: 0.00000283  \n","Epoch: [4][1500/2464] Elapsed 6m 13s (remain 3m 59s) Loss: 0.4136(0.4979) Grad: 35736.3438  LR: 0.00000269  \n","Epoch: [4][1600/2464] Elapsed 6m 37s (remain 3m 34s) Loss: 0.4505(0.4978) Grad: 19510.6133  LR: 0.00000254  \n","Epoch: [4][1700/2464] Elapsed 7m 2s (remain 3m 9s) Loss: 0.3067(0.4975) Grad: 12848.8623  LR: 0.00000240  \n","Epoch: [4][1800/2464] Elapsed 7m 27s (remain 2m 44s) Loss: 0.5210(0.4971) Grad: 83508.4766  LR: 0.00000226  \n","Epoch: [4][1900/2464] Elapsed 7m 52s (remain 2m 19s) Loss: 0.5368(0.4969) Grad: 54101.5781  LR: 0.00000213  \n","Epoch: [4][2000/2464] Elapsed 8m 16s (remain 1m 54s) Loss: 0.5680(0.4971) Grad: 23888.3828  LR: 0.00000199  \n","Epoch: [4][2100/2464] Elapsed 8m 41s (remain 1m 30s) Loss: 0.4572(0.4974) Grad: 8318.0830  LR: 0.00000187  \n","Epoch: [4][2200/2464] Elapsed 9m 6s (remain 1m 5s) Loss: 0.4181(0.4976) Grad: 20377.8750  LR: 0.00000174  \n","Epoch: [4][2300/2464] Elapsed 9m 31s (remain 0m 40s) Loss: 0.4978(0.4974) Grad: 20027.6934  LR: 0.00000162  \n","Epoch: [4][2400/2464] Elapsed 9m 55s (remain 0m 15s) Loss: 0.5094(0.4973) Grad: 60667.2266  LR: 0.00000150  \n","Epoch: [4][2463/2464] Elapsed 10m 11s (remain 0m 0s) Loss: 0.4972(0.4975) Grad: 15031.1035  LR: 0.00000143  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 51s) Loss: 0.5029(0.5029) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5587(0.5781) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4975  avg_val_loss: 0.5785  time: 630s\n","Epoch 4 - Score: 0.8165\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.0562(0.5785) \n","Epoch: [5][0/2464] Elapsed 0m 0s (remain 24m 1s) Loss: 0.4660(0.4660) Grad: 59362.5938  LR: 0.00000143  \n","Epoch: [5][100/2464] Elapsed 0m 25s (remain 9m 53s) Loss: 0.4125(0.5010) Grad: 27834.6543  LR: 0.00000132  \n","Epoch: [5][200/2464] Elapsed 0m 50s (remain 9m 24s) Loss: 0.4157(0.4968) Grad: 76384.1719  LR: 0.00000121  \n","Epoch: [5][300/2464] Elapsed 1m 14s (remain 8m 58s) Loss: 0.4712(0.4966) Grad: 59118.7422  LR: 0.00000111  \n","Epoch: [5][400/2464] Elapsed 1m 39s (remain 8m 32s) Loss: 0.5657(0.4963) Grad: 82730.9219  LR: 0.00000101  \n","Epoch: [5][500/2464] Elapsed 2m 4s (remain 8m 7s) Loss: 0.4820(0.4947) Grad: 276619.1875  LR: 0.00000092  \n","Epoch: [5][600/2464] Elapsed 2m 29s (remain 7m 42s) Loss: 0.4955(0.4932) Grad: 43317.8477  LR: 0.00000083  \n","Epoch: [5][700/2464] Elapsed 2m 54s (remain 7m 18s) Loss: 0.3626(0.4939) Grad: 18851.0703  LR: 0.00000075  \n","Epoch: [5][800/2464] Elapsed 3m 18s (remain 6m 53s) Loss: 0.6035(0.4952) Grad: 85707.6562  LR: 0.00000066  \n","Epoch: [5][900/2464] Elapsed 3m 43s (remain 6m 28s) Loss: 0.3282(0.4927) Grad: 183009.0781  LR: 0.00000059  \n","Epoch: [5][1000/2464] Elapsed 4m 8s (remain 6m 3s) Loss: 0.3660(0.4928) Grad: 31356.6172  LR: 0.00000052  \n","Epoch: [5][1100/2464] Elapsed 4m 33s (remain 5m 38s) Loss: 0.3701(0.4935) Grad: 16273.1240  LR: 0.00000045  \n","Epoch: [5][1200/2464] Elapsed 4m 58s (remain 5m 13s) Loss: 0.5355(0.4929) Grad: 21960.5605  LR: 0.00000039  \n","Epoch: [5][1300/2464] Elapsed 5m 23s (remain 4m 48s) Loss: 0.5432(0.4923) Grad: 27629.4238  LR: 0.00000033  \n","Epoch: [5][1400/2464] Elapsed 5m 47s (remain 4m 24s) Loss: 0.4081(0.4924) Grad: 54913.6172  LR: 0.00000027  \n","Epoch: [5][1500/2464] Elapsed 6m 12s (remain 3m 59s) Loss: 0.5569(0.4921) Grad: 43717.7383  LR: 0.00000022  \n","Epoch: [5][1600/2464] Elapsed 6m 37s (remain 3m 34s) Loss: 0.5187(0.4923) Grad: 39386.6211  LR: 0.00000018  \n","Epoch: [5][1700/2464] Elapsed 7m 2s (remain 3m 9s) Loss: 0.4098(0.4921) Grad: 25387.7051  LR: 0.00000014  \n","Epoch: [5][1800/2464] Elapsed 7m 27s (remain 2m 44s) Loss: 0.5745(0.4925) Grad: 81912.3359  LR: 0.00000011  \n","Epoch: [5][1900/2464] Elapsed 7m 51s (remain 2m 19s) Loss: 0.4527(0.4928) Grad: 37401.1875  LR: 0.00000008  \n","Epoch: [5][2000/2464] Elapsed 8m 16s (remain 1m 54s) Loss: 0.5588(0.4928) Grad: 68206.0547  LR: 0.00000005  \n","Epoch: [5][2100/2464] Elapsed 8m 41s (remain 1m 30s) Loss: 0.4003(0.4929) Grad: 29119.4766  LR: 0.00000003  \n","Epoch: [5][2200/2464] Elapsed 9m 6s (remain 1m 5s) Loss: 0.5341(0.4930) Grad: 232974.8594  LR: 0.00000002  \n","Epoch: [5][2300/2464] Elapsed 9m 30s (remain 0m 40s) Loss: 0.6164(0.4922) Grad: 16889.4023  LR: 0.00000001  \n","Epoch: [5][2400/2464] Elapsed 9m 55s (remain 0m 15s) Loss: 0.4495(0.4925) Grad: 1416.0699  LR: 0.00000000  \n","Epoch: [5][2463/2464] Elapsed 10m 11s (remain 0m 0s) Loss: 0.5901(0.4927) Grad: 5571.5249  LR: 0.00000000  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 49s) Loss: 0.5007(0.5007) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5578(0.5823) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4927  avg_val_loss: 0.5837  time: 630s\n","Epoch 5 - Score: 0.8147\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.0729(0.5837) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 5 result ==========\n","Score: 0.8216\n","========== fold: 6 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2487] Elapsed 0m 0s (remain 19m 0s) Loss: 0.8140(0.8140) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2487] Elapsed 0m 25s (remain 9m 58s) Loss: 0.6322(0.6382) Grad: 50033.2773  LR: 0.00001500  \n","Epoch: [1][200/2487] Elapsed 0m 50s (remain 9m 29s) Loss: 0.6486(0.6181) Grad: 110825.9062  LR: 0.00001499  \n","Epoch: [1][300/2487] Elapsed 1m 14s (remain 9m 3s) Loss: 0.5849(0.6054) Grad: 71747.1328  LR: 0.00001498  \n","Epoch: [1][400/2487] Elapsed 1m 39s (remain 8m 38s) Loss: 0.5686(0.5933) Grad: 42890.3750  LR: 0.00001496  \n","Epoch: [1][500/2487] Elapsed 2m 4s (remain 8m 12s) Loss: 0.5777(0.5875) Grad: 40970.4258  LR: 0.00001494  \n","Epoch: [1][600/2487] Elapsed 2m 29s (remain 7m 47s) Loss: 0.4750(0.5818) Grad: 42729.1289  LR: 0.00001491  \n","Epoch: [1][700/2487] Elapsed 2m 53s (remain 7m 22s) Loss: 0.5697(0.5803) Grad: 35776.5977  LR: 0.00001488  \n","Epoch: [1][800/2487] Elapsed 3m 18s (remain 6m 58s) Loss: 0.4747(0.5793) Grad: 9616.9805  LR: 0.00001485  \n","Epoch: [1][900/2487] Elapsed 3m 43s (remain 6m 33s) Loss: 0.6290(0.5770) Grad: 14586.4678  LR: 0.00001481  \n","Epoch: [1][1000/2487] Elapsed 4m 8s (remain 6m 8s) Loss: 0.5091(0.5750) Grad: 8759.8701  LR: 0.00001476  \n","Epoch: [1][1100/2487] Elapsed 4m 32s (remain 5m 43s) Loss: 0.5845(0.5729) Grad: 61227.5117  LR: 0.00001471  \n","Epoch: [1][1200/2487] Elapsed 4m 58s (remain 5m 19s) Loss: 0.6014(0.5709) Grad: 13040.8203  LR: 0.00001466  \n","Epoch: [1][1300/2487] Elapsed 5m 22s (remain 4m 54s) Loss: 0.7103(0.5693) Grad: 18462.4844  LR: 0.00001460  \n","Epoch: [1][1400/2487] Elapsed 5m 47s (remain 4m 29s) Loss: 0.6314(0.5678) Grad: 45546.2188  LR: 0.00001454  \n","Epoch: [1][1500/2487] Elapsed 6m 12s (remain 4m 4s) Loss: 0.4810(0.5660) Grad: 26667.2324  LR: 0.00001447  \n","Epoch: [1][1600/2487] Elapsed 6m 36s (remain 3m 39s) Loss: 0.3772(0.5643) Grad: 9453.6914  LR: 0.00001439  \n","Epoch: [1][1700/2487] Elapsed 7m 1s (remain 3m 14s) Loss: 0.6226(0.5636) Grad: 35404.1445  LR: 0.00001432  \n","Epoch: [1][1800/2487] Elapsed 7m 26s (remain 2m 50s) Loss: 0.5284(0.5623) Grad: 9373.7070  LR: 0.00001424  \n","Epoch: [1][1900/2487] Elapsed 7m 51s (remain 2m 25s) Loss: 0.4688(0.5614) Grad: 25490.4961  LR: 0.00001415  \n","Epoch: [1][2000/2487] Elapsed 8m 15s (remain 2m 0s) Loss: 0.6143(0.5606) Grad: 15133.9775  LR: 0.00001406  \n","Epoch: [1][2100/2487] Elapsed 8m 40s (remain 1m 35s) Loss: 0.5284(0.5597) Grad: 10813.8877  LR: 0.00001397  \n","Epoch: [1][2200/2487] Elapsed 9m 5s (remain 1m 10s) Loss: 0.5303(0.5584) Grad: 20036.0508  LR: 0.00001387  \n","Epoch: [1][2300/2487] Elapsed 9m 29s (remain 0m 46s) Loss: 0.5873(0.5579) Grad: 12142.8125  LR: 0.00001377  \n","Epoch: [1][2400/2487] Elapsed 9m 54s (remain 0m 21s) Loss: 0.5786(0.5574) Grad: 7087.7119  LR: 0.00001366  \n","Epoch: [1][2486/2487] Elapsed 10m 15s (remain 0m 0s) Loss: 0.6375(0.5569) Grad: 4226.4243  LR: 0.00001357  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 41s) Loss: 0.4780(0.4780) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.6856(0.5526) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5569  avg_val_loss: 0.5555  time: 632s\n","Epoch 1 - Score: 0.8005\n","Epoch 1 - Save Best Score: 0.8005 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6539(0.5555) \n","Epoch: [2][0/2487] Elapsed 0m 0s (remain 25m 1s) Loss: 0.4624(0.4624) Grad: 71226.6328  LR: 0.00001357  \n","Epoch: [2][100/2487] Elapsed 0m 25s (remain 10m 4s) Loss: 0.5427(0.5081) Grad: 66495.6562  LR: 0.00001345  \n","Epoch: [2][200/2487] Elapsed 0m 50s (remain 9m 35s) Loss: 0.4626(0.5105) Grad: 129920.3516  LR: 0.00001334  \n","Epoch: [2][300/2487] Elapsed 1m 15s (remain 9m 7s) Loss: 0.4388(0.5126) Grad: 91130.5078  LR: 0.00001322  \n","Epoch: [2][400/2487] Elapsed 1m 40s (remain 8m 40s) Loss: 0.5394(0.5127) Grad: 46744.2148  LR: 0.00001309  \n","Epoch: [2][500/2487] Elapsed 2m 4s (remain 8m 15s) Loss: 0.5274(0.5148) Grad: 91782.4453  LR: 0.00001296  \n","Epoch: [2][600/2487] Elapsed 2m 29s (remain 7m 49s) Loss: 0.4144(0.5152) Grad: 85280.6406  LR: 0.00001283  \n","Epoch: [2][700/2487] Elapsed 2m 54s (remain 7m 24s) Loss: 0.5460(0.5155) Grad: 450065.8125  LR: 0.00001270  \n","Epoch: [2][800/2487] Elapsed 3m 19s (remain 6m 59s) Loss: 0.3825(0.5159) Grad: 192653.5938  LR: 0.00001256  \n","Epoch: [2][900/2487] Elapsed 3m 43s (remain 6m 34s) Loss: 0.5519(0.5163) Grad: 70179.0078  LR: 0.00001242  \n","Epoch: [2][1000/2487] Elapsed 4m 8s (remain 6m 9s) Loss: 0.4672(0.5157) Grad: 67927.3281  LR: 0.00001227  \n","Epoch: [2][1100/2487] Elapsed 4m 33s (remain 5m 44s) Loss: 0.6045(0.5169) Grad: 71480.9219  LR: 0.00001212  \n","Epoch: [2][1200/2487] Elapsed 4m 58s (remain 5m 19s) Loss: 0.6695(0.5177) Grad: 140695.2812  LR: 0.00001197  \n","Epoch: [2][1300/2487] Elapsed 5m 23s (remain 4m 54s) Loss: 0.5026(0.5175) Grad: 32888.4297  LR: 0.00001182  \n","Epoch: [2][1400/2487] Elapsed 5m 48s (remain 4m 29s) Loss: 0.5101(0.5174) Grad: 25903.8613  LR: 0.00001166  \n","Epoch: [2][1500/2487] Elapsed 6m 12s (remain 4m 4s) Loss: 0.4870(0.5184) Grad: 26848.8281  LR: 0.00001150  \n","Epoch: [2][1600/2487] Elapsed 6m 37s (remain 3m 40s) Loss: 0.5269(0.5177) Grad: 22058.2109  LR: 0.00001134  \n","Epoch: [2][1700/2487] Elapsed 7m 2s (remain 3m 15s) Loss: 0.5628(0.5174) Grad: 15893.4463  LR: 0.00001118  \n","Epoch: [2][1800/2487] Elapsed 7m 27s (remain 2m 50s) Loss: 0.4806(0.5166) Grad: 54146.9922  LR: 0.00001101  \n","Epoch: [2][1900/2487] Elapsed 7m 51s (remain 2m 25s) Loss: 0.5079(0.5168) Grad: 240109.4688  LR: 0.00001084  \n","Epoch: [2][2000/2487] Elapsed 8m 16s (remain 2m 0s) Loss: 0.4993(0.5168) Grad: 54653.2773  LR: 0.00001067  \n","Epoch: [2][2100/2487] Elapsed 8m 41s (remain 1m 35s) Loss: 0.5348(0.5169) Grad: 12843.9033  LR: 0.00001050  \n","Epoch: [2][2200/2487] Elapsed 9m 6s (remain 1m 10s) Loss: 0.5103(0.5165) Grad: 84824.8672  LR: 0.00001033  \n","Epoch: [2][2300/2487] Elapsed 9m 30s (remain 0m 46s) Loss: 0.5961(0.5162) Grad: 12396.5762  LR: 0.00001015  \n","Epoch: [2][2400/2487] Elapsed 9m 55s (remain 0m 21s) Loss: 0.5547(0.5160) Grad: 13780.1738  LR: 0.00000997  \n","Epoch: [2][2486/2487] Elapsed 10m 16s (remain 0m 0s) Loss: 0.5925(0.5163) Grad: 18620.2949  LR: 0.00000982  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 41s) Loss: 0.4939(0.4939) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.6967(0.5618) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5163  avg_val_loss: 0.5668  time: 633s\n","Epoch 2 - Score: 0.8062\n","Epoch 2 - Save Best Score: 0.8062 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6598(0.5668) \n","Epoch: [3][0/2487] Elapsed 0m 0s (remain 23m 42s) Loss: 0.4456(0.4456) Grad: 75766.9453  LR: 0.00000982  \n","Epoch: [3][100/2487] Elapsed 0m 25s (remain 10m 1s) Loss: 0.4807(0.5132) Grad: 58313.8906  LR: 0.00000963  \n","Epoch: [3][200/2487] Elapsed 0m 50s (remain 9m 34s) Loss: 0.6252(0.5126) Grad: 41574.4805  LR: 0.00000945  \n","Epoch: [3][300/2487] Elapsed 1m 15s (remain 9m 5s) Loss: 0.4887(0.5107) Grad: 47971.2461  LR: 0.00000927  \n","Epoch: [3][400/2487] Elapsed 1m 39s (remain 8m 39s) Loss: 0.4241(0.5085) Grad: 93597.7578  LR: 0.00000908  \n","Epoch: [3][500/2487] Elapsed 2m 4s (remain 8m 14s) Loss: 0.4850(0.5085) Grad: 48804.0352  LR: 0.00000890  \n","Epoch: [3][600/2487] Elapsed 2m 29s (remain 7m 48s) Loss: 0.4909(0.5073) Grad: 44692.9648  LR: 0.00000871  \n","Epoch: [3][700/2487] Elapsed 2m 54s (remain 7m 23s) Loss: 0.4792(0.5083) Grad: 18633.6875  LR: 0.00000852  \n","Epoch: [3][800/2487] Elapsed 3m 18s (remain 6m 58s) Loss: 0.5866(0.5061) Grad: 67439.6484  LR: 0.00000834  \n","Epoch: [3][900/2487] Elapsed 3m 43s (remain 6m 33s) Loss: 0.5623(0.5056) Grad: 67869.0234  LR: 0.00000815  \n","Epoch: [3][1000/2487] Elapsed 4m 8s (remain 6m 8s) Loss: 0.5644(0.5049) Grad: 19078.3535  LR: 0.00000796  \n","Epoch: [3][1100/2487] Elapsed 4m 32s (remain 5m 43s) Loss: 0.3634(0.5057) Grad: 26901.1074  LR: 0.00000777  \n","Epoch: [3][1200/2487] Elapsed 4m 57s (remain 5m 18s) Loss: 0.5477(0.5058) Grad: 33383.5508  LR: 0.00000758  \n","Epoch: [3][1300/2487] Elapsed 5m 22s (remain 4m 54s) Loss: 0.5196(0.5062) Grad: 24628.9629  LR: 0.00000739  \n","Epoch: [3][1400/2487] Elapsed 5m 47s (remain 4m 29s) Loss: 0.5063(0.5061) Grad: 9111.8555  LR: 0.00000720  \n","Epoch: [3][1500/2487] Elapsed 6m 11s (remain 4m 4s) Loss: 0.3912(0.5065) Grad: 5898.2710  LR: 0.00000701  \n","Epoch: [3][1600/2487] Elapsed 6m 36s (remain 3m 39s) Loss: 0.5291(0.5065) Grad: 19107.6211  LR: 0.00000682  \n","Epoch: [3][1700/2487] Elapsed 7m 1s (remain 3m 14s) Loss: 0.5488(0.5063) Grad: 150114.6250  LR: 0.00000664  \n","Epoch: [3][1800/2487] Elapsed 7m 26s (remain 2m 49s) Loss: 0.6055(0.5063) Grad: 20152.4453  LR: 0.00000645  \n","Epoch: [3][1900/2487] Elapsed 7m 50s (remain 2m 25s) Loss: 0.5680(0.5063) Grad: 30352.7402  LR: 0.00000626  \n","Epoch: [3][2000/2487] Elapsed 8m 15s (remain 2m 0s) Loss: 0.5362(0.5066) Grad: 29446.6738  LR: 0.00000607  \n","Epoch: [3][2100/2487] Elapsed 8m 40s (remain 1m 35s) Loss: 0.5815(0.5064) Grad: 38769.1250  LR: 0.00000589  \n","Epoch: [3][2200/2487] Elapsed 9m 4s (remain 1m 10s) Loss: 0.4137(0.5068) Grad: 19015.4375  LR: 0.00000570  \n","Epoch: [3][2300/2487] Elapsed 9m 29s (remain 0m 46s) Loss: 0.4193(0.5070) Grad: 116553.0625  LR: 0.00000552  \n","Epoch: [3][2400/2487] Elapsed 9m 54s (remain 0m 21s) Loss: 0.4894(0.5068) Grad: 12864.3838  LR: 0.00000534  \n","Epoch: [3][2486/2487] Elapsed 10m 15s (remain 0m 0s) Loss: 0.5648(0.5065) Grad: 18004.0020  LR: 0.00000518  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 42s) Loss: 0.4626(0.4626) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.6809(0.5698) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5065  avg_val_loss: 0.5770  time: 631s\n","Epoch 3 - Score: 0.8146\n","Epoch 3 - Save Best Score: 0.8146 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6411(0.5770) \n","Epoch: [4][0/2487] Elapsed 0m 0s (remain 24m 12s) Loss: 0.4445(0.4445) Grad: 58710.8633  LR: 0.00000518  \n","Epoch: [4][100/2487] Elapsed 0m 25s (remain 10m 5s) Loss: 0.4766(0.4942) Grad: 31248.8301  LR: 0.00000500  \n","Epoch: [4][200/2487] Elapsed 0m 50s (remain 9m 36s) Loss: 0.6233(0.4956) Grad: 26658.6367  LR: 0.00000482  \n","Epoch: [4][300/2487] Elapsed 1m 15s (remain 9m 7s) Loss: 0.5091(0.5010) Grad: 52695.9961  LR: 0.00000465  \n","Epoch: [4][400/2487] Elapsed 1m 40s (remain 8m 40s) Loss: 0.3689(0.4977) Grad: 97771.0781  LR: 0.00000447  \n","Epoch: [4][500/2487] Elapsed 2m 4s (remain 8m 14s) Loss: 0.4819(0.4945) Grad: 48337.0469  LR: 0.00000430  \n","Epoch: [4][600/2487] Elapsed 2m 29s (remain 7m 49s) Loss: 0.3684(0.4946) Grad: 71867.5625  LR: 0.00000413  \n","Epoch: [4][700/2487] Elapsed 2m 54s (remain 7m 23s) Loss: 0.4807(0.4956) Grad: 24528.6348  LR: 0.00000396  \n","Epoch: [4][800/2487] Elapsed 3m 18s (remain 6m 58s) Loss: 0.3481(0.4958) Grad: 44913.5938  LR: 0.00000380  \n","Epoch: [4][900/2487] Elapsed 3m 43s (remain 6m 33s) Loss: 0.4606(0.4963) Grad: inf  LR: 0.00000363  \n","Epoch: [4][1000/2487] Elapsed 4m 8s (remain 6m 8s) Loss: 0.5930(0.4965) Grad: 71274.0547  LR: 0.00000347  \n","Epoch: [4][1100/2487] Elapsed 4m 33s (remain 5m 43s) Loss: 0.5320(0.4966) Grad: 21962.9883  LR: 0.00000331  \n","Epoch: [4][1200/2487] Elapsed 4m 57s (remain 5m 18s) Loss: 0.5741(0.4963) Grad: 23510.4766  LR: 0.00000316  \n","Epoch: [4][1300/2487] Elapsed 5m 22s (remain 4m 54s) Loss: 0.5596(0.4969) Grad: 19714.1777  LR: 0.00000300  \n","Epoch: [4][1400/2487] Elapsed 5m 47s (remain 4m 29s) Loss: 0.6115(0.4960) Grad: 32425.5605  LR: 0.00000285  \n","Epoch: [4][1500/2487] Elapsed 6m 12s (remain 4m 4s) Loss: 0.4862(0.4965) Grad: 21183.1172  LR: 0.00000271  \n","Epoch: [4][1600/2487] Elapsed 6m 36s (remain 3m 39s) Loss: 0.7086(0.4961) Grad: 211571.6250  LR: 0.00000256  \n","Epoch: [4][1700/2487] Elapsed 7m 1s (remain 3m 14s) Loss: 0.5265(0.4960) Grad: 9548.6855  LR: 0.00000242  \n","Epoch: [4][1800/2487] Elapsed 7m 26s (remain 2m 49s) Loss: 0.5877(0.4963) Grad: 9373.5420  LR: 0.00000228  \n","Epoch: [4][1900/2487] Elapsed 7m 50s (remain 2m 25s) Loss: 0.5940(0.4961) Grad: 19836.6562  LR: 0.00000215  \n","Epoch: [4][2000/2487] Elapsed 8m 15s (remain 2m 0s) Loss: 0.4366(0.4967) Grad: 7391.8564  LR: 0.00000202  \n","Epoch: [4][2100/2487] Elapsed 8m 40s (remain 1m 35s) Loss: 0.4618(0.4967) Grad: 20505.0332  LR: 0.00000189  \n","Epoch: [4][2200/2487] Elapsed 9m 4s (remain 1m 10s) Loss: 0.5684(0.4973) Grad: 41986.5312  LR: 0.00000177  \n","Epoch: [4][2300/2487] Elapsed 9m 29s (remain 0m 46s) Loss: 0.5810(0.4970) Grad: 14961.8857  LR: 0.00000165  \n","Epoch: [4][2400/2487] Elapsed 9m 54s (remain 0m 21s) Loss: 0.4220(0.4972) Grad: 57488.6328  LR: 0.00000153  \n","Epoch: [4][2486/2487] Elapsed 10m 15s (remain 0m 0s) Loss: 0.5128(0.4975) Grad: 9447.4336  LR: 0.00000143  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 42s) Loss: 0.4465(0.4465) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.7027(0.5720) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4975  avg_val_loss: 0.5792  time: 631s\n","Epoch 4 - Score: 0.8193\n","Epoch 4 - Save Best Score: 0.8193 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6332(0.5792) \n","Epoch: [5][0/2487] Elapsed 0m 0s (remain 26m 58s) Loss: 0.4462(0.4462) Grad: 63065.6406  LR: 0.00000143  \n","Epoch: [5][100/2487] Elapsed 0m 25s (remain 10m 6s) Loss: 0.3981(0.4881) Grad: 59376.5977  LR: 0.00000132  \n","Epoch: [5][200/2487] Elapsed 0m 50s (remain 9m 37s) Loss: 0.4045(0.4886) Grad: 85020.2266  LR: 0.00000122  \n","Epoch: [5][300/2487] Elapsed 1m 15s (remain 9m 8s) Loss: 0.5775(0.4874) Grad: 40314.3125  LR: 0.00000111  \n","Epoch: [5][400/2487] Elapsed 1m 40s (remain 8m 41s) Loss: 0.4971(0.4890) Grad: 93475.1797  LR: 0.00000102  \n","Epoch: [5][500/2487] Elapsed 2m 4s (remain 8m 14s) Loss: 0.4570(0.4882) Grad: 13923.9824  LR: 0.00000092  \n","Epoch: [5][600/2487] Elapsed 2m 29s (remain 7m 49s) Loss: 0.4608(0.4909) Grad: 22127.2539  LR: 0.00000084  \n","Epoch: [5][700/2487] Elapsed 2m 54s (remain 7m 23s) Loss: 0.4674(0.4903) Grad: 20389.2734  LR: 0.00000075  \n","Epoch: [5][800/2487] Elapsed 3m 18s (remain 6m 58s) Loss: 0.4807(0.4893) Grad: 24275.4941  LR: 0.00000067  \n","Epoch: [5][900/2487] Elapsed 3m 43s (remain 6m 33s) Loss: 0.4557(0.4898) Grad: 19804.8008  LR: 0.00000059  \n","Epoch: [5][1000/2487] Elapsed 4m 8s (remain 6m 8s) Loss: 0.3825(0.4899) Grad: 12775.3779  LR: 0.00000052  \n","Epoch: [5][1100/2487] Elapsed 4m 33s (remain 5m 43s) Loss: 0.5199(0.4896) Grad: 33696.1758  LR: 0.00000046  \n","Epoch: [5][1200/2487] Elapsed 4m 57s (remain 5m 19s) Loss: 0.5634(0.4900) Grad: 31544.5840  LR: 0.00000039  \n","Epoch: [5][1300/2487] Elapsed 5m 23s (remain 4m 54s) Loss: 0.4851(0.4902) Grad: 14888.0059  LR: 0.00000033  \n","Epoch: [5][1400/2487] Elapsed 5m 47s (remain 4m 29s) Loss: 0.5497(0.4912) Grad: 33916.0664  LR: 0.00000028  \n","Epoch: [5][1500/2487] Elapsed 6m 12s (remain 4m 4s) Loss: 0.7040(0.4916) Grad: 50696.6875  LR: 0.00000023  \n","Epoch: [5][1600/2487] Elapsed 6m 37s (remain 3m 39s) Loss: 0.5028(0.4917) Grad: 132177.4062  LR: 0.00000019  \n","Epoch: [5][1700/2487] Elapsed 7m 1s (remain 3m 14s) Loss: 0.3968(0.4920) Grad: 38923.2773  LR: 0.00000015  \n","Epoch: [5][1800/2487] Elapsed 7m 26s (remain 2m 50s) Loss: 0.4537(0.4915) Grad: 21677.3652  LR: 0.00000011  \n","Epoch: [5][1900/2487] Elapsed 7m 51s (remain 2m 25s) Loss: 0.4418(0.4919) Grad: 7450.4351  LR: 0.00000008  \n","Epoch: [5][2000/2487] Elapsed 8m 16s (remain 2m 0s) Loss: 0.4939(0.4917) Grad: 16601.4570  LR: 0.00000006  \n","Epoch: [5][2100/2487] Elapsed 8m 40s (remain 1m 35s) Loss: 0.6170(0.4922) Grad: 23960.6934  LR: 0.00000004  \n","Epoch: [5][2200/2487] Elapsed 9m 5s (remain 1m 10s) Loss: 0.6108(0.4920) Grad: 18401.2109  LR: 0.00000002  \n","Epoch: [5][2300/2487] Elapsed 9m 30s (remain 0m 46s) Loss: 0.5735(0.4920) Grad: 28049.3008  LR: 0.00000001  \n","Epoch: [5][2400/2487] Elapsed 9m 54s (remain 0m 21s) Loss: 0.4776(0.4921) Grad: 27732.2793  LR: 0.00000000  \n","Epoch: [5][2486/2487] Elapsed 10m 16s (remain 0m 0s) Loss: 0.4643(0.4922) Grad: 31887.7188  LR: 0.00000000  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 40s) Loss: 0.4385(0.4385) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.7056(0.5793) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4922  avg_val_loss: 0.5872  time: 632s\n","Epoch 5 - Score: 0.8166\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6295(0.5872) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 6 result ==========\n","Score: 0.8193\n","========== fold: 7 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2460] Elapsed 0m 0s (remain 21m 37s) Loss: 0.7058(0.7058) Grad: 148615.0469  LR: 0.00001500  \n","Epoch: [1][100/2460] Elapsed 0m 25s (remain 9m 45s) Loss: 0.7334(0.6448) Grad: 159681.3750  LR: 0.00001500  \n","Epoch: [1][200/2460] Elapsed 0m 49s (remain 9m 18s) Loss: 0.4660(0.6193) Grad: 89110.6562  LR: 0.00001499  \n","Epoch: [1][300/2460] Elapsed 1m 14s (remain 8m 52s) Loss: 0.4858(0.5979) Grad: 128351.0156  LR: 0.00001498  \n","Epoch: [1][400/2460] Elapsed 1m 38s (remain 8m 27s) Loss: 0.4750(0.5886) Grad: 45578.8203  LR: 0.00001496  \n","Epoch: [1][500/2460] Elapsed 2m 3s (remain 8m 2s) Loss: 0.5405(0.5847) Grad: 48213.0117  LR: 0.00001494  \n","Epoch: [1][600/2460] Elapsed 2m 27s (remain 7m 37s) Loss: 0.5919(0.5801) Grad: 63030.7109  LR: 0.00001491  \n","Epoch: [1][700/2460] Elapsed 2m 52s (remain 7m 13s) Loss: 0.5187(0.5773) Grad: 69012.6094  LR: 0.00001488  \n","Epoch: [1][800/2460] Elapsed 3m 17s (remain 6m 48s) Loss: 0.5821(0.5752) Grad: 56899.9766  LR: 0.00001484  \n","Epoch: [1][900/2460] Elapsed 3m 41s (remain 6m 24s) Loss: 0.3190(0.5722) Grad: 44090.4570  LR: 0.00001480  \n","Epoch: [1][1000/2460] Elapsed 4m 6s (remain 5m 59s) Loss: 0.6705(0.5702) Grad: 131585.9062  LR: 0.00001476  \n","Epoch: [1][1100/2460] Elapsed 4m 31s (remain 5m 34s) Loss: 0.6059(0.5683) Grad: 47106.7109  LR: 0.00001471  \n","Epoch: [1][1200/2460] Elapsed 4m 56s (remain 5m 10s) Loss: 0.5352(0.5672) Grad: 31635.4570  LR: 0.00001465  \n","Epoch: [1][1300/2460] Elapsed 5m 20s (remain 4m 45s) Loss: 0.5954(0.5655) Grad: 34383.8398  LR: 0.00001459  \n","Epoch: [1][1400/2460] Elapsed 5m 45s (remain 4m 21s) Loss: 0.4346(0.5650) Grad: 66884.8516  LR: 0.00001453  \n","Epoch: [1][1500/2460] Elapsed 6m 10s (remain 3m 56s) Loss: 0.4917(0.5631) Grad: 193580.2656  LR: 0.00001446  \n","Epoch: [1][1600/2460] Elapsed 6m 35s (remain 3m 32s) Loss: 0.4743(0.5623) Grad: 54496.4219  LR: 0.00001438  \n","Epoch: [1][1700/2460] Elapsed 6m 59s (remain 3m 7s) Loss: 0.4176(0.5603) Grad: 33973.1484  LR: 0.00001430  \n","Epoch: [1][1800/2460] Elapsed 7m 24s (remain 2m 42s) Loss: 0.4438(0.5594) Grad: 32634.2246  LR: 0.00001422  \n","Epoch: [1][1900/2460] Elapsed 7m 49s (remain 2m 18s) Loss: 0.5276(0.5580) Grad: 103550.0781  LR: 0.00001413  \n","Epoch: [1][2000/2460] Elapsed 8m 14s (remain 1m 53s) Loss: 0.4881(0.5570) Grad: 50052.5469  LR: 0.00001404  \n","Epoch: [1][2100/2460] Elapsed 8m 38s (remain 1m 28s) Loss: 0.4785(0.5555) Grad: 134227.4062  LR: 0.00001395  \n","Epoch: [1][2200/2460] Elapsed 9m 3s (remain 1m 3s) Loss: 0.5896(0.5545) Grad: 68864.6875  LR: 0.00001385  \n","Epoch: [1][2300/2460] Elapsed 9m 28s (remain 0m 39s) Loss: 0.6868(0.5536) Grad: 572118.2500  LR: 0.00001374  \n","Epoch: [1][2400/2460] Elapsed 9m 52s (remain 0m 14s) Loss: 0.3506(0.5528) Grad: 108573.5547  LR: 0.00001363  \n","Epoch: [1][2459/2460] Elapsed 10m 7s (remain 0m 0s) Loss: 0.5983(0.5519) Grad: 104705.7656  LR: 0.00001357  \n","EVAL: [0/145] Elapsed 0m 0s (remain 0m 51s) Loss: 0.6378(0.6378) \n","EVAL: [100/145] Elapsed 0m 13s (remain 0m 5s) Loss: 0.3756(0.5452) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5519  avg_val_loss: 0.5545  time: 627s\n","Epoch 1 - Score: 0.8121\n","Epoch 1 - Save Best Score: 0.8121 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [144/145] Elapsed 0m 18s (remain 0m 0s) Loss: 0.5732(0.5545) \n","Epoch: [2][0/2460] Elapsed 0m 0s (remain 22m 51s) Loss: 0.5740(0.5740) Grad: 69298.5078  LR: 0.00001357  \n","Epoch: [2][100/2460] Elapsed 0m 25s (remain 9m 56s) Loss: 0.5550(0.5127) Grad: 43843.2188  LR: 0.00001345  \n","Epoch: [2][200/2460] Elapsed 0m 50s (remain 9m 27s) Loss: 0.4586(0.5176) Grad: 48180.1992  LR: 0.00001333  \n","Epoch: [2][300/2460] Elapsed 1m 15s (remain 8m 59s) Loss: 0.4405(0.5165) Grad: 77294.5625  LR: 0.00001321  \n","Epoch: [2][400/2460] Elapsed 1m 40s (remain 8m 34s) Loss: 0.5689(0.5183) Grad: 224552.3594  LR: 0.00001309  \n","Epoch: [2][500/2460] Elapsed 2m 4s (remain 8m 8s) Loss: 0.5871(0.5194) Grad: 71595.2109  LR: 0.00001296  \n","Epoch: [2][600/2460] Elapsed 2m 29s (remain 7m 42s) Loss: 0.5415(0.5179) Grad: 38021.1562  LR: 0.00001282  \n","Epoch: [2][700/2460] Elapsed 2m 54s (remain 7m 17s) Loss: 0.5863(0.5172) Grad: 313169.5312  LR: 0.00001269  \n","Epoch: [2][800/2460] Elapsed 3m 18s (remain 6m 52s) Loss: 0.4830(0.5171) Grad: 60565.8984  LR: 0.00001255  \n","Epoch: [2][900/2460] Elapsed 3m 43s (remain 6m 27s) Loss: 0.3765(0.5178) Grad: 139033.7656  LR: 0.00001240  \n","Epoch: [2][1000/2460] Elapsed 4m 8s (remain 6m 2s) Loss: 0.4988(0.5166) Grad: 47451.9219  LR: 0.00001226  \n","Epoch: [2][1100/2460] Elapsed 4m 33s (remain 5m 37s) Loss: 0.4573(0.5177) Grad: 40638.0742  LR: 0.00001211  \n","Epoch: [2][1200/2460] Elapsed 4m 57s (remain 5m 12s) Loss: 0.5169(0.5176) Grad: 27362.4004  LR: 0.00001195  \n","Epoch: [2][1300/2460] Elapsed 5m 22s (remain 4m 47s) Loss: 0.3615(0.5163) Grad: 150351.3750  LR: 0.00001180  \n","Epoch: [2][1400/2460] Elapsed 5m 47s (remain 4m 22s) Loss: 0.6206(0.5164) Grad: 78468.6328  LR: 0.00001164  \n","Epoch: [2][1500/2460] Elapsed 6m 12s (remain 3m 57s) Loss: 0.3504(0.5165) Grad: 67220.8281  LR: 0.00001148  \n","Epoch: [2][1600/2460] Elapsed 6m 36s (remain 3m 32s) Loss: 0.5230(0.5163) Grad: 65917.0391  LR: 0.00001132  \n","Epoch: [2][1700/2460] Elapsed 7m 1s (remain 3m 8s) Loss: 0.4450(0.5160) Grad: 51392.8281  LR: 0.00001115  \n","Epoch: [2][1800/2460] Elapsed 7m 26s (remain 2m 43s) Loss: 0.6748(0.5164) Grad: 346588.5312  LR: 0.00001098  \n","Epoch: [2][1900/2460] Elapsed 7m 51s (remain 2m 18s) Loss: 0.5244(0.5170) Grad: 105560.5781  LR: 0.00001081  \n","Epoch: [2][2000/2460] Elapsed 8m 15s (remain 1m 53s) Loss: 0.5962(0.5171) Grad: 69519.5391  LR: 0.00001064  \n","Epoch: [2][2100/2460] Elapsed 8m 40s (remain 1m 28s) Loss: 0.4880(0.5174) Grad: 110832.5859  LR: 0.00001046  \n","Epoch: [2][2200/2460] Elapsed 9m 5s (remain 1m 4s) Loss: 0.6310(0.5174) Grad: 331083.1562  LR: 0.00001029  \n","Epoch: [2][2300/2460] Elapsed 9m 29s (remain 0m 39s) Loss: 0.4115(0.5171) Grad: 222694.3906  LR: 0.00001011  \n","Epoch: [2][2400/2460] Elapsed 9m 54s (remain 0m 14s) Loss: 0.4604(0.5170) Grad: 286887.1875  LR: 0.00000993  \n","Epoch: [2][2459/2460] Elapsed 10m 9s (remain 0m 0s) Loss: 0.6013(0.5172) Grad: 67444.4375  LR: 0.00000982  \n","EVAL: [0/145] Elapsed 0m 0s (remain 0m 49s) Loss: 0.6551(0.6551) \n","EVAL: [100/145] Elapsed 0m 13s (remain 0m 5s) Loss: 0.3705(0.5542) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5172  avg_val_loss: 0.5590  time: 628s\n","Epoch 2 - Score: 0.8133\n","Epoch 2 - Save Best Score: 0.8133 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [144/145] Elapsed 0m 18s (remain 0m 0s) Loss: 0.5732(0.5590) \n","Epoch: [3][0/2460] Elapsed 0m 0s (remain 24m 32s) Loss: 0.5505(0.5505) Grad: 83054.7812  LR: 0.00000982  \n","Epoch: [3][100/2460] Elapsed 0m 25s (remain 9m 59s) Loss: 0.4445(0.4976) Grad: 70340.9297  LR: 0.00000963  \n","Epoch: [3][200/2460] Elapsed 0m 50s (remain 9m 32s) Loss: 0.4848(0.5016) Grad: 146851.5781  LR: 0.00000945  \n","Epoch: [3][300/2460] Elapsed 1m 15s (remain 9m 3s) Loss: 0.4229(0.5059) Grad: 122300.0000  LR: 0.00000926  \n","Epoch: [3][400/2460] Elapsed 1m 40s (remain 8m 36s) Loss: 0.4175(0.5037) Grad: 31838.5762  LR: 0.00000908  \n","Epoch: [3][500/2460] Elapsed 2m 5s (remain 8m 9s) Loss: 0.4907(0.5002) Grad: 89929.7656  LR: 0.00000889  \n","Epoch: [3][600/2460] Elapsed 2m 30s (remain 7m 44s) Loss: 0.4533(0.5030) Grad: 84754.3359  LR: 0.00000870  \n","Epoch: [3][700/2460] Elapsed 2m 54s (remain 7m 18s) Loss: 0.5498(0.5037) Grad: 60502.0742  LR: 0.00000851  \n","Epoch: [3][800/2460] Elapsed 3m 19s (remain 6m 53s) Loss: 0.5564(0.5043) Grad: 455316.7812  LR: 0.00000832  \n","Epoch: [3][900/2460] Elapsed 3m 44s (remain 6m 28s) Loss: 0.5747(0.5048) Grad: 1038659.0000  LR: 0.00000813  \n","Epoch: [3][1000/2460] Elapsed 4m 9s (remain 6m 2s) Loss: 0.3307(0.5055) Grad: 107126.9219  LR: 0.00000794  \n","Epoch: [3][1100/2460] Elapsed 4m 33s (remain 5m 37s) Loss: 0.4476(0.5051) Grad: 54367.1602  LR: 0.00000775  \n","Epoch: [3][1200/2460] Elapsed 4m 58s (remain 5m 12s) Loss: 0.5031(0.5052) Grad: 142720.9219  LR: 0.00000756  \n","Epoch: [3][1300/2460] Elapsed 5m 23s (remain 4m 47s) Loss: 0.5083(0.5041) Grad: 95329.0000  LR: 0.00000737  \n","Epoch: [3][1400/2460] Elapsed 5m 48s (remain 4m 23s) Loss: 0.4372(0.5048) Grad: 51585.8203  LR: 0.00000717  \n","Epoch: [3][1500/2460] Elapsed 6m 12s (remain 3m 58s) Loss: 0.4404(0.5046) Grad: 35995.4102  LR: 0.00000698  \n","Epoch: [3][1600/2460] Elapsed 6m 37s (remain 3m 33s) Loss: 0.4864(0.5045) Grad: 66401.3438  LR: 0.00000679  \n","Epoch: [3][1700/2460] Elapsed 7m 2s (remain 3m 8s) Loss: 0.5047(0.5045) Grad: 18144.3984  LR: 0.00000660  \n","Epoch: [3][1800/2460] Elapsed 7m 27s (remain 2m 43s) Loss: 0.4605(0.5056) Grad: 72265.9375  LR: 0.00000641  \n","Epoch: [3][1900/2460] Elapsed 7m 51s (remain 2m 18s) Loss: 0.4326(0.5051) Grad: 57902.0273  LR: 0.00000622  \n","Epoch: [3][2000/2460] Elapsed 8m 16s (remain 1m 53s) Loss: 0.5023(0.5051) Grad: 70353.6406  LR: 0.00000603  \n","Epoch: [3][2100/2460] Elapsed 8m 41s (remain 1m 29s) Loss: 0.6385(0.5055) Grad: 209378.9844  LR: 0.00000585  \n","Epoch: [3][2200/2460] Elapsed 9m 5s (remain 1m 4s) Loss: 0.4473(0.5053) Grad: 129139.5703  LR: 0.00000566  \n","Epoch: [3][2300/2460] Elapsed 9m 30s (remain 0m 39s) Loss: 0.5014(0.5049) Grad: 84253.0469  LR: 0.00000548  \n","Epoch: [3][2400/2460] Elapsed 9m 55s (remain 0m 14s) Loss: 0.6169(0.5050) Grad: 121390.7891  LR: 0.00000529  \n","Epoch: [3][2459/2460] Elapsed 10m 9s (remain 0m 0s) Loss: 0.4889(0.5052) Grad: 117936.6484  LR: 0.00000518  \n","EVAL: [0/145] Elapsed 0m 0s (remain 0m 52s) Loss: 0.6949(0.6949) \n","EVAL: [100/145] Elapsed 0m 13s (remain 0m 5s) Loss: 0.3602(0.5632) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5052  avg_val_loss: 0.5695  time: 629s\n","Epoch 3 - Score: 0.8086\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [144/145] Elapsed 0m 19s (remain 0m 0s) Loss: 0.5621(0.5695) \n","Epoch: [4][0/2460] Elapsed 0m 0s (remain 24m 19s) Loss: 0.2840(0.2840) Grad: 66428.2578  LR: 0.00000518  \n","Epoch: [4][100/2460] Elapsed 0m 25s (remain 9m 52s) Loss: 0.5179(0.4860) Grad: 122922.2109  LR: 0.00000500  \n","Epoch: [4][200/2460] Elapsed 0m 50s (remain 9m 23s) Loss: 0.4832(0.4884) Grad: 16576.9141  LR: 0.00000482  \n","Epoch: [4][300/2460] Elapsed 1m 15s (remain 8m 58s) Loss: 0.5990(0.4924) Grad: 43043.4297  LR: 0.00000464  \n","Epoch: [4][400/2460] Elapsed 1m 40s (remain 8m 34s) Loss: 0.4311(0.4944) Grad: 47219.2930  LR: 0.00000447  \n","Epoch: [4][500/2460] Elapsed 2m 5s (remain 8m 9s) Loss: 0.5837(0.4932) Grad: 59645.3750  LR: 0.00000429  \n","Epoch: [4][600/2460] Elapsed 2m 30s (remain 7m 44s) Loss: 0.4063(0.4961) Grad: 158448.6406  LR: 0.00000412  \n","Epoch: [4][700/2460] Elapsed 2m 55s (remain 7m 19s) Loss: 0.5511(0.4952) Grad: 194424.9844  LR: 0.00000395  \n","Epoch: [4][800/2460] Elapsed 3m 19s (remain 6m 53s) Loss: 0.5620(0.4967) Grad: 169133.2656  LR: 0.00000378  \n","Epoch: [4][900/2460] Elapsed 3m 44s (remain 6m 28s) Loss: 0.5657(0.4959) Grad: 55084.9414  LR: 0.00000362  \n","Epoch: [4][1000/2460] Elapsed 4m 9s (remain 6m 3s) Loss: 0.5749(0.4956) Grad: 60061.2773  LR: 0.00000346  \n","Epoch: [4][1100/2460] Elapsed 4m 34s (remain 5m 38s) Loss: 0.3665(0.4945) Grad: 29001.7910  LR: 0.00000330  \n","Epoch: [4][1200/2460] Elapsed 4m 58s (remain 5m 13s) Loss: 0.4269(0.4949) Grad: 30143.8613  LR: 0.00000314  \n","Epoch: [4][1300/2460] Elapsed 5m 24s (remain 4m 48s) Loss: 0.4612(0.4954) Grad: 36657.7109  LR: 0.00000298  \n","Epoch: [4][1400/2460] Elapsed 5m 48s (remain 4m 23s) Loss: 0.4747(0.4957) Grad: 32237.4102  LR: 0.00000283  \n","Epoch: [4][1500/2460] Elapsed 6m 13s (remain 3m 58s) Loss: 0.4783(0.4961) Grad: 232527.9375  LR: 0.00000268  \n","Epoch: [4][1600/2460] Elapsed 6m 38s (remain 3m 33s) Loss: 0.3292(0.4966) Grad: 23093.3477  LR: 0.00000254  \n","Epoch: [4][1700/2460] Elapsed 7m 3s (remain 3m 8s) Loss: 0.4463(0.4958) Grad: 72950.3672  LR: 0.00000240  \n","Epoch: [4][1800/2460] Elapsed 7m 28s (remain 2m 43s) Loss: 0.6014(0.4955) Grad: 411564.4375  LR: 0.00000226  \n","Epoch: [4][1900/2460] Elapsed 7m 53s (remain 2m 19s) Loss: 0.3965(0.4957) Grad: 25973.8086  LR: 0.00000212  \n","Epoch: [4][2000/2460] Elapsed 8m 18s (remain 1m 54s) Loss: 0.5027(0.4953) Grad: 226402.1094  LR: 0.00000199  \n","Epoch: [4][2100/2460] Elapsed 8m 42s (remain 1m 29s) Loss: 0.4072(0.4957) Grad: 180384.3438  LR: 0.00000186  \n","Epoch: [4][2200/2460] Elapsed 9m 7s (remain 1m 4s) Loss: 0.5108(0.4960) Grad: 106563.6875  LR: 0.00000174  \n","Epoch: [4][2300/2460] Elapsed 9m 32s (remain 0m 39s) Loss: 0.4604(0.4950) Grad: 36866.4297  LR: 0.00000162  \n","Epoch: [4][2400/2460] Elapsed 9m 57s (remain 0m 14s) Loss: 0.5359(0.4957) Grad: 59487.0273  LR: 0.00000150  \n","Epoch: [4][2459/2460] Elapsed 10m 12s (remain 0m 0s) Loss: 0.5536(0.4957) Grad: 168386.2969  LR: 0.00000143  \n","EVAL: [0/145] Elapsed 0m 0s (remain 0m 52s) Loss: 0.7352(0.7352) \n","EVAL: [100/145] Elapsed 0m 13s (remain 0m 5s) Loss: 0.3568(0.5680) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4957  avg_val_loss: 0.5724  time: 632s\n","Epoch 4 - Score: 0.8124\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [144/145] Elapsed 0m 19s (remain 0m 0s) Loss: 0.5544(0.5724) \n","Epoch: [5][0/2460] Elapsed 0m 0s (remain 24m 31s) Loss: 0.4294(0.4294) Grad: 108795.9688  LR: 0.00000143  \n","Epoch: [5][100/2460] Elapsed 0m 25s (remain 9m 56s) Loss: 0.4995(0.4911) Grad: 41982.7930  LR: 0.00000132  \n","Epoch: [5][200/2460] Elapsed 0m 50s (remain 9m 26s) Loss: 0.5616(0.4944) Grad: 187074.7500  LR: 0.00000122  \n","Epoch: [5][300/2460] Elapsed 1m 15s (remain 9m 0s) Loss: 0.5998(0.4943) Grad: 49243.2031  LR: 0.00000111  \n","Epoch: [5][400/2460] Elapsed 1m 40s (remain 8m 35s) Loss: 0.4988(0.4959) Grad: 10680.6074  LR: 0.00000102  \n","Epoch: [5][500/2460] Elapsed 2m 5s (remain 8m 9s) Loss: 0.4446(0.4922) Grad: 33207.4219  LR: 0.00000092  \n","Epoch: [5][600/2460] Elapsed 2m 30s (remain 7m 44s) Loss: 0.4843(0.4916) Grad: 43906.7734  LR: 0.00000083  \n","Epoch: [5][700/2460] Elapsed 2m 55s (remain 7m 19s) Loss: 0.3814(0.4911) Grad: 61608.2656  LR: 0.00000075  \n","Epoch: [5][800/2460] Elapsed 3m 20s (remain 6m 54s) Loss: 0.5697(0.4897) Grad: 50622.8125  LR: 0.00000066  \n","Epoch: [5][900/2460] Elapsed 3m 45s (remain 6m 29s) Loss: 0.3915(0.4910) Grad: 64400.3945  LR: 0.00000059  \n","Epoch: [5][1000/2460] Elapsed 4m 10s (remain 6m 5s) Loss: 0.5334(0.4919) Grad: 43180.4883  LR: 0.00000052  \n","Epoch: [5][1100/2460] Elapsed 4m 35s (remain 5m 40s) Loss: 0.5339(0.4921) Grad: 57988.6875  LR: 0.00000045  \n","Epoch: [5][1200/2460] Elapsed 5m 0s (remain 5m 15s) Loss: 0.3930(0.4917) Grad: 19553.8789  LR: 0.00000039  \n","Epoch: [5][1300/2460] Elapsed 5m 25s (remain 4m 50s) Loss: 0.5521(0.4913) Grad: 34106.5039  LR: 0.00000033  \n","Epoch: [5][1400/2460] Elapsed 5m 51s (remain 4m 25s) Loss: 0.4392(0.4909) Grad: 99713.9453  LR: 0.00000027  \n","Epoch: [5][1500/2460] Elapsed 6m 16s (remain 4m 0s) Loss: 0.5310(0.4909) Grad: 106617.7031  LR: 0.00000022  \n","Epoch: [5][1600/2460] Elapsed 6m 41s (remain 3m 35s) Loss: 0.4616(0.4913) Grad: 34088.4258  LR: 0.00000018  \n","Epoch: [5][1700/2460] Elapsed 7m 6s (remain 3m 10s) Loss: 0.5279(0.4918) Grad: 38728.4844  LR: 0.00000014  \n","Epoch: [5][1800/2460] Elapsed 7m 31s (remain 2m 45s) Loss: 0.6343(0.4918) Grad: 42860.7070  LR: 0.00000011  \n","Epoch: [5][1900/2460] Elapsed 7m 56s (remain 2m 20s) Loss: 0.6526(0.4914) Grad: 199776.6406  LR: 0.00000008  \n","Epoch: [5][2000/2460] Elapsed 8m 21s (remain 1m 55s) Loss: 0.3789(0.4915) Grad: 191898.0000  LR: 0.00000005  \n","Epoch: [5][2100/2460] Elapsed 8m 46s (remain 1m 29s) Loss: 0.5119(0.4911) Grad: 192501.6094  LR: 0.00000003  \n","Epoch: [5][2200/2460] Elapsed 9m 11s (remain 1m 4s) Loss: 0.5614(0.4914) Grad: 80822.2500  LR: 0.00000002  \n","Epoch: [5][2300/2460] Elapsed 9m 36s (remain 0m 39s) Loss: 0.3533(0.4911) Grad: 78697.4766  LR: 0.00000001  \n","Epoch: [5][2400/2460] Elapsed 10m 1s (remain 0m 14s) Loss: 0.3891(0.4915) Grad: 154134.8750  LR: 0.00000000  \n","Epoch: [5][2459/2460] Elapsed 10m 16s (remain 0m 0s) Loss: 0.3860(0.4915) Grad: 79192.6641  LR: 0.00000000  \n","EVAL: [0/145] Elapsed 0m 0s (remain 0m 53s) Loss: 0.7451(0.7451) \n","EVAL: [100/145] Elapsed 0m 13s (remain 0m 5s) Loss: 0.3578(0.5718) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4915  avg_val_loss: 0.5757  time: 636s\n","Epoch 5 - Score: 0.8108\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [144/145] Elapsed 0m 19s (remain 0m 0s) Loss: 0.5558(0.5757) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 7 result ==========\n","Score: 0.8133\n","========== fold: 8 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2476] Elapsed 0m 0s (remain 20m 44s) Loss: 0.6674(0.6674) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2476] Elapsed 0m 25s (remain 10m 1s) Loss: 0.6794(0.6536) Grad: 72331.3672  LR: 0.00001500  \n","Epoch: [1][200/2476] Elapsed 0m 50s (remain 9m 32s) Loss: 0.4966(0.6332) Grad: 69581.2734  LR: 0.00001499  \n","Epoch: [1][300/2476] Elapsed 1m 15s (remain 9m 5s) Loss: 0.6480(0.6222) Grad: 14411.8564  LR: 0.00001498  \n","Epoch: [1][400/2476] Elapsed 1m 40s (remain 8m 39s) Loss: 0.5721(0.6128) Grad: 11972.5957  LR: 0.00001496  \n","Epoch: [1][500/2476] Elapsed 2m 5s (remain 8m 14s) Loss: 0.6387(0.6041) Grad: 8971.3691  LR: 0.00001494  \n","Epoch: [1][600/2476] Elapsed 2m 30s (remain 7m 49s) Loss: 0.6519(0.5980) Grad: 2929.1880  LR: 0.00001491  \n","Epoch: [1][700/2476] Elapsed 2m 55s (remain 7m 24s) Loss: 0.4946(0.5928) Grad: 4791.5386  LR: 0.00001488  \n","Epoch: [1][800/2476] Elapsed 3m 20s (remain 6m 59s) Loss: 0.5927(0.5874) Grad: 14343.2529  LR: 0.00001485  \n","Epoch: [1][900/2476] Elapsed 3m 45s (remain 6m 34s) Loss: 0.6100(0.5850) Grad: 11668.9844  LR: 0.00001480  \n","Epoch: [1][1000/2476] Elapsed 4m 10s (remain 6m 9s) Loss: 0.5663(0.5809) Grad: 4861.6436  LR: 0.00001476  \n","Epoch: [1][1100/2476] Elapsed 4m 35s (remain 5m 44s) Loss: 0.5030(0.5777) Grad: 15052.2510  LR: 0.00001471  \n","Epoch: [1][1200/2476] Elapsed 5m 0s (remain 5m 19s) Loss: 0.5457(0.5748) Grad: 21440.4219  LR: 0.00001465  \n","Epoch: [1][1300/2476] Elapsed 5m 25s (remain 4m 54s) Loss: 0.5228(0.5726) Grad: 9360.8984  LR: 0.00001459  \n","Epoch: [1][1400/2476] Elapsed 5m 50s (remain 4m 29s) Loss: 0.4367(0.5709) Grad: 7858.9282  LR: 0.00001453  \n","Epoch: [1][1500/2476] Elapsed 6m 15s (remain 4m 4s) Loss: 0.5493(0.5693) Grad: 7453.4448  LR: 0.00001446  \n","Epoch: [1][1600/2476] Elapsed 6m 40s (remain 3m 39s) Loss: 0.5140(0.5678) Grad: 8384.6592  LR: 0.00001439  \n","Epoch: [1][1700/2476] Elapsed 7m 5s (remain 3m 14s) Loss: 0.4980(0.5665) Grad: 13681.9160  LR: 0.00001431  \n","Epoch: [1][1800/2476] Elapsed 7m 30s (remain 2m 48s) Loss: 0.5902(0.5645) Grad: 2153.3896  LR: 0.00001423  \n","Epoch: [1][1900/2476] Elapsed 7m 55s (remain 2m 23s) Loss: 0.5045(0.5636) Grad: 2666.3621  LR: 0.00001414  \n","Epoch: [1][2000/2476] Elapsed 8m 20s (remain 1m 58s) Loss: 0.6315(0.5628) Grad: 3881.7163  LR: 0.00001405  \n","Epoch: [1][2100/2476] Elapsed 8m 46s (remain 1m 33s) Loss: 0.5636(0.5622) Grad: 12550.5254  LR: 0.00001396  \n","Epoch: [1][2200/2476] Elapsed 9m 10s (remain 1m 8s) Loss: 0.4842(0.5605) Grad: 7729.8525  LR: 0.00001386  \n","Epoch: [1][2300/2476] Elapsed 9m 36s (remain 0m 43s) Loss: 0.5240(0.5597) Grad: 7262.2998  LR: 0.00001376  \n","Epoch: [1][2400/2476] Elapsed 10m 1s (remain 0m 18s) Loss: 0.6127(0.5587) Grad: 67930.1016  LR: 0.00001365  \n","Epoch: [1][2475/2476] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5884(0.5581) Grad: 8752.2744  LR: 0.00001357  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 49s) Loss: 0.4841(0.4841) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5861(0.5457) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5581  avg_val_loss: 0.5439  time: 637s\n","Epoch 1 - Score: 0.8158\n","Epoch 1 - Save Best Score: 0.8158 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.4085(0.5439) \n","Epoch: [2][0/2476] Elapsed 0m 0s (remain 26m 23s) Loss: 0.5525(0.5525) Grad: 28721.1719  LR: 0.00001357  \n","Epoch: [2][100/2476] Elapsed 0m 25s (remain 10m 11s) Loss: 0.4858(0.5064) Grad: 68773.7422  LR: 0.00001345  \n","Epoch: [2][200/2476] Elapsed 0m 51s (remain 9m 41s) Loss: 0.5056(0.5137) Grad: 121864.3516  LR: 0.00001333  \n","Epoch: [2][300/2476] Elapsed 1m 16s (remain 9m 12s) Loss: 0.4283(0.5109) Grad: 215926.8281  LR: 0.00001321  \n","Epoch: [2][400/2476] Elapsed 1m 41s (remain 8m 45s) Loss: 0.4488(0.5118) Grad: 23616.2148  LR: 0.00001309  \n","Epoch: [2][500/2476] Elapsed 2m 6s (remain 8m 19s) Loss: 0.5687(0.5143) Grad: 315893.3750  LR: 0.00001296  \n","Epoch: [2][600/2476] Elapsed 2m 31s (remain 7m 53s) Loss: 0.4869(0.5124) Grad: 58203.2422  LR: 0.00001283  \n","Epoch: [2][700/2476] Elapsed 2m 56s (remain 7m 27s) Loss: 0.5195(0.5127) Grad: 140426.1875  LR: 0.00001269  \n","Epoch: [2][800/2476] Elapsed 3m 21s (remain 7m 1s) Loss: 0.3594(0.5139) Grad: 45293.0977  LR: 0.00001255  \n","Epoch: [2][900/2476] Elapsed 3m 46s (remain 6m 36s) Loss: 0.4098(0.5140) Grad: 63248.6562  LR: 0.00001241  \n","Epoch: [2][1000/2476] Elapsed 4m 11s (remain 6m 11s) Loss: 0.5776(0.5141) Grad: 75808.5234  LR: 0.00001227  \n","Epoch: [2][1100/2476] Elapsed 4m 36s (remain 5m 45s) Loss: 0.5657(0.5140) Grad: 351415.4062  LR: 0.00001212  \n","Epoch: [2][1200/2476] Elapsed 5m 1s (remain 5m 20s) Loss: 0.5857(0.5146) Grad: 133950.1094  LR: 0.00001197  \n","Epoch: [2][1300/2476] Elapsed 5m 27s (remain 4m 55s) Loss: 0.4040(0.5151) Grad: 63092.6602  LR: 0.00001181  \n","Epoch: [2][1400/2476] Elapsed 5m 52s (remain 4m 30s) Loss: 0.4428(0.5149) Grad: 128838.3125  LR: 0.00001165  \n","Epoch: [2][1500/2476] Elapsed 6m 17s (remain 4m 5s) Loss: 0.5652(0.5153) Grad: 47146.4102  LR: 0.00001149  \n","Epoch: [2][1600/2476] Elapsed 6m 42s (remain 3m 39s) Loss: 0.4436(0.5147) Grad: 73885.5078  LR: 0.00001133  \n","Epoch: [2][1700/2476] Elapsed 7m 7s (remain 3m 14s) Loss: 0.3500(0.5143) Grad: 30328.7148  LR: 0.00001117  \n","Epoch: [2][1800/2476] Elapsed 7m 32s (remain 2m 49s) Loss: 0.5975(0.5142) Grad: 99957.4062  LR: 0.00001100  \n","Epoch: [2][1900/2476] Elapsed 7m 57s (remain 2m 24s) Loss: 0.5670(0.5146) Grad: 47099.8984  LR: 0.00001083  \n","Epoch: [2][2000/2476] Elapsed 8m 22s (remain 1m 59s) Loss: 0.5225(0.5149) Grad: 515653.1562  LR: 0.00001066  \n","Epoch: [2][2100/2476] Elapsed 8m 47s (remain 1m 34s) Loss: 0.4942(0.5150) Grad: 96705.3203  LR: 0.00001048  \n","Epoch: [2][2200/2476] Elapsed 9m 12s (remain 1m 9s) Loss: 0.5313(0.5152) Grad: 152503.6250  LR: 0.00001031  \n","Epoch: [2][2300/2476] Elapsed 9m 37s (remain 0m 43s) Loss: 0.5241(0.5152) Grad: 92060.4453  LR: 0.00001013  \n","Epoch: [2][2400/2476] Elapsed 10m 2s (remain 0m 18s) Loss: 0.4673(0.5155) Grad: 75290.5391  LR: 0.00000995  \n","Epoch: [2][2475/2476] Elapsed 10m 21s (remain 0m 0s) Loss: 0.4749(0.5152) Grad: 101515.6562  LR: 0.00000982  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 51s) Loss: 0.4688(0.4688) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6241(0.5508) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5152  avg_val_loss: 0.5499  time: 639s\n","Epoch 2 - Score: 0.8253\n","Epoch 2 - Save Best Score: 0.8253 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.3843(0.5499) \n","Epoch: [3][0/2476] Elapsed 0m 0s (remain 25m 15s) Loss: 0.2745(0.2745) Grad: 97375.5391  LR: 0.00000982  \n","Epoch: [3][100/2476] Elapsed 0m 26s (remain 10m 11s) Loss: 0.5238(0.5107) Grad: 90167.9531  LR: 0.00000963  \n","Epoch: [3][200/2476] Elapsed 0m 51s (remain 9m 41s) Loss: 0.3961(0.5147) Grad: 33147.3398  LR: 0.00000945  \n","Epoch: [3][300/2476] Elapsed 1m 16s (remain 9m 12s) Loss: 0.4683(0.5088) Grad: 28472.8867  LR: 0.00000927  \n","Epoch: [3][400/2476] Elapsed 1m 41s (remain 8m 45s) Loss: 0.4147(0.5096) Grad: 51207.6836  LR: 0.00000908  \n","Epoch: [3][500/2476] Elapsed 2m 6s (remain 8m 18s) Loss: 0.4945(0.5078) Grad: 126986.6406  LR: 0.00000889  \n","Epoch: [3][600/2476] Elapsed 2m 31s (remain 7m 52s) Loss: 0.5454(0.5076) Grad: 26042.1523  LR: 0.00000871  \n","Epoch: [3][700/2476] Elapsed 2m 56s (remain 7m 26s) Loss: 0.5560(0.5089) Grad: 43050.8125  LR: 0.00000852  \n","Epoch: [3][800/2476] Elapsed 3m 21s (remain 7m 1s) Loss: 0.6932(0.5082) Grad: 86508.8828  LR: 0.00000833  \n","Epoch: [3][900/2476] Elapsed 3m 46s (remain 6m 35s) Loss: 0.4820(0.5090) Grad: 15694.9668  LR: 0.00000814  \n","Epoch: [3][1000/2476] Elapsed 4m 11s (remain 6m 10s) Loss: 0.4893(0.5090) Grad: 109498.9531  LR: 0.00000795  \n","Epoch: [3][1100/2476] Elapsed 4m 36s (remain 5m 45s) Loss: 0.5322(0.5091) Grad: 103944.3281  LR: 0.00000776  \n","Epoch: [3][1200/2476] Elapsed 5m 1s (remain 5m 20s) Loss: 0.5259(0.5087) Grad: 284585.4688  LR: 0.00000757  \n","Epoch: [3][1300/2476] Elapsed 5m 26s (remain 4m 55s) Loss: 0.5145(0.5085) Grad: 32754.3613  LR: 0.00000738  \n","Epoch: [3][1400/2476] Elapsed 5m 51s (remain 4m 29s) Loss: 0.4200(0.5077) Grad: 45480.8359  LR: 0.00000719  \n","Epoch: [3][1500/2476] Elapsed 6m 16s (remain 4m 4s) Loss: 0.4430(0.5078) Grad: 41292.3984  LR: 0.00000700  \n","Epoch: [3][1600/2476] Elapsed 6m 41s (remain 3m 39s) Loss: 0.4374(0.5088) Grad: 21738.9297  LR: 0.00000681  \n","Epoch: [3][1700/2476] Elapsed 7m 6s (remain 3m 14s) Loss: 0.5618(0.5083) Grad: 70464.7969  LR: 0.00000662  \n","Epoch: [3][1800/2476] Elapsed 7m 31s (remain 2m 49s) Loss: 0.4151(0.5078) Grad: 55513.7305  LR: 0.00000643  \n","Epoch: [3][1900/2476] Elapsed 7m 56s (remain 2m 24s) Loss: 0.4195(0.5076) Grad: 49313.2227  LR: 0.00000624  \n","Epoch: [3][2000/2476] Elapsed 8m 21s (remain 1m 59s) Loss: 0.5240(0.5077) Grad: 204821.3125  LR: 0.00000606  \n","Epoch: [3][2100/2476] Elapsed 8m 47s (remain 1m 34s) Loss: 0.5013(0.5080) Grad: 57719.2227  LR: 0.00000587  \n","Epoch: [3][2200/2476] Elapsed 9m 12s (remain 1m 8s) Loss: 0.6077(0.5083) Grad: 69624.8047  LR: 0.00000569  \n","Epoch: [3][2300/2476] Elapsed 9m 37s (remain 0m 43s) Loss: 0.4511(0.5085) Grad: 30803.3926  LR: 0.00000550  \n","Epoch: [3][2400/2476] Elapsed 10m 2s (remain 0m 18s) Loss: 0.5947(0.5083) Grad: 20896.1152  LR: 0.00000532  \n","Epoch: [3][2475/2476] Elapsed 10m 20s (remain 0m 0s) Loss: 0.6100(0.5083) Grad: 40834.8086  LR: 0.00000518  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 51s) Loss: 0.4594(0.4594) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6146(0.5618) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5083  avg_val_loss: 0.5607  time: 638s\n","Epoch 3 - Score: 0.8186\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.3864(0.5607) \n","Epoch: [4][0/2476] Elapsed 0m 0s (remain 25m 24s) Loss: 0.4420(0.4420) Grad: 100247.3047  LR: 0.00000518  \n","Epoch: [4][100/2476] Elapsed 0m 25s (remain 10m 5s) Loss: 0.4633(0.4936) Grad: 506323.3438  LR: 0.00000500  \n","Epoch: [4][200/2476] Elapsed 0m 50s (remain 9m 35s) Loss: 0.3740(0.4965) Grad: 56810.5039  LR: 0.00000482  \n","Epoch: [4][300/2476] Elapsed 1m 15s (remain 9m 9s) Loss: 0.5852(0.4990) Grad: 93135.4453  LR: 0.00000464  \n","Epoch: [4][400/2476] Elapsed 1m 41s (remain 8m 43s) Loss: 0.6058(0.4977) Grad: 52608.8164  LR: 0.00000447  \n","Epoch: [4][500/2476] Elapsed 2m 6s (remain 8m 18s) Loss: 0.4593(0.5009) Grad: 23179.0000  LR: 0.00000430  \n","Epoch: [4][600/2476] Elapsed 2m 31s (remain 7m 52s) Loss: 0.4319(0.5005) Grad: 57291.7578  LR: 0.00000413  \n","Epoch: [4][700/2476] Elapsed 2m 56s (remain 7m 27s) Loss: 0.5881(0.4998) Grad: 41984.3125  LR: 0.00000396  \n","Epoch: [4][800/2476] Elapsed 3m 21s (remain 7m 1s) Loss: 0.4503(0.5011) Grad: 34004.8242  LR: 0.00000379  \n","Epoch: [4][900/2476] Elapsed 3m 46s (remain 6m 36s) Loss: 0.4747(0.5019) Grad: 65516.2578  LR: 0.00000363  \n","Epoch: [4][1000/2476] Elapsed 4m 12s (remain 6m 11s) Loss: 0.4634(0.5019) Grad: 55700.7227  LR: 0.00000346  \n","Epoch: [4][1100/2476] Elapsed 4m 37s (remain 5m 46s) Loss: 0.5993(0.5032) Grad: 74732.7109  LR: 0.00000331  \n","Epoch: [4][1200/2476] Elapsed 5m 2s (remain 5m 20s) Loss: 0.5356(0.5029) Grad: 31908.0820  LR: 0.00000315  \n","Epoch: [4][1300/2476] Elapsed 5m 27s (remain 4m 55s) Loss: 0.6180(0.5032) Grad: 76010.0156  LR: 0.00000300  \n","Epoch: [4][1400/2476] Elapsed 5m 52s (remain 4m 30s) Loss: 0.4904(0.5035) Grad: 39680.5547  LR: 0.00000284  \n","Epoch: [4][1500/2476] Elapsed 6m 17s (remain 4m 5s) Loss: 0.5242(0.5033) Grad: 34943.8203  LR: 0.00000270  \n","Epoch: [4][1600/2476] Elapsed 6m 42s (remain 3m 40s) Loss: 0.4928(0.5026) Grad: 26207.9297  LR: 0.00000255  \n","Epoch: [4][1700/2476] Elapsed 7m 8s (remain 3m 15s) Loss: 0.3634(0.5031) Grad: 93891.0781  LR: 0.00000241  \n","Epoch: [4][1800/2476] Elapsed 7m 33s (remain 2m 49s) Loss: 0.4890(0.5028) Grad: 140523.8438  LR: 0.00000227  \n","Epoch: [4][1900/2476] Elapsed 7m 58s (remain 2m 24s) Loss: 0.3933(0.5032) Grad: 33887.7227  LR: 0.00000214  \n","Epoch: [4][2000/2476] Elapsed 8m 23s (remain 1m 59s) Loss: 0.5031(0.5028) Grad: 96586.8984  LR: 0.00000201  \n","Epoch: [4][2100/2476] Elapsed 8m 48s (remain 1m 34s) Loss: 0.4427(0.5026) Grad: 300216.5625  LR: 0.00000188  \n","Epoch: [4][2200/2476] Elapsed 9m 13s (remain 1m 9s) Loss: 0.4149(0.5022) Grad: 49594.3203  LR: 0.00000175  \n","Epoch: [4][2300/2476] Elapsed 9m 38s (remain 0m 44s) Loss: 0.5465(0.5023) Grad: 56350.6367  LR: 0.00000163  \n","Epoch: [4][2400/2476] Elapsed 10m 3s (remain 0m 18s) Loss: 0.4607(0.5023) Grad: 197006.3906  LR: 0.00000152  \n","Epoch: [4][2475/2476] Elapsed 10m 22s (remain 0m 0s) Loss: 0.4866(0.5018) Grad: 326082.8438  LR: 0.00000143  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 50s) Loss: 0.4787(0.4787) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6546(0.5584) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.5018  avg_val_loss: 0.5625  time: 640s\n","Epoch 4 - Score: 0.8242\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.3993(0.5625) \n","Epoch: [5][0/2476] Elapsed 0m 0s (remain 24m 48s) Loss: 0.4885(0.4885) Grad: 76722.1875  LR: 0.00000143  \n","Epoch: [5][100/2476] Elapsed 0m 25s (remain 10m 6s) Loss: 0.5513(0.4853) Grad: 42377.6445  LR: 0.00000132  \n","Epoch: [5][200/2476] Elapsed 0m 50s (remain 9m 36s) Loss: 0.4901(0.4923) Grad: 39393.9023  LR: 0.00000122  \n","Epoch: [5][300/2476] Elapsed 1m 16s (remain 9m 9s) Loss: 0.5483(0.4962) Grad: 28710.4297  LR: 0.00000111  \n","Epoch: [5][400/2476] Elapsed 1m 41s (remain 8m 43s) Loss: 0.4632(0.4965) Grad: 37446.0195  LR: 0.00000102  \n","Epoch: [5][500/2476] Elapsed 2m 6s (remain 8m 17s) Loss: 0.4464(0.4944) Grad: 326656.0938  LR: 0.00000092  \n","Epoch: [5][600/2476] Elapsed 2m 31s (remain 7m 52s) Loss: 0.5870(0.4942) Grad: 71240.2031  LR: 0.00000083  \n","Epoch: [5][700/2476] Elapsed 2m 56s (remain 7m 26s) Loss: 0.5825(0.4961) Grad: 56514.5000  LR: 0.00000075  \n","Epoch: [5][800/2476] Elapsed 3m 21s (remain 7m 1s) Loss: 0.4676(0.4966) Grad: 79104.7188  LR: 0.00000067  \n","Epoch: [5][900/2476] Elapsed 3m 46s (remain 6m 36s) Loss: 0.4611(0.4966) Grad: 84148.2031  LR: 0.00000059  \n","Epoch: [5][1000/2476] Elapsed 4m 11s (remain 6m 11s) Loss: 0.5942(0.4966) Grad: 194936.7500  LR: 0.00000052  \n","Epoch: [5][1100/2476] Elapsed 4m 36s (remain 5m 45s) Loss: 0.4447(0.4951) Grad: 35402.5430  LR: 0.00000045  \n","Epoch: [5][1200/2476] Elapsed 5m 1s (remain 5m 20s) Loss: 0.5463(0.4967) Grad: 50830.9727  LR: 0.00000039  \n","Epoch: [5][1300/2476] Elapsed 5m 27s (remain 4m 55s) Loss: 0.5271(0.4972) Grad: 453756.1250  LR: 0.00000033  \n","Epoch: [5][1400/2476] Elapsed 5m 52s (remain 4m 30s) Loss: 0.4832(0.4961) Grad: 73838.2656  LR: 0.00000028  \n","Epoch: [5][1500/2476] Elapsed 6m 17s (remain 4m 5s) Loss: 0.5935(0.4966) Grad: 28275.4141  LR: 0.00000023  \n","Epoch: [5][1600/2476] Elapsed 6m 42s (remain 3m 39s) Loss: 0.5974(0.4965) Grad: 24171.3809  LR: 0.00000018  \n","Epoch: [5][1700/2476] Elapsed 7m 7s (remain 3m 14s) Loss: 0.5210(0.4969) Grad: 38905.2578  LR: 0.00000014  \n","Epoch: [5][1800/2476] Elapsed 7m 32s (remain 2m 49s) Loss: 0.4153(0.4970) Grad: 15215.9355  LR: 0.00000011  \n","Epoch: [5][1900/2476] Elapsed 7m 57s (remain 2m 24s) Loss: 0.5728(0.4963) Grad: 17602.7812  LR: 0.00000008  \n","Epoch: [5][2000/2476] Elapsed 8m 22s (remain 1m 59s) Loss: 0.3335(0.4964) Grad: 73334.2891  LR: 0.00000005  \n","Epoch: [5][2100/2476] Elapsed 8m 47s (remain 1m 34s) Loss: 0.5057(0.4966) Grad: 13230.8340  LR: 0.00000003  \n","Epoch: [5][2200/2476] Elapsed 9m 12s (remain 1m 9s) Loss: 0.5306(0.4971) Grad: 12020.7363  LR: 0.00000002  \n","Epoch: [5][2300/2476] Elapsed 9m 38s (remain 0m 43s) Loss: 0.5653(0.4972) Grad: 264637.7500  LR: 0.00000001  \n","Epoch: [5][2400/2476] Elapsed 10m 3s (remain 0m 18s) Loss: 0.5683(0.4974) Grad: 20245.6074  LR: 0.00000000  \n","Epoch: [5][2475/2476] Elapsed 10m 22s (remain 0m 0s) Loss: 0.6355(0.4975) Grad: 35721.8125  LR: 0.00000000  \n","EVAL: [0/130] Elapsed 0m 0s (remain 0m 51s) Loss: 0.4827(0.4827) \n","EVAL: [100/130] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6707(0.5626) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4975  avg_val_loss: 0.5680  time: 639s\n","Epoch 5 - Score: 0.8215\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [129/130] Elapsed 0m 17s (remain 0m 0s) Loss: 0.3972(0.5680) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 8 result ==========\n","Score: 0.8253\n","========== fold: 9 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2464] Elapsed 0m 0s (remain 23m 49s) Loss: 0.6819(0.6819) Grad: 31688.3105  LR: 0.00001500  \n","Epoch: [1][100/2464] Elapsed 0m 25s (remain 9m 59s) Loss: 0.5785(0.6294) Grad: 42579.2539  LR: 0.00001500  \n","Epoch: [1][200/2464] Elapsed 0m 50s (remain 9m 30s) Loss: 0.5278(0.6123) Grad: 53221.9453  LR: 0.00001499  \n","Epoch: [1][300/2464] Elapsed 1m 15s (remain 9m 4s) Loss: 0.5837(0.5988) Grad: 34374.6055  LR: 0.00001498  \n","Epoch: [1][400/2464] Elapsed 1m 40s (remain 8m 38s) Loss: 0.5957(0.5902) Grad: 108047.7031  LR: 0.00001496  \n","Epoch: [1][500/2464] Elapsed 2m 5s (remain 8m 12s) Loss: 0.5681(0.5834) Grad: 33719.3516  LR: 0.00001494  \n","Epoch: [1][600/2464] Elapsed 2m 30s (remain 7m 47s) Loss: 0.6195(0.5793) Grad: 54651.4648  LR: 0.00001491  \n","Epoch: [1][700/2464] Elapsed 2m 55s (remain 7m 22s) Loss: 0.6559(0.5765) Grad: 98724.0625  LR: 0.00001488  \n","Epoch: [1][800/2464] Elapsed 3m 20s (remain 6m 57s) Loss: 0.3941(0.5744) Grad: 46251.4648  LR: 0.00001484  \n","Epoch: [1][900/2464] Elapsed 3m 46s (remain 6m 32s) Loss: 0.5635(0.5706) Grad: 108202.2891  LR: 0.00001480  \n","Epoch: [1][1000/2464] Elapsed 4m 11s (remain 6m 6s) Loss: 0.6175(0.5700) Grad: 53303.3398  LR: 0.00001476  \n","Epoch: [1][1100/2464] Elapsed 4m 36s (remain 5m 41s) Loss: 0.6045(0.5678) Grad: 105180.3359  LR: 0.00001471  \n","Epoch: [1][1200/2464] Elapsed 5m 1s (remain 5m 16s) Loss: 0.6067(0.5664) Grad: 38073.6211  LR: 0.00001465  \n","Epoch: [1][1300/2464] Elapsed 5m 26s (remain 4m 51s) Loss: 0.4991(0.5647) Grad: 41669.8320  LR: 0.00001459  \n","Epoch: [1][1400/2464] Elapsed 5m 51s (remain 4m 26s) Loss: 0.4212(0.5630) Grad: 41716.9961  LR: 0.00001453  \n","Epoch: [1][1500/2464] Elapsed 6m 16s (remain 4m 1s) Loss: 0.5478(0.5617) Grad: 61756.6094  LR: 0.00001446  \n","Epoch: [1][1600/2464] Elapsed 6m 41s (remain 3m 36s) Loss: 0.5173(0.5604) Grad: 23341.9590  LR: 0.00001438  \n","Epoch: [1][1700/2464] Elapsed 7m 6s (remain 3m 11s) Loss: 0.6351(0.5595) Grad: 179378.2031  LR: 0.00001431  \n","Epoch: [1][1800/2464] Elapsed 7m 31s (remain 2m 46s) Loss: 0.6087(0.5586) Grad: 31261.3652  LR: 0.00001422  \n","Epoch: [1][1900/2464] Elapsed 7m 56s (remain 2m 21s) Loss: 0.4680(0.5580) Grad: 97498.3125  LR: 0.00001414  \n","Epoch: [1][2000/2464] Elapsed 8m 21s (remain 1m 56s) Loss: 0.4998(0.5571) Grad: 295202.2812  LR: 0.00001404  \n","Epoch: [1][2100/2464] Elapsed 8m 46s (remain 1m 30s) Loss: 0.4021(0.5560) Grad: 208808.6562  LR: 0.00001395  \n","Epoch: [1][2200/2464] Elapsed 9m 11s (remain 1m 5s) Loss: 0.5140(0.5546) Grad: 199954.2344  LR: 0.00001385  \n","Epoch: [1][2300/2464] Elapsed 9m 36s (remain 0m 40s) Loss: 0.5400(0.5534) Grad: 248828.4062  LR: 0.00001375  \n","Epoch: [1][2400/2464] Elapsed 10m 1s (remain 0m 15s) Loss: 0.5579(0.5526) Grad: 69480.7578  LR: 0.00001364  \n","Epoch: [1][2463/2464] Elapsed 10m 17s (remain 0m 0s) Loss: 0.5939(0.5527) Grad: 129494.2422  LR: 0.00001357  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 55s) Loss: 0.6223(0.6223) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.7360(0.5486) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5527  avg_val_loss: 0.5482  time: 636s\n","Epoch 1 - Score: 0.8126\n","Epoch 1 - Save Best Score: 0.8126 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3969(0.5482) \n","Epoch: [2][0/2464] Elapsed 0m 0s (remain 25m 57s) Loss: 0.5561(0.5561) Grad: 64415.7344  LR: 0.00001357  \n","Epoch: [2][100/2464] Elapsed 0m 26s (remain 10m 10s) Loss: 0.4898(0.5238) Grad: 210523.2500  LR: 0.00001345  \n","Epoch: [2][200/2464] Elapsed 0m 51s (remain 9m 39s) Loss: 0.5414(0.5223) Grad: 41803.6016  LR: 0.00001333  \n","Epoch: [2][300/2464] Elapsed 1m 16s (remain 9m 10s) Loss: 0.5145(0.5222) Grad: 43027.9492  LR: 0.00001321  \n","Epoch: [2][400/2464] Elapsed 1m 41s (remain 8m 42s) Loss: 0.4057(0.5229) Grad: 34207.0938  LR: 0.00001309  \n","Epoch: [2][500/2464] Elapsed 2m 6s (remain 8m 15s) Loss: 0.5463(0.5207) Grad: 45477.5117  LR: 0.00001296  \n","Epoch: [2][600/2464] Elapsed 2m 31s (remain 7m 49s) Loss: 0.6115(0.5221) Grad: 46103.0273  LR: 0.00001282  \n","Epoch: [2][700/2464] Elapsed 2m 56s (remain 7m 24s) Loss: 0.5908(0.5223) Grad: 52414.9414  LR: 0.00001269  \n","Epoch: [2][800/2464] Elapsed 3m 21s (remain 6m 58s) Loss: 0.5650(0.5222) Grad: 29253.9883  LR: 0.00001255  \n","Epoch: [2][900/2464] Elapsed 3m 46s (remain 6m 33s) Loss: 0.5716(0.5226) Grad: 84474.6406  LR: 0.00001240  \n","Epoch: [2][1000/2464] Elapsed 4m 11s (remain 6m 8s) Loss: 0.3942(0.5222) Grad: 37159.2188  LR: 0.00001226  \n","Epoch: [2][1100/2464] Elapsed 4m 36s (remain 5m 42s) Loss: 0.4364(0.5224) Grad: 28239.5430  LR: 0.00001211  \n","Epoch: [2][1200/2464] Elapsed 5m 1s (remain 5m 17s) Loss: 0.6793(0.5218) Grad: 27977.3672  LR: 0.00001196  \n","Epoch: [2][1300/2464] Elapsed 5m 27s (remain 4m 52s) Loss: 0.3212(0.5220) Grad: 25610.1445  LR: 0.00001180  \n","Epoch: [2][1400/2464] Elapsed 5m 52s (remain 4m 27s) Loss: 0.3972(0.5219) Grad: 33693.3008  LR: 0.00001164  \n","Epoch: [2][1500/2464] Elapsed 6m 17s (remain 4m 2s) Loss: 0.5180(0.5217) Grad: 98741.7109  LR: 0.00001148  \n","Epoch: [2][1600/2464] Elapsed 6m 42s (remain 3m 36s) Loss: 0.6200(0.5220) Grad: 118250.7500  LR: 0.00001132  \n","Epoch: [2][1700/2464] Elapsed 7m 7s (remain 3m 11s) Loss: 0.5367(0.5224) Grad: 39981.9961  LR: 0.00001115  \n","Epoch: [2][1800/2464] Elapsed 7m 32s (remain 2m 46s) Loss: 0.6220(0.5225) Grad: 78382.1484  LR: 0.00001098  \n","Epoch: [2][1900/2464] Elapsed 7m 57s (remain 2m 21s) Loss: 0.4315(0.5220) Grad: 33847.0742  LR: 0.00001081  \n","Epoch: [2][2000/2464] Elapsed 8m 22s (remain 1m 56s) Loss: 0.4778(0.5220) Grad: 34158.9141  LR: 0.00001064  \n","Epoch: [2][2100/2464] Elapsed 8m 47s (remain 1m 31s) Loss: 0.5022(0.5221) Grad: 59007.6367  LR: 0.00001047  \n","Epoch: [2][2200/2464] Elapsed 9m 12s (remain 1m 6s) Loss: 0.4643(0.5220) Grad: 98011.1484  LR: 0.00001029  \n","Epoch: [2][2300/2464] Elapsed 9m 37s (remain 0m 40s) Loss: 0.5833(0.5217) Grad: 56203.0703  LR: 0.00001011  \n","Epoch: [2][2400/2464] Elapsed 10m 3s (remain 0m 15s) Loss: 0.5536(0.5215) Grad: 58557.1641  LR: 0.00000993  \n","Epoch: [2][2463/2464] Elapsed 10m 18s (remain 0m 0s) Loss: 0.4535(0.5216) Grad: 17009.1172  LR: 0.00000982  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 55s) Loss: 0.6213(0.6213) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.9368(0.5536) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5216  avg_val_loss: 0.5540  time: 638s\n","Epoch 2 - Score: 0.8245\n","Epoch 2 - Save Best Score: 0.8245 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3772(0.5540) \n","Epoch: [3][0/2464] Elapsed 0m 0s (remain 27m 32s) Loss: 0.6496(0.6496) Grad: 26603.6211  LR: 0.00000982  \n","Epoch: [3][100/2464] Elapsed 0m 26s (remain 10m 9s) Loss: 0.6402(0.5043) Grad: 100958.9453  LR: 0.00000963  \n","Epoch: [3][200/2464] Elapsed 0m 51s (remain 9m 40s) Loss: 0.4476(0.5060) Grad: 66030.0312  LR: 0.00000945  \n","Epoch: [3][300/2464] Elapsed 1m 16s (remain 9m 10s) Loss: 0.3641(0.5017) Grad: 34661.4609  LR: 0.00000926  \n","Epoch: [3][400/2464] Elapsed 1m 41s (remain 8m 43s) Loss: 0.5355(0.5047) Grad: 34370.8438  LR: 0.00000908  \n","Epoch: [3][500/2464] Elapsed 2m 6s (remain 8m 16s) Loss: 0.4710(0.5031) Grad: 64163.4023  LR: 0.00000889  \n","Epoch: [3][600/2464] Elapsed 2m 31s (remain 7m 50s) Loss: 0.5064(0.5056) Grad: 94237.2656  LR: 0.00000870  \n","Epoch: [3][700/2464] Elapsed 2m 56s (remain 7m 24s) Loss: 0.6018(0.5059) Grad: 58172.5664  LR: 0.00000851  \n","Epoch: [3][800/2464] Elapsed 3m 22s (remain 6m 59s) Loss: 0.5709(0.5076) Grad: 48793.1172  LR: 0.00000832  \n","Epoch: [3][900/2464] Elapsed 3m 47s (remain 6m 34s) Loss: 0.4580(0.5062) Grad: 48443.6094  LR: 0.00000813  \n","Epoch: [3][1000/2464] Elapsed 4m 12s (remain 6m 8s) Loss: 0.5473(0.5066) Grad: 87736.6875  LR: 0.00000794  \n","Epoch: [3][1100/2464] Elapsed 4m 37s (remain 5m 43s) Loss: 0.4772(0.5069) Grad: 200155.7656  LR: 0.00000775  \n","Epoch: [3][1200/2464] Elapsed 5m 2s (remain 5m 18s) Loss: 0.4359(0.5055) Grad: 162399.4219  LR: 0.00000756  \n","Epoch: [3][1300/2464] Elapsed 5m 27s (remain 4m 53s) Loss: 0.6148(0.5049) Grad: 199805.4531  LR: 0.00000737  \n","Epoch: [3][1400/2464] Elapsed 5m 52s (remain 4m 27s) Loss: 0.5158(0.5050) Grad: 102604.9688  LR: 0.00000718  \n","Epoch: [3][1500/2464] Elapsed 6m 18s (remain 4m 2s) Loss: 0.4827(0.5050) Grad: 73875.0000  LR: 0.00000699  \n","Epoch: [3][1600/2464] Elapsed 6m 43s (remain 3m 37s) Loss: 0.4881(0.5051) Grad: 73070.2734  LR: 0.00000680  \n","Epoch: [3][1700/2464] Elapsed 7m 8s (remain 3m 12s) Loss: 0.5419(0.5050) Grad: 195239.1562  LR: 0.00000661  \n","Epoch: [3][1800/2464] Elapsed 7m 33s (remain 2m 46s) Loss: 0.3952(0.5054) Grad: 36893.7070  LR: 0.00000642  \n","Epoch: [3][1900/2464] Elapsed 7m 58s (remain 2m 21s) Loss: 0.4453(0.5050) Grad: 52905.0195  LR: 0.00000623  \n","Epoch: [3][2000/2464] Elapsed 8m 23s (remain 1m 56s) Loss: 0.5652(0.5055) Grad: 129278.8125  LR: 0.00000604  \n","Epoch: [3][2100/2464] Elapsed 8m 48s (remain 1m 31s) Loss: 0.5002(0.5047) Grad: 526714.6250  LR: 0.00000585  \n","Epoch: [3][2200/2464] Elapsed 9m 13s (remain 1m 6s) Loss: 0.5476(0.5042) Grad: 111828.2422  LR: 0.00000567  \n","Epoch: [3][2300/2464] Elapsed 9m 38s (remain 0m 40s) Loss: 0.5913(0.5042) Grad: 235329.4219  LR: 0.00000548  \n","Epoch: [3][2400/2464] Elapsed 10m 3s (remain 0m 15s) Loss: 0.5039(0.5040) Grad: 76161.6719  LR: 0.00000530  \n","Epoch: [3][2463/2464] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5611(0.5038) Grad: 186875.7188  LR: 0.00000518  \n","EVAL: [0/142] Elapsed 0m 0s (remain 1m 0s) Loss: 0.6680(0.6680) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.8729(0.5603) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5038  avg_val_loss: 0.5619  time: 638s\n","Epoch 3 - Score: 0.8163\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3864(0.5619) \n","Epoch: [4][0/2464] Elapsed 0m 0s (remain 25m 20s) Loss: 0.5243(0.5243) Grad: 71508.0938  LR: 0.00000518  \n","Epoch: [4][100/2464] Elapsed 0m 25s (remain 10m 3s) Loss: 0.3976(0.4920) Grad: 26800.0078  LR: 0.00000500  \n","Epoch: [4][200/2464] Elapsed 0m 51s (remain 9m 34s) Loss: 0.5071(0.4938) Grad: 34702.5508  LR: 0.00000482  \n","Epoch: [4][300/2464] Elapsed 1m 16s (remain 9m 6s) Loss: 0.6398(0.4951) Grad: 553891.6250  LR: 0.00000464  \n","Epoch: [4][400/2464] Elapsed 1m 41s (remain 8m 40s) Loss: 0.4964(0.4948) Grad: 24943.5215  LR: 0.00000447  \n","Epoch: [4][500/2464] Elapsed 2m 6s (remain 8m 15s) Loss: 0.5177(0.4962) Grad: 45253.2695  LR: 0.00000429  \n","Epoch: [4][600/2464] Elapsed 2m 31s (remain 7m 49s) Loss: 0.4454(0.4969) Grad: 747502.5625  LR: 0.00000412  \n","Epoch: [4][700/2464] Elapsed 2m 56s (remain 7m 24s) Loss: 0.4674(0.4989) Grad: 23366.0176  LR: 0.00000395  \n","Epoch: [4][800/2464] Elapsed 3m 21s (remain 6m 59s) Loss: 0.5475(0.4984) Grad: 96198.9453  LR: 0.00000378  \n","Epoch: [4][900/2464] Elapsed 3m 47s (remain 6m 34s) Loss: 0.4014(0.4983) Grad: 39548.7734  LR: 0.00000362  \n","Epoch: [4][1000/2464] Elapsed 4m 12s (remain 6m 8s) Loss: 0.4443(0.4981) Grad: 29752.9219  LR: 0.00000346  \n","Epoch: [4][1100/2464] Elapsed 4m 37s (remain 5m 43s) Loss: 0.3814(0.4983) Grad: 40859.6094  LR: 0.00000330  \n","Epoch: [4][1200/2464] Elapsed 5m 2s (remain 5m 18s) Loss: 0.5206(0.4986) Grad: 22357.4102  LR: 0.00000314  \n","Epoch: [4][1300/2464] Elapsed 5m 28s (remain 4m 53s) Loss: 0.5133(0.4981) Grad: 49718.9180  LR: 0.00000299  \n","Epoch: [4][1400/2464] Elapsed 5m 53s (remain 4m 28s) Loss: 0.5824(0.4979) Grad: 48999.0469  LR: 0.00000283  \n","Epoch: [4][1500/2464] Elapsed 6m 18s (remain 4m 2s) Loss: 0.5551(0.4979) Grad: 40219.8867  LR: 0.00000269  \n","Epoch: [4][1600/2464] Elapsed 6m 43s (remain 3m 37s) Loss: 0.4604(0.4973) Grad: 34371.6406  LR: 0.00000254  \n","Epoch: [4][1700/2464] Elapsed 7m 9s (remain 3m 12s) Loss: 0.4188(0.4969) Grad: 41840.7188  LR: 0.00000240  \n","Epoch: [4][1800/2464] Elapsed 7m 34s (remain 2m 47s) Loss: 0.5607(0.4962) Grad: 119827.6641  LR: 0.00000226  \n","Epoch: [4][1900/2464] Elapsed 7m 59s (remain 2m 21s) Loss: 0.5774(0.4963) Grad: 57573.8984  LR: 0.00000213  \n","Epoch: [4][2000/2464] Elapsed 8m 24s (remain 1m 56s) Loss: 0.4473(0.4965) Grad: 43271.0898  LR: 0.00000199  \n","Epoch: [4][2100/2464] Elapsed 8m 49s (remain 1m 31s) Loss: 0.4670(0.4964) Grad: 108865.4688  LR: 0.00000187  \n","Epoch: [4][2200/2464] Elapsed 9m 14s (remain 1m 6s) Loss: 0.6131(0.4960) Grad: 30876.1973  LR: 0.00000174  \n","Epoch: [4][2300/2464] Elapsed 9m 40s (remain 0m 41s) Loss: 0.5629(0.4950) Grad: 26405.4355  LR: 0.00000162  \n","Epoch: [4][2400/2464] Elapsed 10m 5s (remain 0m 15s) Loss: 0.4668(0.4956) Grad: 49103.5898  LR: 0.00000150  \n","Epoch: [4][2463/2464] Elapsed 10m 21s (remain 0m 0s) Loss: 0.4304(0.4959) Grad: 251059.4688  LR: 0.00000143  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 55s) Loss: 0.7905(0.7905) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.8334(0.5792) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4959  avg_val_loss: 0.5797  time: 640s\n","Epoch 4 - Score: 0.8159\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3377(0.5797) \n","Epoch: [5][0/2464] Elapsed 0m 0s (remain 25m 23s) Loss: 0.4641(0.4641) Grad: 37170.5586  LR: 0.00000143  \n","Epoch: [5][100/2464] Elapsed 0m 25s (remain 10m 5s) Loss: 0.5383(0.4915) Grad: 76100.2656  LR: 0.00000132  \n","Epoch: [5][200/2464] Elapsed 0m 51s (remain 9m 34s) Loss: 0.5228(0.4954) Grad: 149347.5312  LR: 0.00000121  \n","Epoch: [5][300/2464] Elapsed 1m 16s (remain 9m 8s) Loss: 0.4661(0.4960) Grad: 67839.4453  LR: 0.00000111  \n","Epoch: [5][400/2464] Elapsed 1m 41s (remain 8m 42s) Loss: 0.5081(0.4979) Grad: 39410.1602  LR: 0.00000101  \n","Epoch: [5][500/2464] Elapsed 2m 6s (remain 8m 16s) Loss: 0.4323(0.4956) Grad: 44915.3672  LR: 0.00000092  \n","Epoch: [5][600/2464] Elapsed 2m 31s (remain 7m 50s) Loss: 0.5431(0.4950) Grad: 22165.9453  LR: 0.00000083  \n","Epoch: [5][700/2464] Elapsed 2m 57s (remain 7m 25s) Loss: 0.4731(0.4920) Grad: 10030.2676  LR: 0.00000075  \n","Epoch: [5][800/2464] Elapsed 3m 22s (remain 6m 59s) Loss: 0.5183(0.4931) Grad: 24138.1855  LR: 0.00000066  \n","Epoch: [5][900/2464] Elapsed 3m 47s (remain 6m 34s) Loss: 0.3623(0.4936) Grad: 16481.0078  LR: 0.00000059  \n","Epoch: [5][1000/2464] Elapsed 4m 12s (remain 6m 9s) Loss: 0.5266(0.4942) Grad: 14498.4414  LR: 0.00000052  \n","Epoch: [5][1100/2464] Elapsed 4m 37s (remain 5m 44s) Loss: 0.5373(0.4952) Grad: 15712.7314  LR: 0.00000045  \n","Epoch: [5][1200/2464] Elapsed 5m 3s (remain 5m 18s) Loss: 0.5359(0.4944) Grad: 17161.9434  LR: 0.00000039  \n","Epoch: [5][1300/2464] Elapsed 5m 28s (remain 4m 53s) Loss: 0.5904(0.4954) Grad: 23277.7188  LR: 0.00000033  \n","Epoch: [5][1400/2464] Elapsed 5m 53s (remain 4m 28s) Loss: 0.4335(0.4954) Grad: 24183.4395  LR: 0.00000027  \n","Epoch: [5][1500/2464] Elapsed 6m 19s (remain 4m 3s) Loss: 0.5154(0.4949) Grad: 42004.2617  LR: 0.00000022  \n","Epoch: [5][1600/2464] Elapsed 6m 44s (remain 3m 37s) Loss: 0.4891(0.4953) Grad: 36422.4414  LR: 0.00000018  \n","Epoch: [5][1700/2464] Elapsed 7m 9s (remain 3m 12s) Loss: 0.4500(0.4945) Grad: 28151.0742  LR: 0.00000014  \n","Epoch: [5][1800/2464] Elapsed 7m 34s (remain 2m 47s) Loss: 0.5592(0.4939) Grad: 24668.2266  LR: 0.00000011  \n","Epoch: [5][1900/2464] Elapsed 7m 59s (remain 2m 22s) Loss: 0.4856(0.4933) Grad: 33239.1328  LR: 0.00000008  \n","Epoch: [5][2000/2464] Elapsed 8m 25s (remain 1m 56s) Loss: 0.5175(0.4929) Grad: 33068.5586  LR: 0.00000005  \n","Epoch: [5][2100/2464] Elapsed 8m 50s (remain 1m 31s) Loss: 0.5308(0.4926) Grad: 16844.4238  LR: 0.00000003  \n","Epoch: [5][2200/2464] Elapsed 9m 15s (remain 1m 6s) Loss: 0.4699(0.4929) Grad: 18549.9844  LR: 0.00000002  \n","Epoch: [5][2300/2464] Elapsed 9m 40s (remain 0m 41s) Loss: 0.5228(0.4928) Grad: 17737.1680  LR: 0.00000001  \n","Epoch: [5][2400/2464] Elapsed 10m 6s (remain 0m 15s) Loss: 0.4429(0.4920) Grad: 22463.4043  LR: 0.00000000  \n","Epoch: [5][2463/2464] Elapsed 10m 22s (remain 0m 0s) Loss: 0.5059(0.4918) Grad: 66226.4062  LR: 0.00000000  \n","EVAL: [0/142] Elapsed 0m 0s (remain 0m 55s) Loss: 0.7527(0.7527) \n","EVAL: [100/142] Elapsed 0m 13s (remain 0m 5s) Loss: 0.8148(0.5772) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4918  avg_val_loss: 0.5776  time: 641s\n","Epoch 5 - Score: 0.8177\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [141/142] Elapsed 0m 18s (remain 0m 0s) Loss: 0.3670(0.5776) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 9 result ==========\n","Score: 0.8245\n","========== fold: 10 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2477] Elapsed 0m 0s (remain 23m 41s) Loss: 0.7137(0.7137) Grad: 155449.0312  LR: 0.00001500  \n","Epoch: [1][100/2477] Elapsed 0m 25s (remain 10m 7s) Loss: 0.4928(0.6344) Grad: 83013.7031  LR: 0.00001500  \n","Epoch: [1][200/2477] Elapsed 0m 51s (remain 9m 38s) Loss: 0.6920(0.6102) Grad: 74720.8750  LR: 0.00001499  \n","Epoch: [1][300/2477] Elapsed 1m 16s (remain 9m 12s) Loss: 0.4543(0.5988) Grad: 32390.0957  LR: 0.00001498  \n","Epoch: [1][400/2477] Elapsed 1m 41s (remain 8m 45s) Loss: 0.6017(0.5868) Grad: 76820.8438  LR: 0.00001496  \n","Epoch: [1][500/2477] Elapsed 2m 6s (remain 8m 20s) Loss: 0.5688(0.5782) Grad: 65105.4609  LR: 0.00001494  \n","Epoch: [1][600/2477] Elapsed 2m 32s (remain 7m 54s) Loss: 0.4552(0.5756) Grad: 38093.5352  LR: 0.00001491  \n","Epoch: [1][700/2477] Elapsed 2m 57s (remain 7m 29s) Loss: 0.5649(0.5739) Grad: 32625.0195  LR: 0.00001488  \n","Epoch: [1][800/2477] Elapsed 3m 22s (remain 7m 3s) Loss: 0.6663(0.5737) Grad: 42149.7227  LR: 0.00001485  \n","Epoch: [1][900/2477] Elapsed 3m 47s (remain 6m 38s) Loss: 0.5122(0.5729) Grad: 9415.2881  LR: 0.00001481  \n","Epoch: [1][1000/2477] Elapsed 4m 13s (remain 6m 13s) Loss: 0.5291(0.5714) Grad: 16050.9121  LR: 0.00001476  \n","Epoch: [1][1100/2477] Elapsed 4m 38s (remain 5m 47s) Loss: 0.5284(0.5714) Grad: 40165.3047  LR: 0.00001471  \n","Epoch: [1][1200/2477] Elapsed 5m 3s (remain 5m 22s) Loss: 0.6257(0.5693) Grad: 8478.4941  LR: 0.00001465  \n","Epoch: [1][1300/2477] Elapsed 5m 28s (remain 4m 57s) Loss: 0.5574(0.5677) Grad: 12401.7773  LR: 0.00001460  \n","Epoch: [1][1400/2477] Elapsed 5m 54s (remain 4m 31s) Loss: 0.5756(0.5667) Grad: 18340.2207  LR: 0.00001453  \n","Epoch: [1][1500/2477] Elapsed 6m 19s (remain 4m 6s) Loss: 0.5158(0.5652) Grad: 4411.6021  LR: 0.00001446  \n","Epoch: [1][1600/2477] Elapsed 6m 44s (remain 3m 41s) Loss: 0.5490(0.5651) Grad: 11084.2354  LR: 0.00001439  \n","Epoch: [1][1700/2477] Elapsed 7m 9s (remain 3m 16s) Loss: 0.4729(0.5640) Grad: 14039.0176  LR: 0.00001431  \n","Epoch: [1][1800/2477] Elapsed 7m 34s (remain 2m 50s) Loss: 0.5824(0.5636) Grad: 6095.4062  LR: 0.00001423  \n","Epoch: [1][1900/2477] Elapsed 8m 0s (remain 2m 25s) Loss: 0.5912(0.5627) Grad: 9941.1602  LR: 0.00001415  \n","Epoch: [1][2000/2477] Elapsed 8m 25s (remain 2m 0s) Loss: 0.6535(0.5617) Grad: 141004.4688  LR: 0.00001405  \n","Epoch: [1][2100/2477] Elapsed 8m 50s (remain 1m 34s) Loss: 0.6072(0.5607) Grad: 9323.1221  LR: 0.00001396  \n","Epoch: [1][2200/2477] Elapsed 9m 15s (remain 1m 9s) Loss: 0.6360(0.5599) Grad: 33301.2852  LR: 0.00001386  \n","Epoch: [1][2300/2477] Elapsed 9m 40s (remain 0m 44s) Loss: 0.5355(0.5595) Grad: 10631.9912  LR: 0.00001376  \n","Epoch: [1][2400/2477] Elapsed 10m 5s (remain 0m 19s) Loss: 0.4727(0.5584) Grad: 14431.6807  LR: 0.00001365  \n","Epoch: [1][2476/2477] Elapsed 10m 25s (remain 0m 0s) Loss: 0.5204(0.5579) Grad: 3059.4392  LR: 0.00001357  \n","EVAL: [0/128] Elapsed 0m 0s (remain 0m 51s) Loss: 0.3876(0.3876) \n","EVAL: [100/128] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7894(0.5346) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5579  avg_val_loss: 0.5391  time: 642s\n","Epoch 1 - Score: 0.8427\n","Epoch 1 - Save Best Score: 0.8427 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [127/128] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6372(0.5391) \n","Epoch: [2][0/2477] Elapsed 0m 0s (remain 25m 37s) Loss: 0.5231(0.5231) Grad: 158925.3906  LR: 0.00001357  \n","Epoch: [2][100/2477] Elapsed 0m 26s (remain 10m 12s) Loss: 0.5827(0.5244) Grad: 75759.1641  LR: 0.00001345  \n","Epoch: [2][200/2477] Elapsed 0m 51s (remain 9m 45s) Loss: 0.5857(0.5240) Grad: 81345.7422  LR: 0.00001334  \n","Epoch: [2][300/2477] Elapsed 1m 16s (remain 9m 16s) Loss: 0.5673(0.5174) Grad: 38474.7539  LR: 0.00001321  \n","Epoch: [2][400/2477] Elapsed 1m 42s (remain 8m 48s) Loss: 0.5465(0.5185) Grad: 40093.5391  LR: 0.00001309  \n","Epoch: [2][500/2477] Elapsed 2m 7s (remain 8m 22s) Loss: 0.5265(0.5172) Grad: 159856.8750  LR: 0.00001296  \n","Epoch: [2][600/2477] Elapsed 2m 32s (remain 7m 56s) Loss: 0.5694(0.5166) Grad: 85264.1016  LR: 0.00001283  \n","Epoch: [2][700/2477] Elapsed 2m 57s (remain 7m 30s) Loss: 0.6261(0.5179) Grad: 301210.5938  LR: 0.00001269  \n","Epoch: [2][800/2477] Elapsed 3m 22s (remain 7m 4s) Loss: 0.4636(0.5181) Grad: 85950.0938  LR: 0.00001255  \n","Epoch: [2][900/2477] Elapsed 3m 48s (remain 6m 38s) Loss: 0.5484(0.5163) Grad: 86504.0391  LR: 0.00001241  \n","Epoch: [2][1000/2477] Elapsed 4m 13s (remain 6m 13s) Loss: 0.4079(0.5167) Grad: 86023.6328  LR: 0.00001227  \n","Epoch: [2][1100/2477] Elapsed 4m 38s (remain 5m 47s) Loss: 0.5290(0.5176) Grad: 183955.5312  LR: 0.00001212  \n","Epoch: [2][1200/2477] Elapsed 5m 3s (remain 5m 22s) Loss: 0.4333(0.5176) Grad: 74135.5391  LR: 0.00001197  \n","Epoch: [2][1300/2477] Elapsed 5m 28s (remain 4m 57s) Loss: 0.4632(0.5172) Grad: 67909.5078  LR: 0.00001181  \n","Epoch: [2][1400/2477] Elapsed 5m 54s (remain 4m 32s) Loss: 0.5729(0.5169) Grad: 45146.0742  LR: 0.00001166  \n","Epoch: [2][1500/2477] Elapsed 6m 19s (remain 4m 6s) Loss: 0.5030(0.5161) Grad: 74303.5234  LR: 0.00001150  \n","Epoch: [2][1600/2477] Elapsed 6m 44s (remain 3m 41s) Loss: 0.4648(0.5159) Grad: 119987.0078  LR: 0.00001133  \n","Epoch: [2][1700/2477] Elapsed 7m 9s (remain 3m 16s) Loss: 0.4281(0.5157) Grad: 49978.2852  LR: 0.00001117  \n","Epoch: [2][1800/2477] Elapsed 7m 35s (remain 2m 50s) Loss: 0.4160(0.5164) Grad: 70396.7734  LR: 0.00001100  \n","Epoch: [2][1900/2477] Elapsed 8m 0s (remain 2m 25s) Loss: 0.4683(0.5166) Grad: 140603.7812  LR: 0.00001083  \n","Epoch: [2][2000/2477] Elapsed 8m 25s (remain 2m 0s) Loss: 0.5522(0.5173) Grad: 42156.9961  LR: 0.00001066  \n","Epoch: [2][2100/2477] Elapsed 8m 50s (remain 1m 34s) Loss: 0.5702(0.5171) Grad: 29401.1348  LR: 0.00001049  \n","Epoch: [2][2200/2477] Elapsed 9m 15s (remain 1m 9s) Loss: 0.4558(0.5170) Grad: 20836.2910  LR: 0.00001031  \n","Epoch: [2][2300/2477] Elapsed 9m 40s (remain 0m 44s) Loss: 0.4877(0.5171) Grad: 130329.4375  LR: 0.00001014  \n","Epoch: [2][2400/2477] Elapsed 10m 6s (remain 0m 19s) Loss: 0.6146(0.5170) Grad: 25277.9062  LR: 0.00000996  \n","Epoch: [2][2476/2477] Elapsed 10m 25s (remain 0m 0s) Loss: 0.5009(0.5165) Grad: 42421.2539  LR: 0.00000982  \n","EVAL: [0/128] Elapsed 0m 0s (remain 0m 51s) Loss: 0.3729(0.3729) \n","EVAL: [100/128] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7074(0.5279) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5165  avg_val_loss: 0.5327  time: 642s\n","Epoch 2 - Score: 0.8542\n","Epoch 2 - Save Best Score: 0.8542 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [127/128] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6245(0.5327) \n","Epoch: [3][0/2477] Elapsed 0m 0s (remain 26m 27s) Loss: 0.4796(0.4796) Grad: 41885.4766  LR: 0.00000982  \n","Epoch: [3][100/2477] Elapsed 0m 26s (remain 10m 15s) Loss: 0.5730(0.5072) Grad: 31412.0586  LR: 0.00000964  \n","Epoch: [3][200/2477] Elapsed 0m 51s (remain 9m 45s) Loss: 0.3955(0.5093) Grad: 53253.2812  LR: 0.00000945  \n","Epoch: [3][300/2477] Elapsed 1m 16s (remain 9m 15s) Loss: 0.5582(0.5075) Grad: 59949.5859  LR: 0.00000927  \n","Epoch: [3][400/2477] Elapsed 1m 41s (remain 8m 47s) Loss: 0.5054(0.5060) Grad: 40579.7461  LR: 0.00000908  \n","Epoch: [3][500/2477] Elapsed 2m 7s (remain 8m 21s) Loss: 0.4608(0.5032) Grad: 126629.3125  LR: 0.00000890  \n","Epoch: [3][600/2477] Elapsed 2m 32s (remain 7m 55s) Loss: 0.5316(0.5032) Grad: 21340.5957  LR: 0.00000871  \n","Epoch: [3][700/2477] Elapsed 2m 57s (remain 7m 29s) Loss: 0.4447(0.5044) Grad: 6836.1133  LR: 0.00000852  \n","Epoch: [3][800/2477] Elapsed 3m 22s (remain 7m 3s) Loss: 0.5674(0.5036) Grad: 231673.2656  LR: 0.00000833  \n","Epoch: [3][900/2477] Elapsed 3m 47s (remain 6m 38s) Loss: 0.4596(0.5033) Grad: 25955.1152  LR: 0.00000814  \n","Epoch: [3][1000/2477] Elapsed 4m 12s (remain 6m 12s) Loss: 0.7684(0.5040) Grad: 845856.1250  LR: 0.00000795  \n","Epoch: [3][1100/2477] Elapsed 4m 38s (remain 5m 47s) Loss: 0.5256(0.5050) Grad: 22568.1406  LR: 0.00000776  \n","Epoch: [3][1200/2477] Elapsed 5m 3s (remain 5m 22s) Loss: 0.5948(0.5057) Grad: 41185.4375  LR: 0.00000757  \n","Epoch: [3][1300/2477] Elapsed 5m 28s (remain 4m 56s) Loss: 0.5739(0.5066) Grad: 109060.2188  LR: 0.00000738  \n","Epoch: [3][1400/2477] Elapsed 5m 53s (remain 4m 31s) Loss: 0.4851(0.5066) Grad: 48896.3711  LR: 0.00000719  \n","Epoch: [3][1500/2477] Elapsed 6m 18s (remain 4m 6s) Loss: 0.5735(0.5067) Grad: 63521.3086  LR: 0.00000700  \n","Epoch: [3][1600/2477] Elapsed 6m 44s (remain 3m 41s) Loss: 0.5276(0.5065) Grad: 30262.9160  LR: 0.00000681  \n","Epoch: [3][1700/2477] Elapsed 7m 9s (remain 3m 15s) Loss: 0.4192(0.5067) Grad: 39992.9688  LR: 0.00000662  \n","Epoch: [3][1800/2477] Elapsed 7m 34s (remain 2m 50s) Loss: 0.6000(0.5064) Grad: 32951.7539  LR: 0.00000644  \n","Epoch: [3][1900/2477] Elapsed 7m 59s (remain 2m 25s) Loss: 0.3989(0.5070) Grad: 25477.0605  LR: 0.00000625  \n","Epoch: [3][2000/2477] Elapsed 8m 24s (remain 2m 0s) Loss: 0.4739(0.5068) Grad: 90770.0000  LR: 0.00000606  \n","Epoch: [3][2100/2477] Elapsed 8m 49s (remain 1m 34s) Loss: 0.4652(0.5076) Grad: 46285.7617  LR: 0.00000587  \n","Epoch: [3][2200/2477] Elapsed 9m 14s (remain 1m 9s) Loss: 0.4905(0.5074) Grad: 25674.7930  LR: 0.00000569  \n","Epoch: [3][2300/2477] Elapsed 9m 39s (remain 0m 44s) Loss: 0.6231(0.5080) Grad: 30437.0078  LR: 0.00000551  \n","Epoch: [3][2400/2477] Elapsed 10m 5s (remain 0m 19s) Loss: 0.6012(0.5081) Grad: 37121.8438  LR: 0.00000532  \n","Epoch: [3][2476/2477] Elapsed 10m 24s (remain 0m 0s) Loss: 0.3951(0.5079) Grad: 38331.4766  LR: 0.00000518  \n","EVAL: [0/128] Elapsed 0m 0s (remain 0m 52s) Loss: 0.3747(0.3747) \n","EVAL: [100/128] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7439(0.5339) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5079  avg_val_loss: 0.5397  time: 641s\n","Epoch 3 - Score: 0.8572\n","Epoch 3 - Save Best Score: 0.8572 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [127/128] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6298(0.5397) \n","Epoch: [4][0/2477] Elapsed 0m 0s (remain 26m 34s) Loss: 0.4756(0.4756) Grad: 40923.3750  LR: 0.00000518  \n","Epoch: [4][100/2477] Elapsed 0m 26s (remain 10m 17s) Loss: 0.3937(0.4937) Grad: 64640.3281  LR: 0.00000500  \n","Epoch: [4][200/2477] Elapsed 0m 51s (remain 9m 46s) Loss: 0.5570(0.4959) Grad: 46456.4219  LR: 0.00000482  \n","Epoch: [4][300/2477] Elapsed 1m 16s (remain 9m 15s) Loss: 0.3524(0.4958) Grad: 359563.7500  LR: 0.00000465  \n","Epoch: [4][400/2477] Elapsed 1m 42s (remain 8m 48s) Loss: 0.4204(0.4960) Grad: 51400.6719  LR: 0.00000447  \n","Epoch: [4][500/2477] Elapsed 2m 7s (remain 8m 21s) Loss: 0.4606(0.4966) Grad: 168875.3125  LR: 0.00000430  \n","Epoch: [4][600/2477] Elapsed 2m 32s (remain 7m 55s) Loss: 0.5802(0.4960) Grad: 36780.1211  LR: 0.00000413  \n","Epoch: [4][700/2477] Elapsed 2m 57s (remain 7m 29s) Loss: 0.5615(0.4969) Grad: 91706.3672  LR: 0.00000396  \n","Epoch: [4][800/2477] Elapsed 3m 22s (remain 7m 4s) Loss: 0.4882(0.4978) Grad: 70755.4219  LR: 0.00000379  \n","Epoch: [4][900/2477] Elapsed 3m 47s (remain 6m 38s) Loss: 0.3998(0.4962) Grad: 60613.8906  LR: 0.00000363  \n","Epoch: [4][1000/2477] Elapsed 4m 13s (remain 6m 13s) Loss: 0.5996(0.4966) Grad: 167697.4219  LR: 0.00000347  \n","Epoch: [4][1100/2477] Elapsed 4m 38s (remain 5m 47s) Loss: 0.4110(0.4961) Grad: 45114.5078  LR: 0.00000331  \n","Epoch: [4][1200/2477] Elapsed 5m 3s (remain 5m 22s) Loss: 0.5839(0.4965) Grad: 82455.1953  LR: 0.00000315  \n","Epoch: [4][1300/2477] Elapsed 5m 29s (remain 4m 57s) Loss: 0.4142(0.4957) Grad: 70318.9844  LR: 0.00000300  \n","Epoch: [4][1400/2477] Elapsed 5m 54s (remain 4m 32s) Loss: 0.6651(0.4954) Grad: 41370.1445  LR: 0.00000285  \n","Epoch: [4][1500/2477] Elapsed 6m 19s (remain 4m 6s) Loss: 0.5510(0.4958) Grad: 49684.5664  LR: 0.00000270  \n","Epoch: [4][1600/2477] Elapsed 6m 44s (remain 3m 41s) Loss: 0.5592(0.4971) Grad: 69301.1016  LR: 0.00000256  \n","Epoch: [4][1700/2477] Elapsed 7m 9s (remain 3m 15s) Loss: 0.4624(0.4971) Grad: 22354.0566  LR: 0.00000241  \n","Epoch: [4][1800/2477] Elapsed 7m 34s (remain 2m 50s) Loss: 0.4727(0.4973) Grad: 279077.6250  LR: 0.00000228  \n","Epoch: [4][1900/2477] Elapsed 7m 59s (remain 2m 25s) Loss: 0.5565(0.4975) Grad: 40392.2266  LR: 0.00000214  \n","Epoch: [4][2000/2477] Elapsed 8m 25s (remain 2m 0s) Loss: 0.4335(0.4979) Grad: 261857.9688  LR: 0.00000201  \n","Epoch: [4][2100/2477] Elapsed 8m 50s (remain 1m 34s) Loss: 0.5070(0.4978) Grad: 129386.6484  LR: 0.00000188  \n","Epoch: [4][2200/2477] Elapsed 9m 15s (remain 1m 9s) Loss: 0.3975(0.4977) Grad: 214286.7656  LR: 0.00000176  \n","Epoch: [4][2300/2477] Elapsed 9m 40s (remain 0m 44s) Loss: 0.4661(0.4980) Grad: 69737.6016  LR: 0.00000164  \n","Epoch: [4][2400/2477] Elapsed 10m 5s (remain 0m 19s) Loss: 0.3547(0.4977) Grad: 57567.6484  LR: 0.00000152  \n","Epoch: [4][2476/2477] Elapsed 10m 25s (remain 0m 0s) Loss: 0.4846(0.4979) Grad: 401794.4375  LR: 0.00000143  \n","EVAL: [0/128] Elapsed 0m 0s (remain 0m 51s) Loss: 0.3707(0.3707) \n","EVAL: [100/128] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7432(0.5371) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4979  avg_val_loss: 0.5432  time: 642s\n","Epoch 4 - Score: 0.8582\n","Epoch 4 - Save Best Score: 0.8582 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [127/128] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6470(0.5432) \n","Epoch: [5][0/2477] Elapsed 0m 0s (remain 25m 57s) Loss: 0.5001(0.5001) Grad: 27971.9883  LR: 0.00000143  \n","Epoch: [5][100/2477] Elapsed 0m 26s (remain 10m 13s) Loss: 0.4654(0.4925) Grad: 25022.2285  LR: 0.00000132  \n","Epoch: [5][200/2477] Elapsed 0m 51s (remain 9m 44s) Loss: 0.5284(0.4918) Grad: 35740.0625  LR: 0.00000122  \n","Epoch: [5][300/2477] Elapsed 1m 16s (remain 9m 15s) Loss: 0.5271(0.4913) Grad: 31709.4551  LR: 0.00000112  \n","Epoch: [5][400/2477] Elapsed 1m 42s (remain 8m 48s) Loss: 0.5773(0.4952) Grad: 81559.5938  LR: 0.00000102  \n","Epoch: [5][500/2477] Elapsed 2m 7s (remain 8m 22s) Loss: 0.4578(0.4939) Grad: 45261.6523  LR: 0.00000092  \n","Epoch: [5][600/2477] Elapsed 2m 32s (remain 7m 56s) Loss: 0.4003(0.4927) Grad: 12453.4707  LR: 0.00000083  \n","Epoch: [5][700/2477] Elapsed 2m 57s (remain 7m 30s) Loss: 0.5893(0.4934) Grad: 75101.1328  LR: 0.00000075  \n","Epoch: [5][800/2477] Elapsed 3m 22s (remain 7m 4s) Loss: 0.5599(0.4922) Grad: 43451.4492  LR: 0.00000067  \n","Epoch: [5][900/2477] Elapsed 3m 48s (remain 6m 39s) Loss: 0.6464(0.4930) Grad: 228342.4375  LR: 0.00000059  \n","Epoch: [5][1000/2477] Elapsed 4m 13s (remain 6m 13s) Loss: 0.4551(0.4930) Grad: 33016.6992  LR: 0.00000052  \n","Epoch: [5][1100/2477] Elapsed 4m 38s (remain 5m 48s) Loss: 0.4885(0.4924) Grad: 51200.3164  LR: 0.00000045  \n","Epoch: [5][1200/2477] Elapsed 5m 3s (remain 5m 22s) Loss: 0.3915(0.4926) Grad: 59967.8203  LR: 0.00000039  \n","Epoch: [5][1300/2477] Elapsed 5m 29s (remain 4m 57s) Loss: 0.4081(0.4925) Grad: 137140.4219  LR: 0.00000033  \n","Epoch: [5][1400/2477] Elapsed 5m 54s (remain 4m 32s) Loss: 0.5203(0.4927) Grad: 21429.6875  LR: 0.00000028  \n","Epoch: [5][1500/2477] Elapsed 6m 19s (remain 4m 6s) Loss: 0.4187(0.4945) Grad: 33263.2266  LR: 0.00000023  \n","Epoch: [5][1600/2477] Elapsed 6m 44s (remain 3m 41s) Loss: 0.5370(0.4947) Grad: 100777.4531  LR: 0.00000019  \n","Epoch: [5][1700/2477] Elapsed 7m 10s (remain 3m 16s) Loss: 0.4249(0.4946) Grad: 28478.4648  LR: 0.00000015  \n","Epoch: [5][1800/2477] Elapsed 7m 35s (remain 2m 50s) Loss: 0.5455(0.4950) Grad: 68430.5859  LR: 0.00000011  \n","Epoch: [5][1900/2477] Elapsed 8m 0s (remain 2m 25s) Loss: 0.5199(0.4950) Grad: 64398.0977  LR: 0.00000008  \n","Epoch: [5][2000/2477] Elapsed 8m 25s (remain 2m 0s) Loss: 0.4785(0.4947) Grad: 393039.7188  LR: 0.00000006  \n","Epoch: [5][2100/2477] Elapsed 8m 50s (remain 1m 34s) Loss: 0.4564(0.4949) Grad: 42373.2852  LR: 0.00000003  \n","Epoch: [5][2200/2477] Elapsed 9m 15s (remain 1m 9s) Loss: 0.5067(0.4948) Grad: 72863.4297  LR: 0.00000002  \n","Epoch: [5][2300/2477] Elapsed 9m 41s (remain 0m 44s) Loss: 0.5549(0.4948) Grad: 84553.0938  LR: 0.00000001  \n","Epoch: [5][2400/2477] Elapsed 10m 6s (remain 0m 19s) Loss: 0.4447(0.4946) Grad: 23561.5547  LR: 0.00000000  \n","Epoch: [5][2476/2477] Elapsed 10m 25s (remain 0m 0s) Loss: 0.4165(0.4939) Grad: 48782.7695  LR: 0.00000000  \n","EVAL: [0/128] Elapsed 0m 0s (remain 0m 50s) Loss: 0.3699(0.3699) \n","EVAL: [100/128] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7417(0.5376) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4939  avg_val_loss: 0.5438  time: 643s\n","Epoch 5 - Score: 0.8587\n","Epoch 5 - Save Best Score: 0.8587 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [127/128] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6588(0.5438) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 10 result ==========\n","Score: 0.8587\n","========== fold: 11 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2481] Elapsed 0m 0s (remain 21m 54s) Loss: 0.7473(0.7473) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2481] Elapsed 0m 26s (remain 10m 13s) Loss: 0.6561(0.6500) Grad: 35950.1523  LR: 0.00001500  \n","Epoch: [1][200/2481] Elapsed 0m 51s (remain 9m 44s) Loss: 0.6506(0.6318) Grad: 60095.0781  LR: 0.00001499  \n","Epoch: [1][300/2481] Elapsed 1m 16s (remain 9m 15s) Loss: 0.6451(0.6155) Grad: 122734.5781  LR: 0.00001498  \n","Epoch: [1][400/2481] Elapsed 1m 41s (remain 8m 48s) Loss: 0.5321(0.6058) Grad: 18028.1660  LR: 0.00001496  \n","Epoch: [1][500/2481] Elapsed 2m 6s (remain 8m 21s) Loss: 0.6256(0.5986) Grad: 20860.3691  LR: 0.00001494  \n","Epoch: [1][600/2481] Elapsed 2m 32s (remain 7m 56s) Loss: 0.5936(0.5925) Grad: 14052.8691  LR: 0.00001491  \n","Epoch: [1][700/2481] Elapsed 2m 57s (remain 7m 30s) Loss: 0.7101(0.5894) Grad: 32125.8828  LR: 0.00001488  \n","Epoch: [1][800/2481] Elapsed 3m 22s (remain 7m 4s) Loss: 0.6147(0.5854) Grad: 8276.7080  LR: 0.00001485  \n","Epoch: [1][900/2481] Elapsed 3m 47s (remain 6m 39s) Loss: 0.5929(0.5814) Grad: 13440.0713  LR: 0.00001481  \n","Epoch: [1][1000/2481] Elapsed 4m 13s (remain 6m 14s) Loss: 0.2897(0.5783) Grad: 12970.7568  LR: 0.00001476  \n","Epoch: [1][1100/2481] Elapsed 4m 38s (remain 5m 48s) Loss: 0.5045(0.5763) Grad: 21561.7148  LR: 0.00001471  \n","Epoch: [1][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.4349(0.5739) Grad: 6844.6890  LR: 0.00001466  \n","Epoch: [1][1300/2481] Elapsed 5m 28s (remain 4m 58s) Loss: 0.5454(0.5722) Grad: 20349.2109  LR: 0.00001460  \n","Epoch: [1][1400/2481] Elapsed 5m 54s (remain 4m 32s) Loss: 0.6129(0.5707) Grad: 25444.8555  LR: 0.00001453  \n","Epoch: [1][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.5508(0.5701) Grad: 10806.7715  LR: 0.00001446  \n","Epoch: [1][1600/2481] Elapsed 6m 44s (remain 3m 42s) Loss: 0.5293(0.5679) Grad: 8234.0195  LR: 0.00001439  \n","Epoch: [1][1700/2481] Elapsed 7m 9s (remain 3m 16s) Loss: 0.5938(0.5670) Grad: 18000.7285  LR: 0.00001432  \n","Epoch: [1][1800/2481] Elapsed 7m 34s (remain 2m 51s) Loss: 0.5394(0.5662) Grad: 14168.1396  LR: 0.00001423  \n","Epoch: [1][1900/2481] Elapsed 7m 59s (remain 2m 26s) Loss: 0.5360(0.5648) Grad: 13763.9258  LR: 0.00001415  \n","Epoch: [1][2000/2481] Elapsed 8m 24s (remain 2m 1s) Loss: 0.4491(0.5637) Grad: 10112.9014  LR: 0.00001406  \n","Epoch: [1][2100/2481] Elapsed 8m 49s (remain 1m 35s) Loss: 0.5949(0.5633) Grad: 10225.6689  LR: 0.00001396  \n","Epoch: [1][2200/2481] Elapsed 9m 15s (remain 1m 10s) Loss: 0.5906(0.5612) Grad: 32827.8750  LR: 0.00001387  \n","Epoch: [1][2300/2481] Elapsed 9m 40s (remain 0m 45s) Loss: 0.4411(0.5607) Grad: 31110.1523  LR: 0.00001376  \n","Epoch: [1][2400/2481] Elapsed 10m 5s (remain 0m 20s) Loss: 0.5115(0.5598) Grad: 13175.5205  LR: 0.00001366  \n","Epoch: [1][2480/2481] Elapsed 10m 25s (remain 0m 0s) Loss: 0.7129(0.5593) Grad: 134779.0938  LR: 0.00001357  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 48s) Loss: 0.3154(0.3154) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5761(0.5499) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5593  avg_val_loss: 0.5455  time: 642s\n","Epoch 1 - Score: 0.8401\n","Epoch 1 - Save Best Score: 0.8401 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4482(0.5455) \n","Epoch: [2][0/2481] Elapsed 0m 0s (remain 25m 42s) Loss: 0.5754(0.5754) Grad: 58994.1680  LR: 0.00001357  \n","Epoch: [2][100/2481] Elapsed 0m 26s (remain 10m 16s) Loss: 0.4964(0.5243) Grad: 150181.9688  LR: 0.00001345  \n","Epoch: [2][200/2481] Elapsed 0m 51s (remain 9m 45s) Loss: 0.4577(0.5226) Grad: 26860.4316  LR: 0.00001334  \n","Epoch: [2][300/2481] Elapsed 1m 16s (remain 9m 16s) Loss: 0.5668(0.5240) Grad: 168037.6562  LR: 0.00001322  \n","Epoch: [2][400/2481] Elapsed 1m 41s (remain 8m 48s) Loss: 0.4921(0.5226) Grad: 74848.0547  LR: 0.00001309  \n","Epoch: [2][500/2481] Elapsed 2m 7s (remain 8m 22s) Loss: 0.5982(0.5219) Grad: 172650.7500  LR: 0.00001296  \n","Epoch: [2][600/2481] Elapsed 2m 32s (remain 7m 56s) Loss: 0.5358(0.5218) Grad: 65293.2305  LR: 0.00001283  \n","Epoch: [2][700/2481] Elapsed 2m 57s (remain 7m 30s) Loss: 0.5989(0.5203) Grad: 358943.2500  LR: 0.00001269  \n","Epoch: [2][800/2481] Elapsed 3m 22s (remain 7m 5s) Loss: 0.5732(0.5195) Grad: 83886.0781  LR: 0.00001256  \n","Epoch: [2][900/2481] Elapsed 3m 47s (remain 6m 39s) Loss: 0.3620(0.5191) Grad: 155410.1562  LR: 0.00001241  \n","Epoch: [2][1000/2481] Elapsed 4m 13s (remain 6m 14s) Loss: 0.5120(0.5179) Grad: 33551.8242  LR: 0.00001227  \n","Epoch: [2][1100/2481] Elapsed 4m 38s (remain 5m 48s) Loss: 0.4414(0.5175) Grad: 66621.1719  LR: 0.00001212  \n","Epoch: [2][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.5740(0.5165) Grad: 106678.3906  LR: 0.00001197  \n","Epoch: [2][1300/2481] Elapsed 5m 28s (remain 4m 58s) Loss: 0.6100(0.5163) Grad: 100231.2734  LR: 0.00001182  \n","Epoch: [2][1400/2481] Elapsed 5m 53s (remain 4m 32s) Loss: 0.5212(0.5167) Grad: 251352.6562  LR: 0.00001166  \n","Epoch: [2][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.6211(0.5160) Grad: 211869.4531  LR: 0.00001150  \n","Epoch: [2][1600/2481] Elapsed 6m 44s (remain 3m 42s) Loss: 0.5860(0.5172) Grad: 85660.8359  LR: 0.00001134  \n","Epoch: [2][1700/2481] Elapsed 7m 9s (remain 3m 16s) Loss: 0.6592(0.5177) Grad: 88284.5078  LR: 0.00001117  \n","Epoch: [2][1800/2481] Elapsed 7m 34s (remain 2m 51s) Loss: 0.4302(0.5177) Grad: 100621.8438  LR: 0.00001101  \n","Epoch: [2][1900/2481] Elapsed 7m 59s (remain 2m 26s) Loss: 0.5643(0.5174) Grad: 49058.7461  LR: 0.00001084  \n","Epoch: [2][2000/2481] Elapsed 8m 25s (remain 2m 1s) Loss: 0.5350(0.5174) Grad: 153231.2344  LR: 0.00001067  \n","Epoch: [2][2100/2481] Elapsed 8m 50s (remain 1m 35s) Loss: 0.5564(0.5179) Grad: 89073.5312  LR: 0.00001049  \n","Epoch: [2][2200/2481] Elapsed 9m 15s (remain 1m 10s) Loss: 0.5151(0.5171) Grad: 61991.8438  LR: 0.00001032  \n","Epoch: [2][2300/2481] Elapsed 9m 40s (remain 0m 45s) Loss: 0.5244(0.5169) Grad: 337490.3438  LR: 0.00001014  \n","Epoch: [2][2400/2481] Elapsed 10m 5s (remain 0m 20s) Loss: 0.5393(0.5164) Grad: 78799.2734  LR: 0.00000996  \n","Epoch: [2][2480/2481] Elapsed 10m 25s (remain 0m 0s) Loss: 0.5755(0.5163) Grad: 191997.1250  LR: 0.00000982  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 46s) Loss: 0.3102(0.3102) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5621(0.5472) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5163  avg_val_loss: 0.5427  time: 642s\n","Epoch 2 - Score: 0.8458\n","Epoch 2 - Save Best Score: 0.8458 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4569(0.5427) \n","Epoch: [3][0/2481] Elapsed 0m 0s (remain 25m 13s) Loss: 0.5185(0.5185) Grad: 80628.5781  LR: 0.00000982  \n","Epoch: [3][100/2481] Elapsed 0m 26s (remain 10m 19s) Loss: 0.4382(0.4981) Grad: 108799.1094  LR: 0.00000964  \n","Epoch: [3][200/2481] Elapsed 0m 51s (remain 9m 49s) Loss: 0.4785(0.5066) Grad: 35248.9180  LR: 0.00000945  \n","Epoch: [3][300/2481] Elapsed 1m 17s (remain 9m 19s) Loss: 0.4007(0.5049) Grad: 67734.4844  LR: 0.00000927  \n","Epoch: [3][400/2481] Elapsed 1m 42s (remain 8m 51s) Loss: 0.5514(0.5054) Grad: 14163.1084  LR: 0.00000908  \n","Epoch: [3][500/2481] Elapsed 2m 7s (remain 8m 24s) Loss: 0.5088(0.5064) Grad: 44550.8008  LR: 0.00000890  \n","Epoch: [3][600/2481] Elapsed 2m 32s (remain 7m 58s) Loss: 0.5209(0.5069) Grad: 40435.3008  LR: 0.00000871  \n","Epoch: [3][700/2481] Elapsed 2m 58s (remain 7m 32s) Loss: 0.6119(0.5058) Grad: 129462.7656  LR: 0.00000852  \n","Epoch: [3][800/2481] Elapsed 3m 23s (remain 7m 6s) Loss: 0.5738(0.5053) Grad: 116790.6328  LR: 0.00000834  \n","Epoch: [3][900/2481] Elapsed 3m 48s (remain 6m 40s) Loss: 0.4536(0.5043) Grad: 87048.6406  LR: 0.00000815  \n","Epoch: [3][1000/2481] Elapsed 4m 13s (remain 6m 15s) Loss: 0.6451(0.5046) Grad: 247069.2656  LR: 0.00000796  \n","Epoch: [3][1100/2481] Elapsed 4m 38s (remain 5m 49s) Loss: 0.4977(0.5049) Grad: 55227.7539  LR: 0.00000777  \n","Epoch: [3][1200/2481] Elapsed 5m 4s (remain 5m 24s) Loss: 0.4214(0.5060) Grad: 48497.2070  LR: 0.00000758  \n","Epoch: [3][1300/2481] Elapsed 5m 29s (remain 4m 58s) Loss: 0.4549(0.5068) Grad: 21386.2109  LR: 0.00000739  \n","Epoch: [3][1400/2481] Elapsed 5m 54s (remain 4m 33s) Loss: 0.5436(0.5077) Grad: 28544.0410  LR: 0.00000720  \n","Epoch: [3][1500/2481] Elapsed 6m 20s (remain 4m 8s) Loss: 0.4201(0.5079) Grad: 41422.8945  LR: 0.00000701  \n","Epoch: [3][1600/2481] Elapsed 6m 45s (remain 3m 42s) Loss: 0.4736(0.5079) Grad: 56158.9180  LR: 0.00000682  \n","Epoch: [3][1700/2481] Elapsed 7m 10s (remain 3m 17s) Loss: 0.5463(0.5075) Grad: 29967.3242  LR: 0.00000663  \n","Epoch: [3][1800/2481] Elapsed 7m 36s (remain 2m 52s) Loss: 0.5169(0.5074) Grad: 119472.6328  LR: 0.00000644  \n","Epoch: [3][1900/2481] Elapsed 8m 1s (remain 2m 26s) Loss: 0.5878(0.5074) Grad: 313740.7188  LR: 0.00000625  \n","Epoch: [3][2000/2481] Elapsed 8m 26s (remain 2m 1s) Loss: 0.5787(0.5079) Grad: 24573.8848  LR: 0.00000607  \n","Epoch: [3][2100/2481] Elapsed 8m 51s (remain 1m 36s) Loss: 0.6397(0.5084) Grad: 43912.9180  LR: 0.00000588  \n","Epoch: [3][2200/2481] Elapsed 9m 16s (remain 1m 10s) Loss: 0.5561(0.5086) Grad: 21805.7930  LR: 0.00000570  \n","Epoch: [3][2300/2481] Elapsed 9m 42s (remain 0m 45s) Loss: 0.4101(0.5086) Grad: 19388.5527  LR: 0.00000551  \n","Epoch: [3][2400/2481] Elapsed 10m 7s (remain 0m 20s) Loss: 0.5064(0.5086) Grad: 29103.7598  LR: 0.00000533  \n","Epoch: [3][2480/2481] Elapsed 10m 27s (remain 0m 0s) Loss: 0.5057(0.5089) Grad: 51148.7188  LR: 0.00000519  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 46s) Loss: 0.3079(0.3079) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5617(0.5620) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5089  avg_val_loss: 0.5562  time: 644s\n","Epoch 3 - Score: 0.8439\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4460(0.5562) \n","Epoch: [4][0/2481] Elapsed 0m 0s (remain 25m 5s) Loss: 0.4440(0.4440) Grad: 52168.8906  LR: 0.00000518  \n","Epoch: [4][100/2481] Elapsed 0m 25s (remain 10m 10s) Loss: 0.5191(0.5007) Grad: 37752.0820  LR: 0.00000500  \n","Epoch: [4][200/2481] Elapsed 0m 51s (remain 9m 40s) Loss: 0.4415(0.5043) Grad: 461300.0000  LR: 0.00000483  \n","Epoch: [4][300/2481] Elapsed 1m 16s (remain 9m 13s) Loss: 0.4222(0.5001) Grad: 26097.6719  LR: 0.00000465  \n","Epoch: [4][400/2481] Elapsed 1m 41s (remain 8m 47s) Loss: 0.5678(0.5014) Grad: 33662.4961  LR: 0.00000447  \n","Epoch: [4][500/2481] Elapsed 2m 6s (remain 8m 21s) Loss: 0.4618(0.5012) Grad: 33499.4609  LR: 0.00000430  \n","Epoch: [4][600/2481] Elapsed 2m 32s (remain 7m 56s) Loss: 0.5022(0.5017) Grad: 80581.4688  LR: 0.00000413  \n","Epoch: [4][700/2481] Elapsed 2m 57s (remain 7m 30s) Loss: 0.5430(0.5000) Grad: 65983.3203  LR: 0.00000396  \n","Epoch: [4][800/2481] Elapsed 3m 22s (remain 7m 5s) Loss: 0.5176(0.4997) Grad: 561501.0625  LR: 0.00000380  \n","Epoch: [4][900/2481] Elapsed 3m 47s (remain 6m 39s) Loss: 0.6146(0.4999) Grad: 46216.3281  LR: 0.00000363  \n","Epoch: [4][1000/2481] Elapsed 4m 13s (remain 6m 14s) Loss: 0.3894(0.4985) Grad: 133648.3750  LR: 0.00000347  \n","Epoch: [4][1100/2481] Elapsed 4m 38s (remain 5m 48s) Loss: 0.4085(0.4984) Grad: 50047.3047  LR: 0.00000331  \n","Epoch: [4][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.4934(0.4982) Grad: 43721.1641  LR: 0.00000316  \n","Epoch: [4][1300/2481] Elapsed 5m 29s (remain 4m 58s) Loss: 0.5086(0.4986) Grad: 61490.6094  LR: 0.00000300  \n","Epoch: [4][1400/2481] Elapsed 5m 54s (remain 4m 33s) Loss: 0.4924(0.4991) Grad: 31742.8086  LR: 0.00000285  \n","Epoch: [4][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.4578(0.4990) Grad: 69012.9922  LR: 0.00000270  \n","Epoch: [4][1600/2481] Elapsed 6m 45s (remain 3m 42s) Loss: 0.4746(0.4993) Grad: 27483.0664  LR: 0.00000256  \n","Epoch: [4][1700/2481] Elapsed 7m 10s (remain 3m 17s) Loss: 0.5432(0.4994) Grad: 37936.8242  LR: 0.00000242  \n","Epoch: [4][1800/2481] Elapsed 7m 35s (remain 2m 51s) Loss: 0.5133(0.4995) Grad: 99021.2188  LR: 0.00000228  \n","Epoch: [4][1900/2481] Elapsed 8m 0s (remain 2m 26s) Loss: 0.4928(0.4990) Grad: 19827.4590  LR: 0.00000215  \n","Epoch: [4][2000/2481] Elapsed 8m 25s (remain 2m 1s) Loss: 0.3348(0.4988) Grad: 45239.4219  LR: 0.00000201  \n","Epoch: [4][2100/2481] Elapsed 8m 51s (remain 1m 36s) Loss: 0.5554(0.4988) Grad: 84299.3672  LR: 0.00000189  \n","Epoch: [4][2200/2481] Elapsed 9m 16s (remain 1m 10s) Loss: 0.6074(0.4982) Grad: 84213.2188  LR: 0.00000176  \n","Epoch: [4][2300/2481] Elapsed 9m 41s (remain 0m 45s) Loss: 0.5420(0.4984) Grad: 111302.7656  LR: 0.00000164  \n","Epoch: [4][2400/2481] Elapsed 10m 6s (remain 0m 20s) Loss: 0.3436(0.4991) Grad: 122623.2344  LR: 0.00000153  \n","Epoch: [4][2480/2481] Elapsed 10m 27s (remain 0m 0s) Loss: 0.5451(0.4994) Grad: 87771.5547  LR: 0.00000144  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 46s) Loss: 0.3097(0.3097) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5649(0.5600) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4994  avg_val_loss: 0.5550  time: 644s\n","Epoch 4 - Score: 0.8437\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4476(0.5550) \n","Epoch: [5][0/2481] Elapsed 0m 0s (remain 24m 23s) Loss: 0.5113(0.5113) Grad: 36612.4805  LR: 0.00000143  \n","Epoch: [5][100/2481] Elapsed 0m 25s (remain 10m 9s) Loss: 0.4980(0.4976) Grad: 52887.9766  LR: 0.00000132  \n","Epoch: [5][200/2481] Elapsed 0m 51s (remain 9m 39s) Loss: 0.5586(0.4979) Grad: 69785.6797  LR: 0.00000122  \n","Epoch: [5][300/2481] Elapsed 1m 16s (remain 9m 12s) Loss: 0.5490(0.4991) Grad: 123024.1797  LR: 0.00000112  \n","Epoch: [5][400/2481] Elapsed 1m 41s (remain 8m 46s) Loss: 0.4261(0.4947) Grad: 29228.7773  LR: 0.00000102  \n","Epoch: [5][500/2481] Elapsed 2m 6s (remain 8m 20s) Loss: 0.5578(0.4943) Grad: 34834.3516  LR: 0.00000093  \n","Epoch: [5][600/2481] Elapsed 2m 31s (remain 7m 55s) Loss: 0.3957(0.4958) Grad: 26563.2246  LR: 0.00000084  \n","Epoch: [5][700/2481] Elapsed 2m 57s (remain 7m 29s) Loss: 0.5663(0.4943) Grad: 264492.2188  LR: 0.00000075  \n","Epoch: [5][800/2481] Elapsed 3m 22s (remain 7m 4s) Loss: 0.5946(0.4944) Grad: 68770.1797  LR: 0.00000067  \n","Epoch: [5][900/2481] Elapsed 3m 47s (remain 6m 38s) Loss: 0.4967(0.4938) Grad: 36215.3047  LR: 0.00000059  \n","Epoch: [5][1000/2481] Elapsed 4m 12s (remain 6m 13s) Loss: 0.4279(0.4934) Grad: 38949.8359  LR: 0.00000052  \n","Epoch: [5][1100/2481] Elapsed 4m 37s (remain 5m 48s) Loss: 0.4864(0.4944) Grad: 23693.4531  LR: 0.00000046  \n","Epoch: [5][1200/2481] Elapsed 5m 3s (remain 5m 22s) Loss: 0.4595(0.4950) Grad: 10836.3672  LR: 0.00000039  \n","Epoch: [5][1300/2481] Elapsed 5m 28s (remain 4m 57s) Loss: 0.6324(0.4953) Grad: 19526.5293  LR: 0.00000033  \n","Epoch: [5][1400/2481] Elapsed 5m 53s (remain 4m 32s) Loss: 0.4977(0.4947) Grad: 36248.5312  LR: 0.00000028  \n","Epoch: [5][1500/2481] Elapsed 6m 18s (remain 4m 7s) Loss: 0.4385(0.4947) Grad: 31887.8535  LR: 0.00000023  \n","Epoch: [5][1600/2481] Elapsed 6m 44s (remain 3m 42s) Loss: 0.4898(0.4946) Grad: 37148.8320  LR: 0.00000019  \n","Epoch: [5][1700/2481] Elapsed 7m 9s (remain 3m 16s) Loss: 0.5190(0.4938) Grad: 275278.9375  LR: 0.00000015  \n","Epoch: [5][1800/2481] Elapsed 7m 34s (remain 2m 51s) Loss: 0.5694(0.4936) Grad: 124098.9219  LR: 0.00000011  \n","Epoch: [5][1900/2481] Elapsed 7m 59s (remain 2m 26s) Loss: 0.4549(0.4939) Grad: 84323.4844  LR: 0.00000008  \n","Epoch: [5][2000/2481] Elapsed 8m 24s (remain 2m 1s) Loss: 0.5637(0.4939) Grad: 75065.4922  LR: 0.00000006  \n","Epoch: [5][2100/2481] Elapsed 8m 50s (remain 1m 35s) Loss: 0.4562(0.4945) Grad: 38176.0156  LR: 0.00000004  \n","Epoch: [5][2200/2481] Elapsed 9m 15s (remain 1m 10s) Loss: 0.5473(0.4944) Grad: 53218.0742  LR: 0.00000002  \n","Epoch: [5][2300/2481] Elapsed 9m 40s (remain 0m 45s) Loss: 0.4466(0.4948) Grad: 27813.4473  LR: 0.00000001  \n","Epoch: [5][2400/2481] Elapsed 10m 5s (remain 0m 20s) Loss: 0.5706(0.4952) Grad: 93009.1172  LR: 0.00000000  \n","Epoch: [5][2480/2481] Elapsed 10m 25s (remain 0m 0s) Loss: 0.5093(0.4952) Grad: 58472.3242  LR: 0.00000000  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 45s) Loss: 0.3090(0.3090) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5658(0.5645) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4952  avg_val_loss: 0.5594  time: 642s\n","Epoch 5 - Score: 0.8419\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4482(0.5594) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 11 result ==========\n","Score: 0.8458\n","========== fold: 12 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2481] Elapsed 0m 0s (remain 24m 32s) Loss: 0.6664(0.6664) Grad: 38373.7070  LR: 0.00001500  \n","Epoch: [1][100/2481] Elapsed 0m 25s (remain 10m 6s) Loss: 0.6345(0.6464) Grad: 32633.6270  LR: 0.00001500  \n","Epoch: [1][200/2481] Elapsed 0m 50s (remain 9m 36s) Loss: 0.5861(0.6272) Grad: 22814.8516  LR: 0.00001499  \n","Epoch: [1][300/2481] Elapsed 1m 16s (remain 9m 10s) Loss: 0.5417(0.6111) Grad: 16242.9238  LR: 0.00001498  \n","Epoch: [1][400/2481] Elapsed 1m 41s (remain 8m 45s) Loss: 0.5325(0.6016) Grad: 15247.3262  LR: 0.00001496  \n","Epoch: [1][500/2481] Elapsed 2m 6s (remain 8m 20s) Loss: 0.4173(0.5951) Grad: 40869.2148  LR: 0.00001494  \n","Epoch: [1][600/2481] Elapsed 2m 31s (remain 7m 54s) Loss: 0.5328(0.5897) Grad: 23462.8496  LR: 0.00001491  \n","Epoch: [1][700/2481] Elapsed 2m 57s (remain 7m 29s) Loss: 0.6704(0.5837) Grad: 28020.3828  LR: 0.00001488  \n","Epoch: [1][800/2481] Elapsed 3m 22s (remain 7m 4s) Loss: 0.5775(0.5803) Grad: 50919.2734  LR: 0.00001485  \n","Epoch: [1][900/2481] Elapsed 3m 47s (remain 6m 38s) Loss: 0.4814(0.5768) Grad: 9572.7393  LR: 0.00001481  \n","Epoch: [1][1000/2481] Elapsed 4m 12s (remain 6m 13s) Loss: 0.5784(0.5737) Grad: 10884.7588  LR: 0.00001476  \n","Epoch: [1][1100/2481] Elapsed 4m 37s (remain 5m 48s) Loss: 0.7069(0.5726) Grad: 49473.2266  LR: 0.00001471  \n","Epoch: [1][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.5316(0.5706) Grad: 9528.9639  LR: 0.00001466  \n","Epoch: [1][1300/2481] Elapsed 5m 28s (remain 4m 58s) Loss: 0.5738(0.5683) Grad: 5683.2886  LR: 0.00001460  \n","Epoch: [1][1400/2481] Elapsed 5m 53s (remain 4m 32s) Loss: 0.6000(0.5660) Grad: 12040.0996  LR: 0.00001453  \n","Epoch: [1][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.5503(0.5644) Grad: 10632.1855  LR: 0.00001446  \n","Epoch: [1][1600/2481] Elapsed 6m 44s (remain 3m 42s) Loss: 0.5930(0.5635) Grad: 9365.5498  LR: 0.00001439  \n","Epoch: [1][1700/2481] Elapsed 7m 9s (remain 3m 17s) Loss: 0.3594(0.5625) Grad: 9747.8867  LR: 0.00001432  \n","Epoch: [1][1800/2481] Elapsed 7m 35s (remain 2m 51s) Loss: 0.4263(0.5616) Grad: 3947.1592  LR: 0.00001423  \n","Epoch: [1][1900/2481] Elapsed 8m 0s (remain 2m 26s) Loss: 0.5313(0.5606) Grad: 20829.1074  LR: 0.00001415  \n","Epoch: [1][2000/2481] Elapsed 8m 25s (remain 2m 1s) Loss: 0.5229(0.5595) Grad: 10541.9482  LR: 0.00001406  \n","Epoch: [1][2100/2481] Elapsed 8m 50s (remain 1m 36s) Loss: 0.5427(0.5588) Grad: 9654.7129  LR: 0.00001396  \n","Epoch: [1][2200/2481] Elapsed 9m 16s (remain 1m 10s) Loss: 0.5529(0.5577) Grad: 35700.1523  LR: 0.00001387  \n","Epoch: [1][2300/2481] Elapsed 9m 41s (remain 0m 45s) Loss: 0.5273(0.5570) Grad: 45024.7500  LR: 0.00001376  \n","Epoch: [1][2400/2481] Elapsed 10m 6s (remain 0m 20s) Loss: 0.6124(0.5557) Grad: 41895.9492  LR: 0.00001366  \n","Epoch: [1][2480/2481] Elapsed 10m 26s (remain 0m 0s) Loss: 0.4048(0.5553) Grad: 34603.6172  LR: 0.00001357  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 53s) Loss: 0.6142(0.6142) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6955(0.5480) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5553  avg_val_loss: 0.5458  time: 643s\n","Epoch 1 - Score: 0.8310\n","Epoch 1 - Save Best Score: 0.8310 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.2440(0.5458) \n","Epoch: [2][0/2481] Elapsed 0m 0s (remain 26m 48s) Loss: 0.4795(0.4795) Grad: 78797.1562  LR: 0.00001357  \n","Epoch: [2][100/2481] Elapsed 0m 26s (remain 10m 19s) Loss: 0.6145(0.5133) Grad: 91496.6719  LR: 0.00001345  \n","Epoch: [2][200/2481] Elapsed 0m 52s (remain 9m 50s) Loss: 0.3558(0.5122) Grad: 63144.7812  LR: 0.00001334  \n","Epoch: [2][300/2481] Elapsed 1m 17s (remain 9m 19s) Loss: 0.6329(0.5099) Grad: 77598.5156  LR: 0.00001321  \n","Epoch: [2][400/2481] Elapsed 1m 42s (remain 8m 51s) Loss: 0.5498(0.5119) Grad: 31766.8828  LR: 0.00001309  \n","Epoch: [2][500/2481] Elapsed 2m 7s (remain 8m 24s) Loss: 0.4750(0.5155) Grad: 57106.2734  LR: 0.00001296  \n","Epoch: [2][600/2481] Elapsed 2m 32s (remain 7m 58s) Loss: 0.6035(0.5156) Grad: 43699.3008  LR: 0.00001283  \n","Epoch: [2][700/2481] Elapsed 2m 58s (remain 7m 32s) Loss: 0.5445(0.5155) Grad: 336773.2188  LR: 0.00001269  \n","Epoch: [2][800/2481] Elapsed 3m 23s (remain 7m 6s) Loss: 0.3952(0.5144) Grad: 40218.0312  LR: 0.00001256  \n","Epoch: [2][900/2481] Elapsed 3m 48s (remain 6m 41s) Loss: 0.5021(0.5144) Grad: 20614.8809  LR: 0.00001241  \n","Epoch: [2][1000/2481] Elapsed 4m 13s (remain 6m 15s) Loss: 0.5250(0.5138) Grad: 75046.2734  LR: 0.00001227  \n","Epoch: [2][1100/2481] Elapsed 4m 39s (remain 5m 49s) Loss: 0.5573(0.5146) Grad: 104696.0781  LR: 0.00001212  \n","Epoch: [2][1200/2481] Elapsed 5m 4s (remain 5m 24s) Loss: 0.6320(0.5143) Grad: 222866.7969  LR: 0.00001197  \n","Epoch: [2][1300/2481] Elapsed 5m 29s (remain 4m 59s) Loss: 0.5002(0.5153) Grad: 108803.3281  LR: 0.00001182  \n","Epoch: [2][1400/2481] Elapsed 5m 55s (remain 4m 33s) Loss: 0.5351(0.5158) Grad: 45123.5547  LR: 0.00001166  \n","Epoch: [2][1500/2481] Elapsed 6m 20s (remain 4m 8s) Loss: 0.4821(0.5159) Grad: 243272.3125  LR: 0.00001150  \n","Epoch: [2][1600/2481] Elapsed 6m 45s (remain 3m 42s) Loss: 0.4997(0.5158) Grad: 132286.9375  LR: 0.00001134  \n","Epoch: [2][1700/2481] Elapsed 7m 10s (remain 3m 17s) Loss: 0.4179(0.5154) Grad: 161968.0156  LR: 0.00001117  \n","Epoch: [2][1800/2481] Elapsed 7m 36s (remain 2m 52s) Loss: 0.4481(0.5149) Grad: 28215.1660  LR: 0.00001101  \n","Epoch: [2][1900/2481] Elapsed 8m 1s (remain 2m 26s) Loss: 0.5984(0.5152) Grad: 75387.4297  LR: 0.00001084  \n","Epoch: [2][2000/2481] Elapsed 8m 26s (remain 2m 1s) Loss: 0.4444(0.5159) Grad: 111349.5000  LR: 0.00001067  \n","Epoch: [2][2100/2481] Elapsed 8m 52s (remain 1m 36s) Loss: 0.5069(0.5161) Grad: 96940.0703  LR: 0.00001049  \n","Epoch: [2][2200/2481] Elapsed 9m 17s (remain 1m 10s) Loss: 0.5417(0.5160) Grad: 105051.9688  LR: 0.00001032  \n","Epoch: [2][2300/2481] Elapsed 9m 42s (remain 0m 45s) Loss: 0.5849(0.5159) Grad: 360623.4688  LR: 0.00001014  \n","Epoch: [2][2400/2481] Elapsed 10m 7s (remain 0m 20s) Loss: 0.5537(0.5157) Grad: 198422.2969  LR: 0.00000996  \n","Epoch: [2][2480/2481] Elapsed 10m 28s (remain 0m 0s) Loss: 0.5022(0.5156) Grad: 71819.3047  LR: 0.00000982  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 49s) Loss: 0.7051(0.7051) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6923(0.5568) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5156  avg_val_loss: 0.5543  time: 645s\n","Epoch 2 - Score: 0.8355\n","Epoch 2 - Save Best Score: 0.8355 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.2345(0.5543) \n","Epoch: [3][0/2481] Elapsed 0m 1s (remain 77m 56s) Loss: 0.5201(0.5201) Grad: 51152.8711  LR: 0.00000982  \n","Epoch: [3][100/2481] Elapsed 0m 27s (remain 10m 48s) Loss: 0.5633(0.5158) Grad: 67414.5703  LR: 0.00000964  \n","Epoch: [3][200/2481] Elapsed 0m 53s (remain 10m 3s) Loss: 0.5411(0.5169) Grad: 68333.4297  LR: 0.00000945  \n","Epoch: [3][300/2481] Elapsed 1m 18s (remain 9m 27s) Loss: 0.5620(0.5144) Grad: 70210.0312  LR: 0.00000927  \n","Epoch: [3][400/2481] Elapsed 1m 43s (remain 8m 57s) Loss: 0.3935(0.5135) Grad: 46208.6094  LR: 0.00000908  \n","Epoch: [3][500/2481] Elapsed 2m 8s (remain 8m 29s) Loss: 0.4080(0.5136) Grad: 155821.8750  LR: 0.00000890  \n","Epoch: [3][600/2481] Elapsed 2m 34s (remain 8m 2s) Loss: 0.4770(0.5122) Grad: 82563.7031  LR: 0.00000871  \n","Epoch: [3][700/2481] Elapsed 2m 59s (remain 7m 35s) Loss: 0.3617(0.5115) Grad: 74770.9219  LR: 0.00000852  \n","Epoch: [3][800/2481] Elapsed 3m 24s (remain 7m 9s) Loss: 0.4708(0.5104) Grad: 84120.8672  LR: 0.00000833  \n","Epoch: [3][900/2481] Elapsed 3m 49s (remain 6m 42s) Loss: 0.4038(0.5097) Grad: 127451.9922  LR: 0.00000815  \n","Epoch: [3][1000/2481] Elapsed 4m 15s (remain 6m 17s) Loss: 0.4256(0.5109) Grad: 31730.7617  LR: 0.00000796  \n","Epoch: [3][1100/2481] Elapsed 4m 40s (remain 5m 51s) Loss: 0.4605(0.5106) Grad: 36792.0078  LR: 0.00000777  \n","Epoch: [3][1200/2481] Elapsed 5m 5s (remain 5m 25s) Loss: 0.4805(0.5106) Grad: 82816.0547  LR: 0.00000758  \n","Epoch: [3][1300/2481] Elapsed 5m 30s (remain 5m 0s) Loss: 0.4549(0.5101) Grad: 34486.8867  LR: 0.00000739  \n","Epoch: [3][1400/2481] Elapsed 5m 56s (remain 4m 34s) Loss: 0.5342(0.5099) Grad: 57940.6797  LR: 0.00000720  \n","Epoch: [3][1500/2481] Elapsed 6m 21s (remain 4m 9s) Loss: 0.6935(0.5102) Grad: 51796.0820  LR: 0.00000701  \n","Epoch: [3][1600/2481] Elapsed 6m 47s (remain 3m 43s) Loss: 0.3252(0.5095) Grad: 20265.8242  LR: 0.00000682  \n","Epoch: [3][1700/2481] Elapsed 7m 12s (remain 3m 18s) Loss: 0.3381(0.5092) Grad: 33523.8438  LR: 0.00000663  \n","Epoch: [3][1800/2481] Elapsed 7m 37s (remain 2m 52s) Loss: 0.2934(0.5090) Grad: 50625.1953  LR: 0.00000644  \n","Epoch: [3][1900/2481] Elapsed 8m 3s (remain 2m 27s) Loss: 0.5547(0.5093) Grad: 13555.3486  LR: 0.00000625  \n","Epoch: [3][2000/2481] Elapsed 8m 28s (remain 2m 1s) Loss: 0.5703(0.5095) Grad: 25457.5859  LR: 0.00000607  \n","Epoch: [3][2100/2481] Elapsed 8m 53s (remain 1m 36s) Loss: 0.6155(0.5087) Grad: 37376.1836  LR: 0.00000588  \n","Epoch: [3][2200/2481] Elapsed 9m 19s (remain 1m 11s) Loss: 0.4911(0.5086) Grad: 17164.1230  LR: 0.00000570  \n","Epoch: [3][2300/2481] Elapsed 9m 44s (remain 0m 45s) Loss: 0.5466(0.5083) Grad: 50170.5781  LR: 0.00000551  \n","Epoch: [3][2400/2481] Elapsed 10m 9s (remain 0m 20s) Loss: 0.5027(0.5084) Grad: 169666.1562  LR: 0.00000533  \n","Epoch: [3][2480/2481] Elapsed 10m 29s (remain 0m 0s) Loss: 0.6186(0.5088) Grad: 20413.0039  LR: 0.00000518  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 47s) Loss: 0.6170(0.6170) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7007(0.5737) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5088  avg_val_loss: 0.5700  time: 646s\n","Epoch 3 - Score: 0.8296\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.2275(0.5700) \n","Epoch: [4][0/2481] Elapsed 0m 0s (remain 25m 36s) Loss: 0.3387(0.3387) Grad: 47753.9219  LR: 0.00000518  \n","Epoch: [4][100/2481] Elapsed 0m 25s (remain 10m 9s) Loss: 0.5651(0.5174) Grad: 109087.1562  LR: 0.00000500  \n","Epoch: [4][200/2481] Elapsed 0m 51s (remain 9m 40s) Loss: 0.4420(0.5117) Grad: 55714.7344  LR: 0.00000482  \n","Epoch: [4][300/2481] Elapsed 1m 16s (remain 9m 14s) Loss: 0.4698(0.5055) Grad: 61607.9961  LR: 0.00000465  \n","Epoch: [4][400/2481] Elapsed 1m 41s (remain 8m 48s) Loss: 0.4123(0.5048) Grad: 32172.5645  LR: 0.00000447  \n","Epoch: [4][500/2481] Elapsed 2m 7s (remain 8m 22s) Loss: 0.5532(0.5028) Grad: 18795.1914  LR: 0.00000430  \n","Epoch: [4][600/2481] Elapsed 2m 32s (remain 7m 56s) Loss: 0.5152(0.5045) Grad: 271617.8750  LR: 0.00000413  \n","Epoch: [4][700/2481] Elapsed 2m 57s (remain 7m 31s) Loss: 0.5195(0.5044) Grad: 41583.6875  LR: 0.00000396  \n","Epoch: [4][800/2481] Elapsed 3m 22s (remain 7m 5s) Loss: 0.4684(0.5030) Grad: 47979.4219  LR: 0.00000380  \n","Epoch: [4][900/2481] Elapsed 3m 48s (remain 6m 40s) Loss: 0.5310(0.5030) Grad: 61368.5195  LR: 0.00000363  \n","Epoch: [4][1000/2481] Elapsed 4m 13s (remain 6m 14s) Loss: 0.4754(0.5046) Grad: 60872.5195  LR: 0.00000347  \n","Epoch: [4][1100/2481] Elapsed 4m 38s (remain 5m 49s) Loss: 0.5386(0.5035) Grad: 75731.7109  LR: 0.00000331  \n","Epoch: [4][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.4393(0.5021) Grad: 81985.4141  LR: 0.00000315  \n","Epoch: [4][1300/2481] Elapsed 5m 29s (remain 4m 58s) Loss: 0.6544(0.5025) Grad: 64475.4961  LR: 0.00000300  \n","Epoch: [4][1400/2481] Elapsed 5m 54s (remain 4m 33s) Loss: 0.5404(0.5016) Grad: 53991.3438  LR: 0.00000285  \n","Epoch: [4][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.6443(0.5019) Grad: 66773.8203  LR: 0.00000270  \n","Epoch: [4][1600/2481] Elapsed 6m 45s (remain 3m 42s) Loss: 0.5644(0.5013) Grad: 63412.0195  LR: 0.00000256  \n","Epoch: [4][1700/2481] Elapsed 7m 10s (remain 3m 17s) Loss: 0.4396(0.5009) Grad: 55415.1367  LR: 0.00000242  \n","Epoch: [4][1800/2481] Elapsed 7m 35s (remain 2m 52s) Loss: 0.6523(0.5013) Grad: 26672.4336  LR: 0.00000228  \n","Epoch: [4][1900/2481] Elapsed 8m 0s (remain 2m 26s) Loss: 0.4395(0.5008) Grad: 53312.8164  LR: 0.00000214  \n","Epoch: [4][2000/2481] Elapsed 8m 26s (remain 2m 1s) Loss: 0.4339(0.5008) Grad: 158228.5469  LR: 0.00000201  \n","Epoch: [4][2100/2481] Elapsed 8m 51s (remain 1m 36s) Loss: 0.4915(0.5005) Grad: 65536.0625  LR: 0.00000189  \n","Epoch: [4][2200/2481] Elapsed 9m 16s (remain 1m 10s) Loss: 0.4092(0.5003) Grad: 133193.4375  LR: 0.00000176  \n","Epoch: [4][2300/2481] Elapsed 9m 41s (remain 0m 45s) Loss: 0.3925(0.4996) Grad: 78043.5156  LR: 0.00000164  \n","Epoch: [4][2400/2481] Elapsed 10m 7s (remain 0m 20s) Loss: 0.3673(0.4992) Grad: 38495.5078  LR: 0.00000152  \n","Epoch: [4][2480/2481] Elapsed 10m 27s (remain 0m 0s) Loss: 0.6061(0.4991) Grad: 101012.0859  LR: 0.00000143  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 49s) Loss: 0.5719(0.5719) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7144(0.5663) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4991  avg_val_loss: 0.5642  time: 644s\n","Epoch 4 - Score: 0.8381\n","Epoch 4 - Save Best Score: 0.8381 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.2400(0.5642) \n","Epoch: [5][0/2481] Elapsed 0m 0s (remain 26m 51s) Loss: 0.4494(0.4494) Grad: 73463.2969  LR: 0.00000143  \n","Epoch: [5][100/2481] Elapsed 0m 26s (remain 10m 21s) Loss: 0.4076(0.5036) Grad: 24779.1250  LR: 0.00000132  \n","Epoch: [5][200/2481] Elapsed 0m 51s (remain 9m 48s) Loss: 0.4795(0.4974) Grad: 83497.0391  LR: 0.00000122  \n","Epoch: [5][300/2481] Elapsed 1m 17s (remain 9m 17s) Loss: 0.4232(0.4953) Grad: 73727.3438  LR: 0.00000112  \n","Epoch: [5][400/2481] Elapsed 1m 42s (remain 8m 50s) Loss: 0.5928(0.4977) Grad: 62227.6445  LR: 0.00000102  \n","Epoch: [5][500/2481] Elapsed 2m 7s (remain 8m 23s) Loss: 0.4737(0.4940) Grad: 33692.8555  LR: 0.00000092  \n","Epoch: [5][600/2481] Elapsed 2m 32s (remain 7m 56s) Loss: 0.3185(0.4932) Grad: 22617.9844  LR: 0.00000084  \n","Epoch: [5][700/2481] Elapsed 2m 57s (remain 7m 31s) Loss: 0.5453(0.4936) Grad: 31740.8184  LR: 0.00000075  \n","Epoch: [5][800/2481] Elapsed 3m 22s (remain 7m 5s) Loss: 0.4630(0.4944) Grad: 58123.8906  LR: 0.00000067  \n","Epoch: [5][900/2481] Elapsed 3m 47s (remain 6m 39s) Loss: 0.5413(0.4949) Grad: 81233.8750  LR: 0.00000059  \n","Epoch: [5][1000/2481] Elapsed 4m 13s (remain 6m 14s) Loss: 0.5027(0.4963) Grad: 40847.0547  LR: 0.00000052  \n","Epoch: [5][1100/2481] Elapsed 4m 38s (remain 5m 48s) Loss: 0.3989(0.4959) Grad: 61113.6836  LR: 0.00000045  \n","Epoch: [5][1200/2481] Elapsed 5m 3s (remain 5m 23s) Loss: 0.5385(0.4961) Grad: 229898.2969  LR: 0.00000039  \n","Epoch: [5][1300/2481] Elapsed 5m 28s (remain 4m 58s) Loss: 0.5534(0.4951) Grad: 219957.5625  LR: 0.00000033  \n","Epoch: [5][1400/2481] Elapsed 5m 53s (remain 4m 32s) Loss: 0.5270(0.4945) Grad: 40175.8750  LR: 0.00000028  \n","Epoch: [5][1500/2481] Elapsed 6m 19s (remain 4m 7s) Loss: 0.5583(0.4950) Grad: 9571.2070  LR: 0.00000023  \n","Epoch: [5][1600/2481] Elapsed 6m 44s (remain 3m 42s) Loss: 0.5117(0.4958) Grad: 66463.0781  LR: 0.00000019  \n","Epoch: [5][1700/2481] Elapsed 7m 9s (remain 3m 16s) Loss: 0.5140(0.4957) Grad: 50622.7070  LR: 0.00000015  \n","Epoch: [5][1800/2481] Elapsed 7m 34s (remain 2m 51s) Loss: 0.5193(0.4958) Grad: 34230.7734  LR: 0.00000011  \n","Epoch: [5][1900/2481] Elapsed 7m 59s (remain 2m 26s) Loss: 0.4721(0.4955) Grad: 28655.1660  LR: 0.00000008  \n","Epoch: [5][2000/2481] Elapsed 8m 25s (remain 2m 1s) Loss: 0.5939(0.4955) Grad: 136172.2656  LR: 0.00000006  \n","Epoch: [5][2100/2481] Elapsed 8m 50s (remain 1m 35s) Loss: 0.4990(0.4958) Grad: 82138.8438  LR: 0.00000004  \n","Epoch: [5][2200/2481] Elapsed 9m 15s (remain 1m 10s) Loss: 0.6181(0.4955) Grad: 117040.5625  LR: 0.00000002  \n","Epoch: [5][2300/2481] Elapsed 9m 40s (remain 0m 45s) Loss: 0.4626(0.4956) Grad: 87252.7344  LR: 0.00000001  \n","Epoch: [5][2400/2481] Elapsed 10m 5s (remain 0m 20s) Loss: 0.5902(0.4955) Grad: 98437.0625  LR: 0.00000000  \n","Epoch: [5][2480/2481] Elapsed 10m 25s (remain 0m 0s) Loss: 0.4805(0.4951) Grad: 112768.0469  LR: 0.00000000  \n","EVAL: [0/124] Elapsed 0m 0s (remain 0m 48s) Loss: 0.5726(0.5726) \n","EVAL: [100/124] Elapsed 0m 13s (remain 0m 3s) Loss: 0.7171(0.5720) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4951  avg_val_loss: 0.5694  time: 642s\n","Epoch 5 - Score: 0.8348\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [123/124] Elapsed 0m 16s (remain 0m 0s) Loss: 0.2386(0.5694) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 12 result ==========\n","Score: 0.8381\n","========== fold: 13 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2502] Elapsed 0m 0s (remain 24m 23s) Loss: 0.6850(0.6850) Grad: 123307.3438  LR: 0.00001500  \n","Epoch: [1][100/2502] Elapsed 0m 25s (remain 10m 11s) Loss: 0.5571(0.6288) Grad: 25778.6738  LR: 0.00001500  \n","Epoch: [1][200/2502] Elapsed 0m 50s (remain 9m 42s) Loss: 0.6118(0.6111) Grad: 51669.4414  LR: 0.00001499  \n","Epoch: [1][300/2502] Elapsed 1m 16s (remain 9m 16s) Loss: 0.5833(0.5973) Grad: 30236.9434  LR: 0.00001498  \n","Epoch: [1][400/2502] Elapsed 1m 41s (remain 8m 50s) Loss: 0.4958(0.5866) Grad: 24957.4844  LR: 0.00001496  \n","Epoch: [1][500/2502] Elapsed 2m 6s (remain 8m 25s) Loss: 0.5209(0.5797) Grad: 23865.8164  LR: 0.00001494  \n","Epoch: [1][600/2502] Elapsed 2m 31s (remain 7m 59s) Loss: 0.5355(0.5740) Grad: 44950.4844  LR: 0.00001491  \n","Epoch: [1][700/2502] Elapsed 2m 56s (remain 7m 34s) Loss: 0.4819(0.5720) Grad: 38485.7695  LR: 0.00001488  \n","Epoch: [1][800/2502] Elapsed 3m 21s (remain 7m 8s) Loss: 0.5751(0.5686) Grad: 38766.5664  LR: 0.00001485  \n","Epoch: [1][900/2502] Elapsed 3m 47s (remain 6m 43s) Loss: 0.5561(0.5681) Grad: 42012.1797  LR: 0.00001481  \n","Epoch: [1][1000/2502] Elapsed 4m 12s (remain 6m 18s) Loss: 0.6131(0.5650) Grad: 50640.8867  LR: 0.00001476  \n","Epoch: [1][1100/2502] Elapsed 4m 37s (remain 5m 53s) Loss: 0.5375(0.5635) Grad: 27063.4297  LR: 0.00001472  \n","Epoch: [1][1200/2502] Elapsed 5m 2s (remain 5m 28s) Loss: 0.4662(0.5617) Grad: 35334.6484  LR: 0.00001466  \n","Epoch: [1][1300/2502] Elapsed 5m 28s (remain 5m 2s) Loss: 0.4373(0.5607) Grad: 34799.7266  LR: 0.00001460  \n","Epoch: [1][1400/2502] Elapsed 5m 53s (remain 4m 37s) Loss: 0.5122(0.5602) Grad: 30096.6602  LR: 0.00001454  \n","Epoch: [1][1500/2502] Elapsed 6m 18s (remain 4m 12s) Loss: 0.4842(0.5596) Grad: 33225.8945  LR: 0.00001447  \n","Epoch: [1][1600/2502] Elapsed 6m 43s (remain 3m 47s) Loss: 0.4918(0.5588) Grad: 14472.4590  LR: 0.00001440  \n","Epoch: [1][1700/2502] Elapsed 7m 8s (remain 3m 21s) Loss: 0.5578(0.5576) Grad: 10582.6777  LR: 0.00001433  \n","Epoch: [1][1800/2502] Elapsed 7m 34s (remain 2m 56s) Loss: 0.4139(0.5568) Grad: 21432.5527  LR: 0.00001425  \n","Epoch: [1][1900/2502] Elapsed 7m 59s (remain 2m 31s) Loss: 0.5023(0.5562) Grad: 19927.8887  LR: 0.00001416  \n","Epoch: [1][2000/2502] Elapsed 8m 24s (remain 2m 6s) Loss: 0.5822(0.5556) Grad: 36983.3086  LR: 0.00001407  \n","Epoch: [1][2100/2502] Elapsed 8m 49s (remain 1m 41s) Loss: 0.5222(0.5543) Grad: 72593.4609  LR: 0.00001398  \n","Epoch: [1][2200/2502] Elapsed 9m 14s (remain 1m 15s) Loss: 0.5544(0.5542) Grad: 33989.6055  LR: 0.00001388  \n","Epoch: [1][2300/2502] Elapsed 9m 40s (remain 0m 50s) Loss: 0.6409(0.5536) Grad: 78924.0312  LR: 0.00001378  \n","Epoch: [1][2400/2502] Elapsed 10m 5s (remain 0m 25s) Loss: 0.4867(0.5535) Grad: 28654.9082  LR: 0.00001368  \n","Epoch: [1][2500/2502] Elapsed 10m 30s (remain 0m 0s) Loss: 0.5729(0.5531) Grad: 29312.5664  LR: 0.00001357  \n","Epoch: [1][2501/2502] Elapsed 10m 30s (remain 0m 0s) Loss: 0.6297(0.5531) Grad: 77850.6797  LR: 0.00001357  \n","EVAL: [0/103] Elapsed 0m 0s (remain 0m 42s) Loss: 0.6098(0.6098) \n","EVAL: [100/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.5140(0.5336) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5531  avg_val_loss: 0.5337  time: 645s\n","Epoch 1 - Score: 0.8278\n","Epoch 1 - Save Best Score: 0.8278 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [102/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6337(0.5337) \n","Epoch: [2][0/2502] Elapsed 0m 0s (remain 27m 15s) Loss: 0.4947(0.4947) Grad: 79479.4531  LR: 0.00001357  \n","Epoch: [2][100/2502] Elapsed 0m 26s (remain 10m 23s) Loss: 0.4942(0.5214) Grad: 40277.6289  LR: 0.00001345  \n","Epoch: [2][200/2502] Elapsed 0m 51s (remain 9m 54s) Loss: 0.5997(0.5216) Grad: 116886.7344  LR: 0.00001334  \n","Epoch: [2][300/2502] Elapsed 1m 17s (remain 9m 24s) Loss: 0.4568(0.5228) Grad: 61005.5000  LR: 0.00001322  \n","Epoch: [2][400/2502] Elapsed 1m 42s (remain 8m 57s) Loss: 0.5180(0.5195) Grad: 100208.2578  LR: 0.00001309  \n","Epoch: [2][500/2502] Elapsed 2m 7s (remain 8m 30s) Loss: 0.4517(0.5169) Grad: 52092.2891  LR: 0.00001297  \n","Epoch: [2][600/2502] Elapsed 2m 32s (remain 8m 3s) Loss: 0.6561(0.5187) Grad: 151652.8438  LR: 0.00001284  \n","Epoch: [2][700/2502] Elapsed 2m 58s (remain 7m 37s) Loss: 0.5472(0.5161) Grad: 116100.8672  LR: 0.00001270  \n","Epoch: [2][800/2502] Elapsed 3m 23s (remain 7m 11s) Loss: 0.6177(0.5172) Grad: 203989.2188  LR: 0.00001256  \n","Epoch: [2][900/2502] Elapsed 3m 48s (remain 6m 46s) Loss: 0.4952(0.5161) Grad: 158774.5781  LR: 0.00001242  \n","Epoch: [2][1000/2502] Elapsed 4m 13s (remain 6m 20s) Loss: 0.5516(0.5169) Grad: 94461.6641  LR: 0.00001228  \n","Epoch: [2][1100/2502] Elapsed 4m 39s (remain 5m 55s) Loss: 0.5713(0.5167) Grad: 53915.9648  LR: 0.00001213  \n","Epoch: [2][1200/2502] Elapsed 5m 4s (remain 5m 29s) Loss: 0.5109(0.5164) Grad: 42274.7305  LR: 0.00001198  \n","Epoch: [2][1300/2502] Elapsed 5m 29s (remain 5m 4s) Loss: 0.5343(0.5166) Grad: 125287.9141  LR: 0.00001183  \n","Epoch: [2][1400/2502] Elapsed 5m 55s (remain 4m 39s) Loss: 0.5812(0.5163) Grad: 36180.2969  LR: 0.00001168  \n","Epoch: [2][1500/2502] Elapsed 6m 20s (remain 4m 13s) Loss: 0.3725(0.5153) Grad: 67887.7266  LR: 0.00001152  \n","Epoch: [2][1600/2502] Elapsed 6m 45s (remain 3m 48s) Loss: 0.6186(0.5160) Grad: 57743.7734  LR: 0.00001136  \n","Epoch: [2][1700/2502] Elapsed 7m 10s (remain 3m 22s) Loss: 0.4293(0.5166) Grad: 117172.6719  LR: 0.00001120  \n","Epoch: [2][1800/2502] Elapsed 7m 36s (remain 2m 57s) Loss: 0.5477(0.5162) Grad: 40453.3828  LR: 0.00001103  \n","Epoch: [2][1900/2502] Elapsed 8m 1s (remain 2m 32s) Loss: 0.5331(0.5164) Grad: 35608.9180  LR: 0.00001086  \n","Epoch: [2][2000/2502] Elapsed 8m 26s (remain 2m 6s) Loss: 0.4624(0.5167) Grad: 167611.2969  LR: 0.00001069  \n","Epoch: [2][2100/2502] Elapsed 8m 51s (remain 1m 41s) Loss: 0.5640(0.5167) Grad: 163907.9375  LR: 0.00001052  \n","Epoch: [2][2200/2502] Elapsed 9m 17s (remain 1m 16s) Loss: 0.4569(0.5166) Grad: 142029.6250  LR: 0.00001035  \n","Epoch: [2][2300/2502] Elapsed 9m 42s (remain 0m 50s) Loss: 0.4802(0.5168) Grad: 184997.1719  LR: 0.00001018  \n","Epoch: [2][2400/2502] Elapsed 10m 7s (remain 0m 25s) Loss: 0.3096(0.5168) Grad: 59416.1914  LR: 0.00001000  \n","Epoch: [2][2500/2502] Elapsed 10m 32s (remain 0m 0s) Loss: 0.4701(0.5167) Grad: 81478.4688  LR: 0.00000982  \n","Epoch: [2][2501/2502] Elapsed 10m 32s (remain 0m 0s) Loss: 0.6348(0.5167) Grad: 1228715.6250  LR: 0.00000982  \n","EVAL: [0/103] Elapsed 0m 0s (remain 0m 42s) Loss: 0.5014(0.5014) \n","EVAL: [100/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.4902(0.5245) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5167  avg_val_loss: 0.5247  time: 647s\n","Epoch 2 - Score: 0.8352\n","Epoch 2 - Save Best Score: 0.8352 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [102/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6024(0.5247) \n","Epoch: [3][0/2502] Elapsed 0m 0s (remain 28m 34s) Loss: 0.3926(0.3926) Grad: 33730.5859  LR: 0.00000982  \n","Epoch: [3][100/2502] Elapsed 0m 26s (remain 10m 21s) Loss: 0.5617(0.4969) Grad: 192932.9219  LR: 0.00000964  \n","Epoch: [3][200/2502] Elapsed 0m 51s (remain 9m 51s) Loss: 0.6607(0.5067) Grad: 72909.8984  LR: 0.00000946  \n","Epoch: [3][300/2502] Elapsed 1m 16s (remain 9m 21s) Loss: 0.6340(0.5100) Grad: 200748.8125  LR: 0.00000927  \n","Epoch: [3][400/2502] Elapsed 1m 41s (remain 8m 54s) Loss: 0.4907(0.5094) Grad: 11087.7334  LR: 0.00000909  \n","Epoch: [3][500/2502] Elapsed 2m 7s (remain 8m 27s) Loss: 0.4842(0.5075) Grad: 109092.0703  LR: 0.00000891  \n","Epoch: [3][600/2502] Elapsed 2m 32s (remain 8m 1s) Loss: 0.6456(0.5077) Grad: 99045.4609  LR: 0.00000872  \n","Epoch: [3][700/2502] Elapsed 2m 57s (remain 7m 35s) Loss: 0.4674(0.5061) Grad: 414569.0938  LR: 0.00000853  \n","Epoch: [3][800/2502] Elapsed 3m 22s (remain 7m 10s) Loss: 0.5151(0.5054) Grad: 74112.7500  LR: 0.00000835  \n","Epoch: [3][900/2502] Elapsed 3m 47s (remain 6m 44s) Loss: 0.5042(0.5054) Grad: 229796.9375  LR: 0.00000816  \n","Epoch: [3][1000/2502] Elapsed 4m 12s (remain 6m 19s) Loss: 0.4971(0.5066) Grad: 66244.0312  LR: 0.00000797  \n","Epoch: [3][1100/2502] Elapsed 4m 38s (remain 5m 53s) Loss: 0.6747(0.5063) Grad: 107577.4375  LR: 0.00000778  \n","Epoch: [3][1200/2502] Elapsed 5m 3s (remain 5m 28s) Loss: 0.4218(0.5066) Grad: 65899.0156  LR: 0.00000760  \n","Epoch: [3][1300/2502] Elapsed 5m 28s (remain 5m 3s) Loss: 0.4797(0.5067) Grad: 41965.1562  LR: 0.00000741  \n","Epoch: [3][1400/2502] Elapsed 5m 53s (remain 4m 37s) Loss: 0.5686(0.5070) Grad: 257524.4375  LR: 0.00000722  \n","Epoch: [3][1500/2502] Elapsed 6m 18s (remain 4m 12s) Loss: 0.6041(0.5068) Grad: 33109.6445  LR: 0.00000703  \n","Epoch: [3][1600/2502] Elapsed 6m 44s (remain 3m 47s) Loss: 0.4963(0.5073) Grad: 94192.9141  LR: 0.00000684  \n","Epoch: [3][1700/2502] Elapsed 7m 9s (remain 3m 22s) Loss: 0.5077(0.5072) Grad: 357727.8750  LR: 0.00000666  \n","Epoch: [3][1800/2502] Elapsed 7m 34s (remain 2m 56s) Loss: 0.4986(0.5071) Grad: 54223.2188  LR: 0.00000647  \n","Epoch: [3][1900/2502] Elapsed 7m 59s (remain 2m 31s) Loss: 0.4117(0.5067) Grad: 86403.7188  LR: 0.00000628  \n","Epoch: [3][2000/2502] Elapsed 8m 24s (remain 2m 6s) Loss: 0.5180(0.5062) Grad: 117673.5234  LR: 0.00000610  \n","Epoch: [3][2100/2502] Elapsed 8m 49s (remain 1m 41s) Loss: 0.6064(0.5065) Grad: 658148.8125  LR: 0.00000591  \n","Epoch: [3][2200/2502] Elapsed 9m 14s (remain 1m 15s) Loss: 0.4741(0.5061) Grad: 77366.8828  LR: 0.00000573  \n","Epoch: [3][2300/2502] Elapsed 9m 39s (remain 0m 50s) Loss: 0.5346(0.5061) Grad: 378842.3438  LR: 0.00000555  \n","Epoch: [3][2400/2502] Elapsed 10m 4s (remain 0m 25s) Loss: 0.4448(0.5067) Grad: 268172.8125  LR: 0.00000537  \n","Epoch: [3][2500/2502] Elapsed 10m 30s (remain 0m 0s) Loss: 0.6092(0.5065) Grad: 192267.3750  LR: 0.00000519  \n","Epoch: [3][2501/2502] Elapsed 10m 30s (remain 0m 0s) Loss: 0.4322(0.5065) Grad: 77011.2969  LR: 0.00000518  \n","EVAL: [0/103] Elapsed 0m 0s (remain 0m 45s) Loss: 0.5535(0.5535) \n","EVAL: [100/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.4815(0.5586) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5065  avg_val_loss: 0.5584  time: 644s\n","Epoch 3 - Score: 0.8274\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [102/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6580(0.5584) \n","Epoch: [4][0/2502] Elapsed 0m 0s (remain 26m 25s) Loss: 0.4955(0.4955) Grad: 37701.1289  LR: 0.00000518  \n","Epoch: [4][100/2502] Elapsed 0m 25s (remain 10m 13s) Loss: 0.6304(0.5059) Grad: 48890.2891  LR: 0.00000500  \n","Epoch: [4][200/2502] Elapsed 0m 50s (remain 9m 43s) Loss: 0.5631(0.5082) Grad: 109663.9922  LR: 0.00000483  \n","Epoch: [4][300/2502] Elapsed 1m 16s (remain 9m 16s) Loss: 0.5049(0.5012) Grad: 59943.8867  LR: 0.00000465  \n","Epoch: [4][400/2502] Elapsed 1m 41s (remain 8m 50s) Loss: 0.5639(0.4997) Grad: 83249.6875  LR: 0.00000448  \n","Epoch: [4][500/2502] Elapsed 2m 6s (remain 8m 24s) Loss: 0.5633(0.5031) Grad: 85131.9453  LR: 0.00000431  \n","Epoch: [4][600/2502] Elapsed 2m 31s (remain 7m 59s) Loss: 0.5339(0.5033) Grad: 67946.5547  LR: 0.00000414  \n","Epoch: [4][700/2502] Elapsed 2m 56s (remain 7m 33s) Loss: 0.5472(0.5026) Grad: 26683.9297  LR: 0.00000397  \n","Epoch: [4][800/2502] Elapsed 3m 21s (remain 7m 8s) Loss: 0.4275(0.5020) Grad: 111097.1094  LR: 0.00000381  \n","Epoch: [4][900/2502] Elapsed 3m 46s (remain 6m 43s) Loss: 0.4057(0.5013) Grad: 46999.6719  LR: 0.00000364  \n","Epoch: [4][1000/2502] Elapsed 4m 12s (remain 6m 18s) Loss: 0.6061(0.5019) Grad: 53542.1328  LR: 0.00000348  \n","Epoch: [4][1100/2502] Elapsed 4m 37s (remain 5m 52s) Loss: 0.4240(0.5020) Grad: 31370.1250  LR: 0.00000332  \n","Epoch: [4][1200/2502] Elapsed 5m 2s (remain 5m 27s) Loss: 0.6246(0.5004) Grad: 616402.1250  LR: 0.00000317  \n","Epoch: [4][1300/2502] Elapsed 5m 27s (remain 5m 2s) Loss: 0.5557(0.5000) Grad: 32380.6406  LR: 0.00000302  \n","Epoch: [4][1400/2502] Elapsed 5m 52s (remain 4m 37s) Loss: 0.5767(0.4998) Grad: 58154.5039  LR: 0.00000287  \n","Epoch: [4][1500/2502] Elapsed 6m 17s (remain 4m 12s) Loss: 0.5923(0.5005) Grad: 34514.9727  LR: 0.00000272  \n","Epoch: [4][1600/2502] Elapsed 6m 42s (remain 3m 46s) Loss: 0.5086(0.5011) Grad: 52484.5117  LR: 0.00000258  \n","Epoch: [4][1700/2502] Elapsed 7m 8s (remain 3m 21s) Loss: 0.4318(0.5006) Grad: 22691.1641  LR: 0.00000244  \n","Epoch: [4][1800/2502] Elapsed 7m 33s (remain 2m 56s) Loss: 0.4572(0.5004) Grad: 52179.6758  LR: 0.00000230  \n","Epoch: [4][1900/2502] Elapsed 7m 58s (remain 2m 31s) Loss: 0.5047(0.5000) Grad: 47839.6797  LR: 0.00000217  \n","Epoch: [4][2000/2502] Elapsed 8m 23s (remain 2m 6s) Loss: 0.4924(0.4996) Grad: 94889.1250  LR: 0.00000203  \n","Epoch: [4][2100/2502] Elapsed 8m 48s (remain 1m 40s) Loss: 0.4014(0.4992) Grad: 145553.3750  LR: 0.00000191  \n","Epoch: [4][2200/2502] Elapsed 9m 13s (remain 1m 15s) Loss: 0.4886(0.4987) Grad: 43208.3125  LR: 0.00000178  \n","Epoch: [4][2300/2502] Elapsed 9m 38s (remain 0m 50s) Loss: 0.6089(0.4986) Grad: 82041.3828  LR: 0.00000166  \n","Epoch: [4][2400/2502] Elapsed 10m 4s (remain 0m 25s) Loss: 0.4272(0.4984) Grad: 137653.9844  LR: 0.00000155  \n","Epoch: [4][2500/2502] Elapsed 10m 29s (remain 0m 0s) Loss: 0.5106(0.4987) Grad: 234158.6094  LR: 0.00000143  \n","Epoch: [4][2501/2502] Elapsed 10m 29s (remain 0m 0s) Loss: 0.5321(0.4987) Grad: 253753.1250  LR: 0.00000143  \n","EVAL: [0/103] Elapsed 0m 0s (remain 0m 42s) Loss: 0.5253(0.5253) \n","EVAL: [100/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.4904(0.5560) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4987  avg_val_loss: 0.5559  time: 643s\n","Epoch 4 - Score: 0.8304\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [102/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6514(0.5559) \n","Epoch: [5][0/2502] Elapsed 0m 0s (remain 26m 50s) Loss: 0.5895(0.5895) Grad: 26019.2480  LR: 0.00000143  \n","Epoch: [5][100/2502] Elapsed 0m 25s (remain 10m 13s) Loss: 0.5035(0.4951) Grad: 68848.8125  LR: 0.00000132  \n","Epoch: [5][200/2502] Elapsed 0m 50s (remain 9m 42s) Loss: 0.5535(0.5044) Grad: 82586.1562  LR: 0.00000122  \n","Epoch: [5][300/2502] Elapsed 1m 16s (remain 9m 15s) Loss: 0.6022(0.5027) Grad: 63099.2617  LR: 0.00000112  \n","Epoch: [5][400/2502] Elapsed 1m 41s (remain 8m 49s) Loss: 0.4860(0.4981) Grad: 21700.1445  LR: 0.00000102  \n","Epoch: [5][500/2502] Elapsed 2m 6s (remain 8m 24s) Loss: 0.5418(0.4988) Grad: 119238.0469  LR: 0.00000093  \n","Epoch: [5][600/2502] Elapsed 2m 31s (remain 7m 58s) Loss: 0.4918(0.4976) Grad: 44658.2930  LR: 0.00000084  \n","Epoch: [5][700/2502] Elapsed 2m 56s (remain 7m 33s) Loss: 0.4380(0.4955) Grad: 29165.2773  LR: 0.00000075  \n","Epoch: [5][800/2502] Elapsed 3m 21s (remain 7m 8s) Loss: 0.4042(0.4957) Grad: 30669.8359  LR: 0.00000067  \n","Epoch: [5][900/2502] Elapsed 3m 46s (remain 6m 42s) Loss: 0.3272(0.4946) Grad: 75877.4844  LR: 0.00000060  \n","Epoch: [5][1000/2502] Elapsed 4m 11s (remain 6m 17s) Loss: 0.3786(0.4935) Grad: 49209.3555  LR: 0.00000053  \n","Epoch: [5][1100/2502] Elapsed 4m 36s (remain 5m 52s) Loss: 0.4782(0.4930) Grad: 34959.0195  LR: 0.00000046  \n","Epoch: [5][1200/2502] Elapsed 5m 2s (remain 5m 27s) Loss: 0.3845(0.4924) Grad: 31094.8789  LR: 0.00000040  \n","Epoch: [5][1300/2502] Elapsed 5m 27s (remain 5m 2s) Loss: 0.4807(0.4933) Grad: 29413.1250  LR: 0.00000034  \n","Epoch: [5][1400/2502] Elapsed 5m 52s (remain 4m 37s) Loss: 0.5825(0.4936) Grad: 26701.4277  LR: 0.00000029  \n","Epoch: [5][1500/2502] Elapsed 6m 17s (remain 4m 11s) Loss: 0.5848(0.4940) Grad: 18715.6953  LR: 0.00000024  \n","Epoch: [5][1600/2502] Elapsed 6m 42s (remain 3m 46s) Loss: 0.4672(0.4942) Grad: 31760.4688  LR: 0.00000019  \n","Epoch: [5][1700/2502] Elapsed 7m 8s (remain 3m 21s) Loss: 0.4254(0.4938) Grad: 27083.1250  LR: 0.00000015  \n","Epoch: [5][1800/2502] Elapsed 7m 33s (remain 2m 56s) Loss: 0.5486(0.4938) Grad: 15218.4268  LR: 0.00000012  \n","Epoch: [5][1900/2502] Elapsed 7m 58s (remain 2m 31s) Loss: 0.4877(0.4936) Grad: 21493.5020  LR: 0.00000009  \n","Epoch: [5][2000/2502] Elapsed 8m 23s (remain 2m 6s) Loss: 0.4536(0.4938) Grad: 16778.9316  LR: 0.00000006  \n","Epoch: [5][2100/2502] Elapsed 8m 48s (remain 1m 40s) Loss: 0.6371(0.4939) Grad: 24050.5977  LR: 0.00000004  \n","Epoch: [5][2200/2502] Elapsed 9m 13s (remain 1m 15s) Loss: 0.5136(0.4936) Grad: 9855.8945  LR: 0.00000002  \n","Epoch: [5][2300/2502] Elapsed 9m 38s (remain 0m 50s) Loss: 0.6482(0.4938) Grad: 98490.7031  LR: 0.00000001  \n","Epoch: [5][2400/2502] Elapsed 10m 4s (remain 0m 25s) Loss: 0.5782(0.4941) Grad: 91976.8516  LR: 0.00000000  \n","Epoch: [5][2500/2502] Elapsed 10m 29s (remain 0m 0s) Loss: 0.5136(0.4938) Grad: 28287.5957  LR: 0.00000000  \n","Epoch: [5][2501/2502] Elapsed 10m 29s (remain 0m 0s) Loss: 0.6548(0.4939) Grad: 24113.8809  LR: 0.00000000  \n","EVAL: [0/103] Elapsed 0m 0s (remain 0m 42s) Loss: 0.5258(0.5258) \n","EVAL: [100/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.4897(0.5602) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4939  avg_val_loss: 0.5600  time: 643s\n","Epoch 5 - Score: 0.8300\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [102/103] Elapsed 0m 13s (remain 0m 0s) Loss: 0.6448(0.5600) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 13 result ==========\n","Score: 0.8352\n","========== fold: 14 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2463] Elapsed 0m 0s (remain 24m 22s) Loss: 0.7050(0.7050) Grad: 102414.7656  LR: 0.00001500  \n","Epoch: [1][100/2463] Elapsed 0m 25s (remain 9m 59s) Loss: 0.5763(0.6495) Grad: 74440.8750  LR: 0.00001500  \n","Epoch: [1][200/2463] Elapsed 0m 50s (remain 9m 30s) Loss: 0.5830(0.6174) Grad: 33616.8828  LR: 0.00001499  \n","Epoch: [1][300/2463] Elapsed 1m 15s (remain 9m 3s) Loss: 0.4868(0.5972) Grad: 51213.4727  LR: 0.00001498  \n","Epoch: [1][400/2463] Elapsed 1m 40s (remain 8m 38s) Loss: 0.6508(0.5882) Grad: 75674.5938  LR: 0.00001496  \n","Epoch: [1][500/2463] Elapsed 2m 5s (remain 8m 12s) Loss: 0.5487(0.5812) Grad: 190136.8906  LR: 0.00001494  \n","Epoch: [1][600/2463] Elapsed 2m 30s (remain 7m 47s) Loss: 0.6455(0.5765) Grad: 90599.3828  LR: 0.00001491  \n","Epoch: [1][700/2463] Elapsed 2m 56s (remain 7m 22s) Loss: 0.5546(0.5728) Grad: 45124.2617  LR: 0.00001488  \n","Epoch: [1][800/2463] Elapsed 3m 21s (remain 6m 57s) Loss: 0.6239(0.5696) Grad: 53973.7891  LR: 0.00001484  \n","Epoch: [1][900/2463] Elapsed 3m 46s (remain 6m 32s) Loss: 0.5560(0.5679) Grad: 136161.7500  LR: 0.00001480  \n","Epoch: [1][1000/2463] Elapsed 4m 11s (remain 6m 7s) Loss: 0.5806(0.5656) Grad: 32183.3359  LR: 0.00001476  \n","Epoch: [1][1100/2463] Elapsed 4m 36s (remain 5m 42s) Loss: 0.5227(0.5650) Grad: 83956.4844  LR: 0.00001471  \n","Epoch: [1][1200/2463] Elapsed 5m 1s (remain 5m 17s) Loss: 0.3973(0.5636) Grad: 34964.6172  LR: 0.00001465  \n","Epoch: [1][1300/2463] Elapsed 5m 27s (remain 4m 52s) Loss: 0.4801(0.5622) Grad: 46574.2227  LR: 0.00001459  \n","Epoch: [1][1400/2463] Elapsed 5m 52s (remain 4m 26s) Loss: 0.5773(0.5612) Grad: 43380.1953  LR: 0.00001453  \n","Epoch: [1][1500/2463] Elapsed 6m 17s (remain 4m 1s) Loss: 0.4930(0.5596) Grad: 84564.4844  LR: 0.00001446  \n","Epoch: [1][1600/2463] Elapsed 6m 42s (remain 3m 36s) Loss: 0.3834(0.5579) Grad: 156618.3125  LR: 0.00001438  \n","Epoch: [1][1700/2463] Elapsed 7m 7s (remain 3m 11s) Loss: 0.4087(0.5568) Grad: 136594.2344  LR: 0.00001430  \n","Epoch: [1][1800/2463] Elapsed 7m 32s (remain 2m 46s) Loss: 0.5073(0.5562) Grad: 40219.8398  LR: 0.00001422  \n","Epoch: [1][1900/2463] Elapsed 7m 57s (remain 2m 21s) Loss: 0.5827(0.5557) Grad: 44963.1602  LR: 0.00001414  \n","Epoch: [1][2000/2463] Elapsed 8m 22s (remain 1m 56s) Loss: 0.5940(0.5557) Grad: 29357.6406  LR: 0.00001404  \n","Epoch: [1][2100/2463] Elapsed 8m 47s (remain 1m 30s) Loss: 0.5645(0.5550) Grad: 109285.6797  LR: 0.00001395  \n","Epoch: [1][2200/2463] Elapsed 9m 12s (remain 1m 5s) Loss: 0.7048(0.5543) Grad: 102611.7500  LR: 0.00001385  \n","Epoch: [1][2300/2463] Elapsed 9m 37s (remain 0m 40s) Loss: 0.6621(0.5541) Grad: 143009.4219  LR: 0.00001374  \n","Epoch: [1][2400/2463] Elapsed 10m 2s (remain 0m 15s) Loss: 0.4893(0.5532) Grad: 63226.3359  LR: 0.00001364  \n","Epoch: [1][2462/2463] Elapsed 10m 18s (remain 0m 0s) Loss: 0.5253(0.5528) Grad: 40428.6758  LR: 0.00001357  \n","EVAL: [0/143] Elapsed 0m 0s (remain 0m 58s) Loss: 0.3942(0.3942) \n","EVAL: [100/143] Elapsed 0m 13s (remain 0m 5s) Loss: 0.6057(0.5457) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5528  avg_val_loss: 0.5515  time: 637s\n","Epoch 1 - Score: 0.8251\n","Epoch 1 - Save Best Score: 0.8251 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [142/143] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7043(0.5515) \n","Epoch: [2][0/2463] Elapsed 0m 0s (remain 26m 17s) Loss: 0.5959(0.5959) Grad: 150015.5156  LR: 0.00001357  \n","Epoch: [2][100/2463] Elapsed 0m 26s (remain 10m 8s) Loss: 0.5190(0.5254) Grad: 56709.8164  LR: 0.00001345  \n","Epoch: [2][200/2463] Elapsed 0m 51s (remain 9m 40s) Loss: 0.5192(0.5217) Grad: 271099.3438  LR: 0.00001333  \n","Epoch: [2][300/2463] Elapsed 1m 16s (remain 9m 11s) Loss: 0.4363(0.5208) Grad: 81586.7734  LR: 0.00001321  \n","Epoch: [2][400/2463] Elapsed 1m 41s (remain 8m 44s) Loss: 0.5119(0.5205) Grad: 54694.5000  LR: 0.00001309  \n","Epoch: [2][500/2463] Elapsed 2m 6s (remain 8m 17s) Loss: 0.4873(0.5213) Grad: 263393.9375  LR: 0.00001296  \n","Epoch: [2][600/2463] Elapsed 2m 32s (remain 7m 51s) Loss: 0.5198(0.5212) Grad: 73770.7656  LR: 0.00001282  \n","Epoch: [2][700/2463] Elapsed 2m 57s (remain 7m 25s) Loss: 0.5313(0.5204) Grad: 52070.0664  LR: 0.00001269  \n","Epoch: [2][800/2463] Elapsed 3m 22s (remain 7m 0s) Loss: 0.4844(0.5216) Grad: 58213.0234  LR: 0.00001255  \n","Epoch: [2][900/2463] Elapsed 3m 47s (remain 6m 34s) Loss: 0.6221(0.5218) Grad: 50929.7344  LR: 0.00001240  \n","Epoch: [2][1000/2463] Elapsed 4m 12s (remain 6m 9s) Loss: 0.6261(0.5204) Grad: 180056.9219  LR: 0.00001226  \n","Epoch: [2][1100/2463] Elapsed 4m 38s (remain 5m 43s) Loss: 0.5441(0.5204) Grad: 76228.2266  LR: 0.00001211  \n","Epoch: [2][1200/2463] Elapsed 5m 3s (remain 5m 18s) Loss: 0.5093(0.5203) Grad: 73249.9453  LR: 0.00001196  \n","Epoch: [2][1300/2463] Elapsed 5m 28s (remain 4m 53s) Loss: 0.4201(0.5197) Grad: 95340.9141  LR: 0.00001180  \n","Epoch: [2][1400/2463] Elapsed 5m 53s (remain 4m 28s) Loss: 0.4211(0.5188) Grad: 136968.8281  LR: 0.00001164  \n","Epoch: [2][1500/2463] Elapsed 6m 18s (remain 4m 2s) Loss: 0.5268(0.5180) Grad: 72352.4609  LR: 0.00001148  \n","Epoch: [2][1600/2463] Elapsed 6m 43s (remain 3m 37s) Loss: 0.5706(0.5186) Grad: 132119.5625  LR: 0.00001132  \n","Epoch: [2][1700/2463] Elapsed 7m 9s (remain 3m 12s) Loss: 0.5535(0.5180) Grad: 66882.7734  LR: 0.00001115  \n","Epoch: [2][1800/2463] Elapsed 7m 34s (remain 2m 46s) Loss: 0.6027(0.5182) Grad: 40135.6133  LR: 0.00001098  \n","Epoch: [2][1900/2463] Elapsed 7m 59s (remain 2m 21s) Loss: 0.4014(0.5182) Grad: 41722.2305  LR: 0.00001081  \n","Epoch: [2][2000/2463] Elapsed 8m 24s (remain 1m 56s) Loss: 0.4899(0.5181) Grad: 53605.9375  LR: 0.00001064  \n","Epoch: [2][2100/2463] Elapsed 8m 49s (remain 1m 31s) Loss: 0.4541(0.5178) Grad: 66011.9688  LR: 0.00001047  \n","Epoch: [2][2200/2463] Elapsed 9m 14s (remain 1m 6s) Loss: 0.4512(0.5182) Grad: 265289.7188  LR: 0.00001029  \n","Epoch: [2][2300/2463] Elapsed 9m 39s (remain 0m 40s) Loss: 0.4270(0.5176) Grad: 108041.7109  LR: 0.00001011  \n","Epoch: [2][2400/2463] Elapsed 10m 4s (remain 0m 15s) Loss: 0.6615(0.5176) Grad: 110634.2891  LR: 0.00000993  \n","Epoch: [2][2462/2463] Elapsed 10m 20s (remain 0m 0s) Loss: 0.4659(0.5177) Grad: 99707.1172  LR: 0.00000982  \n","EVAL: [0/143] Elapsed 0m 0s (remain 0m 56s) Loss: 0.3983(0.3983) \n","EVAL: [100/143] Elapsed 0m 13s (remain 0m 5s) Loss: 0.6126(0.5473) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5177  avg_val_loss: 0.5508  time: 639s\n","Epoch 2 - Score: 0.8312\n","Epoch 2 - Save Best Score: 0.8312 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [142/143] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7000(0.5508) \n","Epoch: [3][0/2463] Elapsed 0m 0s (remain 26m 20s) Loss: 0.5676(0.5676) Grad: 46667.0078  LR: 0.00000982  \n","Epoch: [3][100/2463] Elapsed 0m 26s (remain 10m 9s) Loss: 0.5219(0.5094) Grad: 87398.0938  LR: 0.00000963  \n","Epoch: [3][200/2463] Elapsed 0m 51s (remain 9m 40s) Loss: 0.5003(0.5023) Grad: 86000.4453  LR: 0.00000945  \n","Epoch: [3][300/2463] Elapsed 1m 16s (remain 9m 10s) Loss: 0.5823(0.5068) Grad: 75921.8281  LR: 0.00000926  \n","Epoch: [3][400/2463] Elapsed 1m 41s (remain 8m 43s) Loss: 0.3341(0.5076) Grad: 16980.2695  LR: 0.00000908  \n","Epoch: [3][500/2463] Elapsed 2m 6s (remain 8m 16s) Loss: 0.4692(0.5034) Grad: 55435.3125  LR: 0.00000889  \n","Epoch: [3][600/2463] Elapsed 2m 31s (remain 7m 50s) Loss: 0.5708(0.5045) Grad: 84594.1719  LR: 0.00000870  \n","Epoch: [3][700/2463] Elapsed 2m 56s (remain 7m 24s) Loss: 0.4166(0.5035) Grad: 79603.7109  LR: 0.00000851  \n","Epoch: [3][800/2463] Elapsed 3m 22s (remain 6m 59s) Loss: 0.5261(0.5049) Grad: 77154.1484  LR: 0.00000832  \n","Epoch: [3][900/2463] Elapsed 3m 47s (remain 6m 33s) Loss: 0.4462(0.5032) Grad: 70003.8125  LR: 0.00000813  \n","Epoch: [3][1000/2463] Elapsed 4m 12s (remain 6m 8s) Loss: 0.5052(0.5029) Grad: 48656.1250  LR: 0.00000794  \n","Epoch: [3][1100/2463] Elapsed 4m 37s (remain 5m 43s) Loss: 0.5515(0.5016) Grad: 122378.6172  LR: 0.00000775  \n","Epoch: [3][1200/2463] Elapsed 5m 2s (remain 5m 17s) Loss: 0.4871(0.5019) Grad: 53756.2031  LR: 0.00000756  \n","Epoch: [3][1300/2463] Elapsed 5m 27s (remain 4m 52s) Loss: 0.5638(0.5023) Grad: 33077.4727  LR: 0.00000737  \n","Epoch: [3][1400/2463] Elapsed 5m 52s (remain 4m 27s) Loss: 0.3976(0.5027) Grad: 50239.7383  LR: 0.00000718  \n","Epoch: [3][1500/2463] Elapsed 6m 17s (remain 4m 2s) Loss: 0.4882(0.5039) Grad: 255154.9844  LR: 0.00000698  \n","Epoch: [3][1600/2463] Elapsed 6m 42s (remain 3m 36s) Loss: 0.2934(0.5043) Grad: 57781.9219  LR: 0.00000679  \n","Epoch: [3][1700/2463] Elapsed 7m 7s (remain 3m 11s) Loss: 0.5010(0.5048) Grad: 319546.2812  LR: 0.00000660  \n","Epoch: [3][1800/2463] Elapsed 7m 32s (remain 2m 46s) Loss: 0.5380(0.5054) Grad: 66738.2344  LR: 0.00000641  \n","Epoch: [3][1900/2463] Elapsed 7m 58s (remain 2m 21s) Loss: 0.4743(0.5052) Grad: 16871.0801  LR: 0.00000623  \n","Epoch: [3][2000/2463] Elapsed 8m 23s (remain 1m 56s) Loss: 0.5679(0.5049) Grad: 132350.0469  LR: 0.00000604  \n","Epoch: [3][2100/2463] Elapsed 8m 48s (remain 1m 30s) Loss: 0.4258(0.5047) Grad: 444367.9688  LR: 0.00000585  \n","Epoch: [3][2200/2463] Elapsed 9m 13s (remain 1m 5s) Loss: 0.5687(0.5046) Grad: 117163.6484  LR: 0.00000566  \n","Epoch: [3][2300/2463] Elapsed 9m 38s (remain 0m 40s) Loss: 0.5630(0.5045) Grad: 129635.0781  LR: 0.00000548  \n","Epoch: [3][2400/2463] Elapsed 10m 3s (remain 0m 15s) Loss: 0.3130(0.5045) Grad: 56352.7344  LR: 0.00000530  \n","Epoch: [3][2462/2463] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5698(0.5042) Grad: 78089.8125  LR: 0.00000518  \n","EVAL: [0/143] Elapsed 0m 0s (remain 1m 1s) Loss: 0.3880(0.3880) \n","EVAL: [100/143] Elapsed 0m 13s (remain 0m 5s) Loss: 0.6750(0.5599) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5042  avg_val_loss: 0.5633  time: 638s\n","Epoch 3 - Score: 0.8406\n","Epoch 3 - Save Best Score: 0.8406 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [142/143] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7027(0.5633) \n","Epoch: [4][0/2463] Elapsed 0m 0s (remain 27m 44s) Loss: 0.5154(0.5154) Grad: 40025.6797  LR: 0.00000518  \n","Epoch: [4][100/2463] Elapsed 0m 25s (remain 10m 6s) Loss: 0.5193(0.5048) Grad: 63393.9023  LR: 0.00000500  \n","Epoch: [4][200/2463] Elapsed 0m 51s (remain 9m 38s) Loss: 0.5273(0.4993) Grad: 27178.8535  LR: 0.00000482  \n","Epoch: [4][300/2463] Elapsed 1m 16s (remain 9m 9s) Loss: 0.5176(0.5048) Grad: 31628.2969  LR: 0.00000464  \n","Epoch: [4][400/2463] Elapsed 1m 41s (remain 8m 41s) Loss: 0.5997(0.5032) Grad: 66958.8125  LR: 0.00000447  \n","Epoch: [4][500/2463] Elapsed 2m 6s (remain 8m 15s) Loss: 0.4446(0.5013) Grad: 40943.8906  LR: 0.00000429  \n","Epoch: [4][600/2463] Elapsed 2m 31s (remain 7m 49s) Loss: 0.4604(0.4997) Grad: 64331.9727  LR: 0.00000412  \n","Epoch: [4][700/2463] Elapsed 2m 56s (remain 7m 23s) Loss: 0.4382(0.4988) Grad: 45875.3555  LR: 0.00000395  \n","Epoch: [4][800/2463] Elapsed 3m 21s (remain 6m 58s) Loss: 0.5559(0.4989) Grad: 60545.1445  LR: 0.00000378  \n","Epoch: [4][900/2463] Elapsed 3m 46s (remain 6m 33s) Loss: 0.3713(0.4984) Grad: 96233.3359  LR: 0.00000362  \n","Epoch: [4][1000/2463] Elapsed 4m 11s (remain 6m 7s) Loss: 0.4945(0.4974) Grad: 51799.4531  LR: 0.00000346  \n","Epoch: [4][1100/2463] Elapsed 4m 36s (remain 5m 42s) Loss: 0.4751(0.4980) Grad: 38842.0547  LR: 0.00000330  \n","Epoch: [4][1200/2463] Elapsed 5m 1s (remain 5m 17s) Loss: 0.3642(0.4980) Grad: 28593.8691  LR: 0.00000314  \n","Epoch: [4][1300/2463] Elapsed 5m 27s (remain 4m 52s) Loss: 0.5440(0.4977) Grad: 52966.6445  LR: 0.00000298  \n","Epoch: [4][1400/2463] Elapsed 5m 52s (remain 4m 27s) Loss: 0.5031(0.4980) Grad: 26686.2422  LR: 0.00000283  \n","Epoch: [4][1500/2463] Elapsed 6m 17s (remain 4m 1s) Loss: 0.3651(0.4975) Grad: 22402.5586  LR: 0.00000269  \n","Epoch: [4][1600/2463] Elapsed 6m 42s (remain 3m 36s) Loss: 0.3949(0.4971) Grad: 31296.9414  LR: 0.00000254  \n","Epoch: [4][1700/2463] Elapsed 7m 7s (remain 3m 11s) Loss: 0.5034(0.4969) Grad: 20260.3262  LR: 0.00000240  \n","Epoch: [4][1800/2463] Elapsed 7m 32s (remain 2m 46s) Loss: 0.5773(0.4968) Grad: 385619.0625  LR: 0.00000226  \n","Epoch: [4][1900/2463] Elapsed 7m 57s (remain 2m 21s) Loss: 0.3530(0.4964) Grad: 50001.2383  LR: 0.00000212  \n","Epoch: [4][2000/2463] Elapsed 8m 23s (remain 1m 56s) Loss: 0.6025(0.4965) Grad: 83337.8516  LR: 0.00000199  \n","Epoch: [4][2100/2463] Elapsed 8m 48s (remain 1m 30s) Loss: 0.4792(0.4972) Grad: 119052.1250  LR: 0.00000186  \n","Epoch: [4][2200/2463] Elapsed 9m 13s (remain 1m 5s) Loss: 0.5839(0.4972) Grad: 52613.2852  LR: 0.00000174  \n","Epoch: [4][2300/2463] Elapsed 9m 38s (remain 0m 40s) Loss: 0.5969(0.4968) Grad: 86097.2891  LR: 0.00000162  \n","Epoch: [4][2400/2463] Elapsed 10m 3s (remain 0m 15s) Loss: 0.4617(0.4963) Grad: 487595.0938  LR: 0.00000150  \n","Epoch: [4][2462/2463] Elapsed 10m 18s (remain 0m 0s) Loss: 0.4048(0.4960) Grad: 52572.8164  LR: 0.00000143  \n","EVAL: [0/143] Elapsed 0m 0s (remain 1m 0s) Loss: 0.3939(0.3939) \n","EVAL: [100/143] Elapsed 0m 13s (remain 0m 5s) Loss: 0.6232(0.5666) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4960  avg_val_loss: 0.5705  time: 638s\n","Epoch 4 - Score: 0.8370\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [142/143] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7036(0.5705) \n","Epoch: [5][0/2463] Elapsed 0m 0s (remain 26m 33s) Loss: 0.5631(0.5631) Grad: 24632.8809  LR: 0.00000143  \n","Epoch: [5][100/2463] Elapsed 0m 25s (remain 10m 2s) Loss: 0.3627(0.4901) Grad: 18535.7812  LR: 0.00000132  \n","Epoch: [5][200/2463] Elapsed 0m 50s (remain 9m 32s) Loss: 0.5692(0.4980) Grad: 118188.3516  LR: 0.00000121  \n","Epoch: [5][300/2463] Elapsed 1m 15s (remain 9m 5s) Loss: 0.4693(0.4976) Grad: 112287.5312  LR: 0.00000111  \n","Epoch: [5][400/2463] Elapsed 1m 41s (remain 8m 39s) Loss: 0.5049(0.4940) Grad: 13951.9941  LR: 0.00000101  \n","Epoch: [5][500/2463] Elapsed 2m 6s (remain 8m 14s) Loss: 0.5423(0.4939) Grad: 49300.1953  LR: 0.00000092  \n","Epoch: [5][600/2463] Elapsed 2m 31s (remain 7m 48s) Loss: 0.5804(0.4915) Grad: 21688.0000  LR: 0.00000083  \n","Epoch: [5][700/2463] Elapsed 2m 56s (remain 7m 23s) Loss: 0.5330(0.4918) Grad: 15121.2549  LR: 0.00000074  \n","Epoch: [5][800/2463] Elapsed 3m 21s (remain 6m 58s) Loss: 0.4610(0.4933) Grad: 31617.9062  LR: 0.00000066  \n","Epoch: [5][900/2463] Elapsed 3m 46s (remain 6m 33s) Loss: 0.5869(0.4926) Grad: 13082.6025  LR: 0.00000059  \n","Epoch: [5][1000/2463] Elapsed 4m 11s (remain 6m 8s) Loss: 0.4690(0.4929) Grad: 39302.6211  LR: 0.00000052  \n","Epoch: [5][1100/2463] Elapsed 4m 37s (remain 5m 42s) Loss: 0.5598(0.4925) Grad: 20219.9023  LR: 0.00000045  \n","Epoch: [5][1200/2463] Elapsed 5m 2s (remain 5m 17s) Loss: 0.7454(0.4920) Grad: 38650.3633  LR: 0.00000039  \n","Epoch: [5][1300/2463] Elapsed 5m 27s (remain 4m 52s) Loss: 0.4794(0.4916) Grad: 22869.9414  LR: 0.00000033  \n","Epoch: [5][1400/2463] Elapsed 5m 52s (remain 4m 27s) Loss: 0.5496(0.4917) Grad: 42318.5039  LR: 0.00000027  \n","Epoch: [5][1500/2463] Elapsed 6m 18s (remain 4m 2s) Loss: 0.3687(0.4921) Grad: 11838.7773  LR: 0.00000022  \n","Epoch: [5][1600/2463] Elapsed 6m 43s (remain 3m 37s) Loss: 0.4733(0.4911) Grad: 18065.9414  LR: 0.00000018  \n","Epoch: [5][1700/2463] Elapsed 7m 8s (remain 3m 11s) Loss: 0.5733(0.4906) Grad: 26302.2402  LR: 0.00000014  \n","Epoch: [5][1800/2463] Elapsed 7m 33s (remain 2m 46s) Loss: 0.5230(0.4901) Grad: 29878.2402  LR: 0.00000011  \n","Epoch: [5][1900/2463] Elapsed 7m 58s (remain 2m 21s) Loss: 0.4048(0.4910) Grad: 17978.6797  LR: 0.00000008  \n","Epoch: [5][2000/2463] Elapsed 8m 23s (remain 1m 56s) Loss: 0.5146(0.4911) Grad: 22818.4844  LR: 0.00000005  \n","Epoch: [5][2100/2463] Elapsed 8m 48s (remain 1m 31s) Loss: 0.5972(0.4915) Grad: 24462.0059  LR: 0.00000003  \n","Epoch: [5][2200/2463] Elapsed 9m 13s (remain 1m 5s) Loss: 0.4232(0.4913) Grad: 10745.1426  LR: 0.00000002  \n","Epoch: [5][2300/2463] Elapsed 9m 38s (remain 0m 40s) Loss: 0.3481(0.4911) Grad: 97137.1562  LR: 0.00000001  \n","Epoch: [5][2400/2463] Elapsed 10m 3s (remain 0m 15s) Loss: 0.5492(0.4916) Grad: 108564.4609  LR: 0.00000000  \n","Epoch: [5][2462/2463] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5428(0.4911) Grad: 28905.1465  LR: 0.00000000  \n","EVAL: [0/143] Elapsed 0m 0s (remain 0m 59s) Loss: 0.3968(0.3968) \n","EVAL: [100/143] Elapsed 0m 13s (remain 0m 5s) Loss: 0.6202(0.5718) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4911  avg_val_loss: 0.5760  time: 639s\n","Epoch 5 - Score: 0.8344\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [142/143] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7039(0.5760) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 14 result ==========\n","Score: 0.8406\n","========== fold: 15 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2461] Elapsed 0m 0s (remain 24m 32s) Loss: 0.7249(0.7249) Grad: 44473.1406  LR: 0.00001500  \n","Epoch: [1][100/2461] Elapsed 0m 25s (remain 9m 59s) Loss: 0.5456(0.6440) Grad: 55535.9570  LR: 0.00001500  \n","Epoch: [1][200/2461] Elapsed 0m 50s (remain 9m 29s) Loss: 0.6584(0.6212) Grad: 107552.3594  LR: 0.00001499  \n","Epoch: [1][300/2461] Elapsed 1m 15s (remain 9m 2s) Loss: 0.4434(0.6037) Grad: 24842.6543  LR: 0.00001498  \n","Epoch: [1][400/2461] Elapsed 1m 40s (remain 8m 36s) Loss: 0.6607(0.5951) Grad: 52622.2500  LR: 0.00001496  \n","Epoch: [1][500/2461] Elapsed 2m 5s (remain 8m 11s) Loss: 0.6345(0.5887) Grad: 47247.9570  LR: 0.00001494  \n","Epoch: [1][600/2461] Elapsed 2m 30s (remain 7m 46s) Loss: 0.4343(0.5837) Grad: 18973.3496  LR: 0.00001491  \n","Epoch: [1][700/2461] Elapsed 2m 55s (remain 7m 21s) Loss: 0.3713(0.5802) Grad: 42570.7930  LR: 0.00001488  \n","Epoch: [1][800/2461] Elapsed 3m 20s (remain 6m 55s) Loss: 0.5311(0.5768) Grad: 34218.1250  LR: 0.00001484  \n","Epoch: [1][900/2461] Elapsed 3m 45s (remain 6m 30s) Loss: 0.5421(0.5720) Grad: 20331.4688  LR: 0.00001480  \n","Epoch: [1][1000/2461] Elapsed 4m 10s (remain 6m 5s) Loss: 0.5358(0.5679) Grad: 17235.9023  LR: 0.00001476  \n","Epoch: [1][1100/2461] Elapsed 4m 35s (remain 5m 40s) Loss: 0.4927(0.5668) Grad: 14106.2061  LR: 0.00001471  \n","Epoch: [1][1200/2461] Elapsed 5m 0s (remain 5m 15s) Loss: 0.4068(0.5660) Grad: 38432.7930  LR: 0.00001465  \n","Epoch: [1][1300/2461] Elapsed 5m 26s (remain 4m 50s) Loss: 0.5317(0.5646) Grad: 24135.1973  LR: 0.00001459  \n","Epoch: [1][1400/2461] Elapsed 5m 51s (remain 4m 25s) Loss: 0.5458(0.5631) Grad: 30497.7988  LR: 0.00001453  \n","Epoch: [1][1500/2461] Elapsed 6m 16s (remain 4m 0s) Loss: 0.5305(0.5611) Grad: 16051.5752  LR: 0.00001446  \n","Epoch: [1][1600/2461] Elapsed 6m 41s (remain 3m 35s) Loss: 0.6118(0.5599) Grad: 12691.8574  LR: 0.00001438  \n","Epoch: [1][1700/2461] Elapsed 7m 6s (remain 3m 10s) Loss: 0.6579(0.5590) Grad: 53971.9336  LR: 0.00001430  \n","Epoch: [1][1800/2461] Elapsed 7m 31s (remain 2m 45s) Loss: 0.6072(0.5584) Grad: 20476.3105  LR: 0.00001422  \n","Epoch: [1][1900/2461] Elapsed 7m 56s (remain 2m 20s) Loss: 0.5911(0.5575) Grad: 28457.8496  LR: 0.00001413  \n","Epoch: [1][2000/2461] Elapsed 8m 21s (remain 1m 55s) Loss: 0.3668(0.5559) Grad: 56250.5664  LR: 0.00001404  \n","Epoch: [1][2100/2461] Elapsed 8m 46s (remain 1m 30s) Loss: 0.5950(0.5556) Grad: 14789.1924  LR: 0.00001395  \n","Epoch: [1][2200/2461] Elapsed 9m 11s (remain 1m 5s) Loss: 0.6359(0.5551) Grad: 20306.7793  LR: 0.00001385  \n","Epoch: [1][2300/2461] Elapsed 9m 36s (remain 0m 40s) Loss: 0.4404(0.5544) Grad: 48200.3008  LR: 0.00001374  \n","Epoch: [1][2400/2461] Elapsed 10m 1s (remain 0m 15s) Loss: 0.6476(0.5538) Grad: 181998.6719  LR: 0.00001364  \n","Epoch: [1][2460/2461] Elapsed 10m 16s (remain 0m 0s) Loss: 0.6033(0.5531) Grad: 133921.9688  LR: 0.00001357  \n","EVAL: [0/144] Elapsed 0m 0s (remain 0m 59s) Loss: 0.4720(0.4720) \n","EVAL: [100/144] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5473(0.5463) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5531  avg_val_loss: 0.5553  time: 636s\n","Epoch 1 - Score: 0.8175\n","Epoch 1 - Save Best Score: 0.8175 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [143/144] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8451(0.5553) \n","Epoch: [2][0/2461] Elapsed 0m 0s (remain 25m 30s) Loss: 0.7072(0.7072) Grad: 345336.5938  LR: 0.00001357  \n","Epoch: [2][100/2461] Elapsed 0m 26s (remain 10m 8s) Loss: 0.6025(0.5134) Grad: 76167.5625  LR: 0.00001345  \n","Epoch: [2][200/2461] Elapsed 0m 51s (remain 9m 40s) Loss: 0.4089(0.5104) Grad: 31161.5762  LR: 0.00001333  \n","Epoch: [2][300/2461] Elapsed 1m 16s (remain 9m 10s) Loss: 0.5318(0.5127) Grad: 67113.6484  LR: 0.00001321  \n","Epoch: [2][400/2461] Elapsed 1m 41s (remain 8m 42s) Loss: 0.5959(0.5132) Grad: 42991.9102  LR: 0.00001309  \n","Epoch: [2][500/2461] Elapsed 2m 6s (remain 8m 16s) Loss: 0.4534(0.5120) Grad: 63097.8242  LR: 0.00001296  \n","Epoch: [2][600/2461] Elapsed 2m 32s (remain 7m 50s) Loss: 0.3821(0.5113) Grad: 88762.4688  LR: 0.00001282  \n","Epoch: [2][700/2461] Elapsed 2m 57s (remain 7m 24s) Loss: 0.4821(0.5128) Grad: 55028.1953  LR: 0.00001269  \n","Epoch: [2][800/2461] Elapsed 3m 22s (remain 6m 59s) Loss: 0.7590(0.5121) Grad: 232140.6250  LR: 0.00001255  \n","Epoch: [2][900/2461] Elapsed 3m 47s (remain 6m 33s) Loss: 0.5709(0.5132) Grad: 113112.4844  LR: 0.00001240  \n","Epoch: [2][1000/2461] Elapsed 4m 12s (remain 6m 8s) Loss: 0.4981(0.5140) Grad: 167196.6406  LR: 0.00001226  \n","Epoch: [2][1100/2461] Elapsed 4m 37s (remain 5m 43s) Loss: 0.6328(0.5145) Grad: 52374.3672  LR: 0.00001211  \n","Epoch: [2][1200/2461] Elapsed 5m 2s (remain 5m 17s) Loss: 0.5182(0.5153) Grad: 69031.1484  LR: 0.00001196  \n","Epoch: [2][1300/2461] Elapsed 5m 28s (remain 4m 52s) Loss: 0.5233(0.5148) Grad: 106436.3984  LR: 0.00001180  \n","Epoch: [2][1400/2461] Elapsed 5m 53s (remain 4m 27s) Loss: 0.6050(0.5158) Grad: 107388.3672  LR: 0.00001164  \n","Epoch: [2][1500/2461] Elapsed 6m 18s (remain 4m 2s) Loss: 0.4175(0.5152) Grad: 207724.2812  LR: 0.00001148  \n","Epoch: [2][1600/2461] Elapsed 6m 43s (remain 3m 36s) Loss: 0.4780(0.5145) Grad: 192389.0469  LR: 0.00001132  \n","Epoch: [2][1700/2461] Elapsed 7m 8s (remain 3m 11s) Loss: 0.6171(0.5145) Grad: 47721.3242  LR: 0.00001115  \n","Epoch: [2][1800/2461] Elapsed 7m 34s (remain 2m 46s) Loss: 0.5823(0.5154) Grad: 61220.4727  LR: 0.00001098  \n","Epoch: [2][1900/2461] Elapsed 7m 59s (remain 2m 21s) Loss: 0.5075(0.5153) Grad: 82892.7734  LR: 0.00001081  \n","Epoch: [2][2000/2461] Elapsed 8m 24s (remain 1m 55s) Loss: 0.5422(0.5159) Grad: 80469.0234  LR: 0.00001064  \n","Epoch: [2][2100/2461] Elapsed 8m 49s (remain 1m 30s) Loss: 0.4120(0.5152) Grad: 259967.3750  LR: 0.00001046  \n","Epoch: [2][2200/2461] Elapsed 9m 14s (remain 1m 5s) Loss: 0.3429(0.5150) Grad: 73938.0312  LR: 0.00001029  \n","Epoch: [2][2300/2461] Elapsed 9m 39s (remain 0m 40s) Loss: 0.5030(0.5153) Grad: 64678.2227  LR: 0.00001011  \n","Epoch: [2][2400/2461] Elapsed 10m 4s (remain 0m 15s) Loss: 0.3590(0.5151) Grad: 173885.2500  LR: 0.00000993  \n","Epoch: [2][2460/2461] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5640(0.5151) Grad: 98783.0703  LR: 0.00000982  \n","EVAL: [0/144] Elapsed 0m 0s (remain 0m 54s) Loss: 0.4802(0.4802) \n","EVAL: [100/144] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5514(0.5458) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5151  avg_val_loss: 0.5520  time: 639s\n","Epoch 2 - Score: 0.8287\n","Epoch 2 - Save Best Score: 0.8287 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [143/144] Elapsed 0m 18s (remain 0m 0s) Loss: 0.6830(0.5520) \n","Epoch: [3][0/2461] Elapsed 0m 0s (remain 26m 6s) Loss: 0.4446(0.4446) Grad: 56500.5781  LR: 0.00000982  \n","Epoch: [3][100/2461] Elapsed 0m 26s (remain 10m 8s) Loss: 0.5908(0.5129) Grad: 56400.1172  LR: 0.00000964  \n","Epoch: [3][200/2461] Elapsed 0m 51s (remain 9m 38s) Loss: 0.4493(0.5092) Grad: 21808.7617  LR: 0.00000945  \n","Epoch: [3][300/2461] Elapsed 1m 16s (remain 9m 9s) Loss: 0.5109(0.5084) Grad: 43114.5430  LR: 0.00000927  \n","Epoch: [3][400/2461] Elapsed 1m 41s (remain 8m 42s) Loss: 0.4915(0.5087) Grad: 61261.5859  LR: 0.00000908  \n","Epoch: [3][500/2461] Elapsed 2m 6s (remain 8m 15s) Loss: 0.4405(0.5076) Grad: 58195.7891  LR: 0.00000889  \n","Epoch: [3][600/2461] Elapsed 2m 31s (remain 7m 50s) Loss: 0.5718(0.5068) Grad: 67640.4141  LR: 0.00000870  \n","Epoch: [3][700/2461] Elapsed 2m 57s (remain 7m 24s) Loss: 0.5863(0.5059) Grad: 117398.6250  LR: 0.00000851  \n","Epoch: [3][800/2461] Elapsed 3m 22s (remain 6m 58s) Loss: 0.5018(0.5074) Grad: 78265.6875  LR: 0.00000832  \n","Epoch: [3][900/2461] Elapsed 3m 47s (remain 6m 33s) Loss: 0.5264(0.5071) Grad: 38785.4180  LR: 0.00000813  \n","Epoch: [3][1000/2461] Elapsed 4m 12s (remain 6m 8s) Loss: 0.5333(0.5074) Grad: 82048.7266  LR: 0.00000794  \n","Epoch: [3][1100/2461] Elapsed 4m 37s (remain 5m 42s) Loss: 0.4964(0.5068) Grad: 21740.6934  LR: 0.00000775  \n","Epoch: [3][1200/2461] Elapsed 5m 2s (remain 5m 17s) Loss: 0.5972(0.5072) Grad: 137523.6406  LR: 0.00000756  \n","Epoch: [3][1300/2461] Elapsed 5m 27s (remain 4m 52s) Loss: 0.6461(0.5076) Grad: 78924.4688  LR: 0.00000737  \n","Epoch: [3][1400/2461] Elapsed 5m 52s (remain 4m 27s) Loss: 0.4677(0.5079) Grad: 66561.4609  LR: 0.00000718  \n","Epoch: [3][1500/2461] Elapsed 6m 18s (remain 4m 1s) Loss: 0.6072(0.5076) Grad: 61643.3828  LR: 0.00000699  \n","Epoch: [3][1600/2461] Elapsed 6m 43s (remain 3m 36s) Loss: 0.5521(0.5068) Grad: 74764.9922  LR: 0.00000679  \n","Epoch: [3][1700/2461] Elapsed 7m 8s (remain 3m 11s) Loss: 0.5407(0.5066) Grad: 39108.2617  LR: 0.00000660  \n","Epoch: [3][1800/2461] Elapsed 7m 33s (remain 2m 46s) Loss: 0.5664(0.5056) Grad: 171525.3438  LR: 0.00000641  \n","Epoch: [3][1900/2461] Elapsed 7m 58s (remain 2m 20s) Loss: 0.5383(0.5055) Grad: 89336.4219  LR: 0.00000623  \n","Epoch: [3][2000/2461] Elapsed 8m 23s (remain 1m 55s) Loss: 0.3584(0.5060) Grad: 72947.0000  LR: 0.00000604  \n","Epoch: [3][2100/2461] Elapsed 8m 48s (remain 1m 30s) Loss: 0.5707(0.5061) Grad: 226073.7812  LR: 0.00000585  \n","Epoch: [3][2200/2461] Elapsed 9m 13s (remain 1m 5s) Loss: 0.4854(0.5060) Grad: 151567.1250  LR: 0.00000566  \n","Epoch: [3][2300/2461] Elapsed 9m 38s (remain 0m 40s) Loss: 0.6061(0.5060) Grad: 1882083.0000  LR: 0.00000548  \n","Epoch: [3][2400/2461] Elapsed 10m 3s (remain 0m 15s) Loss: 0.5919(0.5055) Grad: 176813.6562  LR: 0.00000530  \n","Epoch: [3][2460/2461] Elapsed 10m 18s (remain 0m 0s) Loss: 0.5510(0.5056) Grad: 114343.8438  LR: 0.00000519  \n","EVAL: [0/144] Elapsed 0m 0s (remain 0m 57s) Loss: 0.4593(0.4593) \n","EVAL: [100/144] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5542(0.5543) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5056  avg_val_loss: 0.5598  time: 638s\n","Epoch 3 - Score: 0.8199\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [143/144] Elapsed 0m 18s (remain 0m 0s) Loss: 0.5919(0.5598) \n","Epoch: [4][0/2461] Elapsed 0m 0s (remain 25m 30s) Loss: 0.5066(0.5066) Grad: 56525.0312  LR: 0.00000518  \n","Epoch: [4][100/2461] Elapsed 0m 25s (remain 10m 1s) Loss: 0.5019(0.4931) Grad: 43021.1602  LR: 0.00000500  \n","Epoch: [4][200/2461] Elapsed 0m 50s (remain 9m 31s) Loss: 0.4290(0.4934) Grad: 30429.2051  LR: 0.00000482  \n","Epoch: [4][300/2461] Elapsed 1m 15s (remain 9m 4s) Loss: 0.5014(0.4921) Grad: 246265.5000  LR: 0.00000464  \n","Epoch: [4][400/2461] Elapsed 1m 40s (remain 8m 38s) Loss: 0.4652(0.4929) Grad: 130645.1797  LR: 0.00000447  \n","Epoch: [4][500/2461] Elapsed 2m 6s (remain 8m 13s) Loss: 0.5353(0.4927) Grad: 40012.0039  LR: 0.00000429  \n","Epoch: [4][600/2461] Elapsed 2m 31s (remain 7m 47s) Loss: 0.4490(0.4943) Grad: 45888.7969  LR: 0.00000412  \n","Epoch: [4][700/2461] Elapsed 2m 56s (remain 7m 22s) Loss: 0.5272(0.4955) Grad: 25302.0527  LR: 0.00000395  \n","Epoch: [4][800/2461] Elapsed 3m 21s (remain 6m 57s) Loss: 0.5471(0.4944) Grad: 187728.8125  LR: 0.00000379  \n","Epoch: [4][900/2461] Elapsed 3m 46s (remain 6m 32s) Loss: 0.3744(0.4952) Grad: 25550.8105  LR: 0.00000362  \n","Epoch: [4][1000/2461] Elapsed 4m 11s (remain 6m 6s) Loss: 0.4582(0.4952) Grad: 159225.0312  LR: 0.00000346  \n","Epoch: [4][1100/2461] Elapsed 4m 36s (remain 5m 41s) Loss: 0.5205(0.4942) Grad: 57620.4805  LR: 0.00000330  \n","Epoch: [4][1200/2461] Elapsed 5m 1s (remain 5m 16s) Loss: 0.4565(0.4944) Grad: 228365.4844  LR: 0.00000314  \n","Epoch: [4][1300/2461] Elapsed 5m 26s (remain 4m 51s) Loss: 0.4987(0.4951) Grad: 79582.5703  LR: 0.00000299  \n","Epoch: [4][1400/2461] Elapsed 5m 52s (remain 4m 26s) Loss: 0.4074(0.4949) Grad: 22022.1387  LR: 0.00000283  \n","Epoch: [4][1500/2461] Elapsed 6m 17s (remain 4m 1s) Loss: 0.4860(0.4959) Grad: 135644.1875  LR: 0.00000269  \n","Epoch: [4][1600/2461] Elapsed 6m 42s (remain 3m 36s) Loss: 0.4664(0.4956) Grad: 69858.7109  LR: 0.00000254  \n","Epoch: [4][1700/2461] Elapsed 7m 7s (remain 3m 10s) Loss: 0.5495(0.4960) Grad: 57761.2930  LR: 0.00000240  \n","Epoch: [4][1800/2461] Elapsed 7m 32s (remain 2m 45s) Loss: 0.5207(0.4962) Grad: 49628.6797  LR: 0.00000226  \n","Epoch: [4][1900/2461] Elapsed 7m 57s (remain 2m 20s) Loss: 0.5436(0.4963) Grad: 38724.3203  LR: 0.00000213  \n","Epoch: [4][2000/2461] Elapsed 8m 22s (remain 1m 55s) Loss: 0.6798(0.4961) Grad: 1388435.8750  LR: 0.00000199  \n","Epoch: [4][2100/2461] Elapsed 8m 47s (remain 1m 30s) Loss: 0.5004(0.4960) Grad: 186241.8750  LR: 0.00000187  \n","Epoch: [4][2200/2461] Elapsed 9m 12s (remain 1m 5s) Loss: 0.4806(0.4966) Grad: 82603.0547  LR: 0.00000174  \n","Epoch: [4][2300/2461] Elapsed 9m 37s (remain 0m 40s) Loss: 0.4414(0.4971) Grad: 63658.0078  LR: 0.00000162  \n","Epoch: [4][2400/2461] Elapsed 10m 3s (remain 0m 15s) Loss: 0.5347(0.4974) Grad: 101798.9531  LR: 0.00000150  \n","Epoch: [4][2460/2461] Elapsed 10m 18s (remain 0m 0s) Loss: 0.5575(0.4975) Grad: 148373.2031  LR: 0.00000144  \n","EVAL: [0/144] Elapsed 0m 0s (remain 0m 56s) Loss: 0.4424(0.4424) \n","EVAL: [100/144] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5522(0.5634) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4975  avg_val_loss: 0.5679  time: 637s\n","Epoch 4 - Score: 0.8138\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [143/144] Elapsed 0m 18s (remain 0m 0s) Loss: 0.6323(0.5679) \n","Epoch: [5][0/2461] Elapsed 0m 0s (remain 25m 13s) Loss: 0.5449(0.5449) Grad: 27656.2168  LR: 0.00000143  \n","Epoch: [5][100/2461] Elapsed 0m 25s (remain 10m 2s) Loss: 0.5040(0.4810) Grad: 44418.2344  LR: 0.00000132  \n","Epoch: [5][200/2461] Elapsed 0m 50s (remain 9m 31s) Loss: 0.5358(0.4863) Grad: 66833.6484  LR: 0.00000122  \n","Epoch: [5][300/2461] Elapsed 1m 15s (remain 9m 5s) Loss: 0.3899(0.4888) Grad: 44956.6875  LR: 0.00000111  \n","Epoch: [5][400/2461] Elapsed 1m 41s (remain 8m 39s) Loss: 0.5050(0.4928) Grad: 31363.2070  LR: 0.00000102  \n","Epoch: [5][500/2461] Elapsed 2m 6s (remain 8m 13s) Loss: 0.4538(0.4920) Grad: 19229.2988  LR: 0.00000092  \n","Epoch: [5][600/2461] Elapsed 2m 31s (remain 7m 48s) Loss: 0.4739(0.4911) Grad: 40534.4297  LR: 0.00000083  \n","Epoch: [5][700/2461] Elapsed 2m 56s (remain 7m 22s) Loss: 0.5114(0.4922) Grad: 67398.8125  LR: 0.00000075  \n","Epoch: [5][800/2461] Elapsed 3m 21s (remain 6m 57s) Loss: 0.4972(0.4921) Grad: 65788.5703  LR: 0.00000067  \n","Epoch: [5][900/2461] Elapsed 3m 46s (remain 6m 32s) Loss: 0.5792(0.4933) Grad: 108801.9609  LR: 0.00000059  \n","Epoch: [5][1000/2461] Elapsed 4m 11s (remain 6m 7s) Loss: 0.5785(0.4942) Grad: 50723.1133  LR: 0.00000052  \n","Epoch: [5][1100/2461] Elapsed 4m 36s (remain 5m 41s) Loss: 0.5838(0.4932) Grad: 33921.1602  LR: 0.00000045  \n","Epoch: [5][1200/2461] Elapsed 5m 1s (remain 5m 16s) Loss: 0.5231(0.4926) Grad: 46301.5508  LR: 0.00000039  \n","Epoch: [5][1300/2461] Elapsed 5m 26s (remain 4m 51s) Loss: 0.5156(0.4931) Grad: 37586.7109  LR: 0.00000033  \n","Epoch: [5][1400/2461] Elapsed 5m 52s (remain 4m 26s) Loss: 0.5332(0.4929) Grad: 36536.1445  LR: 0.00000027  \n","Epoch: [5][1500/2461] Elapsed 6m 17s (remain 4m 1s) Loss: 0.4794(0.4923) Grad: 42194.6953  LR: 0.00000023  \n","Epoch: [5][1600/2461] Elapsed 6m 42s (remain 3m 36s) Loss: 0.5278(0.4919) Grad: 56247.7656  LR: 0.00000018  \n","Epoch: [5][1700/2461] Elapsed 7m 7s (remain 3m 11s) Loss: 0.4994(0.4918) Grad: 212207.5625  LR: 0.00000014  \n","Epoch: [5][1800/2461] Elapsed 7m 32s (remain 2m 45s) Loss: 0.4936(0.4925) Grad: 31042.4141  LR: 0.00000011  \n","Epoch: [5][1900/2461] Elapsed 7m 57s (remain 2m 20s) Loss: 0.4598(0.4920) Grad: 20918.5215  LR: 0.00000008  \n","Epoch: [5][2000/2461] Elapsed 8m 22s (remain 1m 55s) Loss: 0.4210(0.4914) Grad: 67466.5781  LR: 0.00000005  \n","Epoch: [5][2100/2461] Elapsed 8m 48s (remain 1m 30s) Loss: 0.4520(0.4917) Grad: 51065.8086  LR: 0.00000003  \n","Epoch: [5][2200/2461] Elapsed 9m 13s (remain 1m 5s) Loss: 0.5395(0.4924) Grad: 70349.0000  LR: 0.00000002  \n","Epoch: [5][2300/2461] Elapsed 9m 38s (remain 0m 40s) Loss: 0.5440(0.4924) Grad: 55173.8203  LR: 0.00000001  \n","Epoch: [5][2400/2461] Elapsed 10m 3s (remain 0m 15s) Loss: 0.3103(0.4924) Grad: 64887.9727  LR: 0.00000000  \n","Epoch: [5][2460/2461] Elapsed 10m 18s (remain 0m 0s) Loss: 0.4798(0.4924) Grad: 202095.7812  LR: 0.00000000  \n","EVAL: [0/144] Elapsed 0m 0s (remain 0m 58s) Loss: 0.4415(0.4415) \n","EVAL: [100/144] Elapsed 0m 13s (remain 0m 5s) Loss: 0.5514(0.5729) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4924  avg_val_loss: 0.5762  time: 638s\n","Epoch 5 - Score: 0.8128\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [143/144] Elapsed 0m 19s (remain 0m 0s) Loss: 0.6419(0.5762) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 15 result ==========\n","Score: 0.8287\n","========== fold: 16 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2470] Elapsed 0m 0s (remain 21m 47s) Loss: 1.0568(1.0568) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2470] Elapsed 0m 25s (remain 10m 0s) Loss: 0.6067(0.6548) Grad: 31086.0176  LR: 0.00001500  \n","Epoch: [1][200/2470] Elapsed 0m 50s (remain 9m 31s) Loss: 0.6368(0.6200) Grad: 31264.3984  LR: 0.00001499  \n","Epoch: [1][300/2470] Elapsed 1m 15s (remain 9m 5s) Loss: 0.5986(0.6029) Grad: 41420.8438  LR: 0.00001498  \n","Epoch: [1][400/2470] Elapsed 1m 40s (remain 8m 39s) Loss: 0.6171(0.5941) Grad: 38093.1484  LR: 0.00001496  \n","Epoch: [1][500/2470] Elapsed 2m 5s (remain 8m 14s) Loss: 0.6113(0.5859) Grad: 40108.9766  LR: 0.00001494  \n","Epoch: [1][600/2470] Elapsed 2m 30s (remain 7m 49s) Loss: 0.6255(0.5811) Grad: 28092.2070  LR: 0.00001491  \n","Epoch: [1][700/2470] Elapsed 2m 55s (remain 7m 24s) Loss: 0.5821(0.5774) Grad: 36449.9570  LR: 0.00001488  \n","Epoch: [1][800/2470] Elapsed 3m 20s (remain 6m 58s) Loss: 0.6414(0.5741) Grad: 65092.5781  LR: 0.00001484  \n","Epoch: [1][900/2470] Elapsed 3m 46s (remain 6m 33s) Loss: 0.4660(0.5706) Grad: 25835.3672  LR: 0.00001480  \n","Epoch: [1][1000/2470] Elapsed 4m 11s (remain 6m 8s) Loss: 0.5634(0.5679) Grad: 32689.8262  LR: 0.00001476  \n","Epoch: [1][1100/2470] Elapsed 4m 36s (remain 5m 43s) Loss: 0.4578(0.5662) Grad: 23680.6289  LR: 0.00001471  \n","Epoch: [1][1200/2470] Elapsed 5m 1s (remain 5m 18s) Loss: 0.5466(0.5650) Grad: 26575.4277  LR: 0.00001465  \n","Epoch: [1][1300/2470] Elapsed 5m 26s (remain 4m 53s) Loss: 0.6047(0.5630) Grad: 81609.6562  LR: 0.00001459  \n","Epoch: [1][1400/2470] Elapsed 5m 51s (remain 4m 28s) Loss: 0.6689(0.5611) Grad: 36510.2734  LR: 0.00001453  \n","Epoch: [1][1500/2470] Elapsed 6m 16s (remain 4m 3s) Loss: 0.6169(0.5605) Grad: 28205.6953  LR: 0.00001446  \n","Epoch: [1][1600/2470] Elapsed 6m 41s (remain 3m 38s) Loss: 0.4920(0.5598) Grad: 36510.0352  LR: 0.00001439  \n","Epoch: [1][1700/2470] Elapsed 7m 6s (remain 3m 12s) Loss: 0.6045(0.5590) Grad: 22028.4531  LR: 0.00001431  \n","Epoch: [1][1800/2470] Elapsed 7m 31s (remain 2m 47s) Loss: 0.4254(0.5577) Grad: 12788.4258  LR: 0.00001423  \n","Epoch: [1][1900/2470] Elapsed 7m 57s (remain 2m 22s) Loss: 0.5606(0.5571) Grad: 41628.2695  LR: 0.00001414  \n","Epoch: [1][2000/2470] Elapsed 8m 22s (remain 1m 57s) Loss: 0.5798(0.5566) Grad: 18709.4277  LR: 0.00001405  \n","Epoch: [1][2100/2470] Elapsed 8m 47s (remain 1m 32s) Loss: 0.6221(0.5570) Grad: 109792.6484  LR: 0.00001395  \n","Epoch: [1][2200/2470] Elapsed 9m 12s (remain 1m 7s) Loss: 0.5545(0.5564) Grad: 87044.6641  LR: 0.00001386  \n","Epoch: [1][2300/2470] Elapsed 9m 37s (remain 0m 42s) Loss: 0.5703(0.5553) Grad: 56535.3203  LR: 0.00001375  \n","Epoch: [1][2400/2470] Elapsed 10m 2s (remain 0m 17s) Loss: 0.3792(0.5544) Grad: 40177.3008  LR: 0.00001364  \n","Epoch: [1][2469/2470] Elapsed 10m 19s (remain 0m 0s) Loss: 0.6300(0.5535) Grad: 19702.4512  LR: 0.00001357  \n","EVAL: [0/135] Elapsed 0m 0s (remain 0m 57s) Loss: 0.4983(0.4983) \n","EVAL: [100/135] Elapsed 0m 13s (remain 0m 4s) Loss: 0.7450(0.5431) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5535  avg_val_loss: 0.5419  time: 638s\n","Epoch 1 - Score: 0.8256\n","Epoch 1 - Save Best Score: 0.8256 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [134/135] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6511(0.5419) \n","Epoch: [2][0/2470] Elapsed 0m 0s (remain 27m 24s) Loss: 0.5327(0.5327) Grad: 75474.8438  LR: 0.00001357  \n","Epoch: [2][100/2470] Elapsed 0m 26s (remain 10m 13s) Loss: 0.5989(0.5159) Grad: 50075.8984  LR: 0.00001345  \n","Epoch: [2][200/2470] Elapsed 0m 51s (remain 9m 45s) Loss: 0.4004(0.5154) Grad: 159251.8125  LR: 0.00001334  \n","Epoch: [2][300/2470] Elapsed 1m 17s (remain 9m 15s) Loss: 0.5611(0.5171) Grad: 59828.0508  LR: 0.00001321  \n","Epoch: [2][400/2470] Elapsed 1m 42s (remain 8m 47s) Loss: 0.6092(0.5156) Grad: 84435.8984  LR: 0.00001309  \n","Epoch: [2][500/2470] Elapsed 2m 7s (remain 8m 20s) Loss: 0.5564(0.5171) Grad: 39717.9062  LR: 0.00001296  \n","Epoch: [2][600/2470] Elapsed 2m 32s (remain 7m 54s) Loss: 0.5032(0.5180) Grad: 68998.1094  LR: 0.00001283  \n","Epoch: [2][700/2470] Elapsed 2m 57s (remain 7m 28s) Loss: 0.4448(0.5167) Grad: 152363.2031  LR: 0.00001269  \n","Epoch: [2][800/2470] Elapsed 3m 22s (remain 7m 2s) Loss: 0.5221(0.5167) Grad: 225197.3906  LR: 0.00001255  \n","Epoch: [2][900/2470] Elapsed 3m 47s (remain 6m 36s) Loss: 0.6309(0.5159) Grad: 73589.9062  LR: 0.00001241  \n","Epoch: [2][1000/2470] Elapsed 4m 13s (remain 6m 11s) Loss: 0.5101(0.5160) Grad: 61416.9219  LR: 0.00001226  \n","Epoch: [2][1100/2470] Elapsed 4m 38s (remain 5m 45s) Loss: 0.4918(0.5167) Grad: 45324.1797  LR: 0.00001211  \n","Epoch: [2][1200/2470] Elapsed 5m 3s (remain 5m 20s) Loss: 0.6002(0.5161) Grad: 158894.3750  LR: 0.00001196  \n","Epoch: [2][1300/2470] Elapsed 5m 28s (remain 4m 55s) Loss: 0.5343(0.5160) Grad: 148362.0625  LR: 0.00001181  \n","Epoch: [2][1400/2470] Elapsed 5m 53s (remain 4m 29s) Loss: 0.5862(0.5155) Grad: 126052.5469  LR: 0.00001165  \n","Epoch: [2][1500/2470] Elapsed 6m 18s (remain 4m 4s) Loss: 0.3524(0.5145) Grad: 64263.2461  LR: 0.00001149  \n","Epoch: [2][1600/2470] Elapsed 6m 44s (remain 3m 39s) Loss: 0.4833(0.5145) Grad: 212756.1875  LR: 0.00001133  \n","Epoch: [2][1700/2470] Elapsed 7m 9s (remain 3m 14s) Loss: 0.3690(0.5145) Grad: 114076.9453  LR: 0.00001116  \n","Epoch: [2][1800/2470] Elapsed 7m 34s (remain 2m 48s) Loss: 0.5207(0.5145) Grad: 67281.3438  LR: 0.00001099  \n","Epoch: [2][1900/2470] Elapsed 7m 59s (remain 2m 23s) Loss: 0.4488(0.5146) Grad: 186208.3750  LR: 0.00001082  \n","Epoch: [2][2000/2470] Elapsed 8m 24s (remain 1m 58s) Loss: 0.5585(0.5143) Grad: 180237.0156  LR: 0.00001065  \n","Epoch: [2][2100/2470] Elapsed 8m 49s (remain 1m 33s) Loss: 0.4252(0.5142) Grad: 238968.1094  LR: 0.00001048  \n","Epoch: [2][2200/2470] Elapsed 9m 14s (remain 1m 7s) Loss: 0.5484(0.5148) Grad: 191613.9375  LR: 0.00001030  \n","Epoch: [2][2300/2470] Elapsed 9m 39s (remain 0m 42s) Loss: 0.5520(0.5149) Grad: 199713.2812  LR: 0.00001012  \n","Epoch: [2][2400/2470] Elapsed 10m 4s (remain 0m 17s) Loss: 0.4904(0.5152) Grad: 53045.2109  LR: 0.00000994  \n","Epoch: [2][2469/2470] Elapsed 10m 22s (remain 0m 0s) Loss: 0.3271(0.5152) Grad: 143933.0938  LR: 0.00000982  \n","EVAL: [0/135] Elapsed 0m 0s (remain 0m 56s) Loss: 0.5299(0.5299) \n","EVAL: [100/135] Elapsed 0m 13s (remain 0m 4s) Loss: 0.8444(0.5377) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5152  avg_val_loss: 0.5376  time: 640s\n","Epoch 2 - Score: 0.8374\n","Epoch 2 - Save Best Score: 0.8374 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [134/135] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6552(0.5376) \n","Epoch: [3][0/2470] Elapsed 0m 0s (remain 27m 15s) Loss: 0.5752(0.5752) Grad: 83207.1406  LR: 0.00000982  \n","Epoch: [3][100/2470] Elapsed 0m 26s (remain 10m 10s) Loss: 0.6614(0.5129) Grad: 56253.5781  LR: 0.00000964  \n","Epoch: [3][200/2470] Elapsed 0m 51s (remain 9m 42s) Loss: 0.6001(0.5151) Grad: 146017.9219  LR: 0.00000945  \n","Epoch: [3][300/2470] Elapsed 1m 16s (remain 9m 12s) Loss: 0.5809(0.5093) Grad: 80256.0625  LR: 0.00000927  \n","Epoch: [3][400/2470] Elapsed 1m 41s (remain 8m 44s) Loss: 0.6171(0.5060) Grad: 360432.3750  LR: 0.00000908  \n","Epoch: [3][500/2470] Elapsed 2m 6s (remain 8m 17s) Loss: 0.4847(0.5065) Grad: 77785.2812  LR: 0.00000889  \n","Epoch: [3][600/2470] Elapsed 2m 31s (remain 7m 51s) Loss: 0.4556(0.5069) Grad: 219395.7969  LR: 0.00000871  \n","Epoch: [3][700/2470] Elapsed 2m 56s (remain 7m 25s) Loss: 0.3997(0.5056) Grad: 54165.9570  LR: 0.00000852  \n","Epoch: [3][800/2470] Elapsed 3m 21s (remain 6m 59s) Loss: 0.3991(0.5031) Grad: 41878.8086  LR: 0.00000833  \n","Epoch: [3][900/2470] Elapsed 3m 46s (remain 6m 34s) Loss: 0.5475(0.5030) Grad: 42381.8438  LR: 0.00000814  \n","Epoch: [3][1000/2470] Elapsed 4m 11s (remain 6m 8s) Loss: 0.5698(0.5041) Grad: 100661.3359  LR: 0.00000795  \n","Epoch: [3][1100/2470] Elapsed 4m 36s (remain 5m 43s) Loss: 0.5976(0.5048) Grad: 72972.8828  LR: 0.00000776  \n","Epoch: [3][1200/2470] Elapsed 5m 1s (remain 5m 18s) Loss: 0.4641(0.5044) Grad: 39996.5742  LR: 0.00000757  \n","Epoch: [3][1300/2470] Elapsed 5m 26s (remain 4m 52s) Loss: 0.4653(0.5044) Grad: 355118.6875  LR: 0.00000738  \n","Epoch: [3][1400/2470] Elapsed 5m 51s (remain 4m 28s) Loss: 0.5675(0.5041) Grad: 47869.6055  LR: 0.00000719  \n","Epoch: [3][1500/2470] Elapsed 6m 16s (remain 4m 2s) Loss: 0.4633(0.5040) Grad: 60405.2578  LR: 0.00000700  \n","Epoch: [3][1600/2470] Elapsed 6m 41s (remain 3m 37s) Loss: 0.5513(0.5045) Grad: 91485.7578  LR: 0.00000681  \n","Epoch: [3][1700/2470] Elapsed 7m 5s (remain 3m 12s) Loss: 0.4613(0.5046) Grad: 40164.8594  LR: 0.00000662  \n","Epoch: [3][1800/2470] Elapsed 7m 30s (remain 2m 47s) Loss: 0.5467(0.5043) Grad: 53296.6875  LR: 0.00000643  \n","Epoch: [3][1900/2470] Elapsed 7m 55s (remain 2m 22s) Loss: 0.3489(0.5045) Grad: 68760.7109  LR: 0.00000624  \n","Epoch: [3][2000/2470] Elapsed 8m 20s (remain 1m 57s) Loss: 0.2425(0.5043) Grad: 216809.8750  LR: 0.00000605  \n","Epoch: [3][2100/2470] Elapsed 8m 45s (remain 1m 32s) Loss: 0.6403(0.5048) Grad: 323995.7500  LR: 0.00000586  \n","Epoch: [3][2200/2470] Elapsed 9m 10s (remain 1m 7s) Loss: 0.6367(0.5053) Grad: 32699.6562  LR: 0.00000568  \n","Epoch: [3][2300/2470] Elapsed 9m 35s (remain 0m 42s) Loss: 0.4760(0.5053) Grad: 48351.1641  LR: 0.00000549  \n","Epoch: [3][2400/2470] Elapsed 10m 0s (remain 0m 17s) Loss: 0.4178(0.5056) Grad: 44745.1445  LR: 0.00000531  \n","Epoch: [3][2469/2470] Elapsed 10m 17s (remain 0m 0s) Loss: 0.4682(0.5052) Grad: 30361.4668  LR: 0.00000519  \n","EVAL: [0/135] Elapsed 0m 0s (remain 0m 56s) Loss: 0.5069(0.5069) \n","EVAL: [100/135] Elapsed 0m 13s (remain 0m 4s) Loss: 0.9678(0.5645) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5052  avg_val_loss: 0.5594  time: 636s\n","Epoch 3 - Score: 0.8236\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [134/135] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6675(0.5594) \n","Epoch: [4][0/2470] Elapsed 0m 0s (remain 26m 7s) Loss: 0.5681(0.5681) Grad: 50020.0938  LR: 0.00000518  \n","Epoch: [4][100/2470] Elapsed 0m 25s (remain 10m 1s) Loss: 0.5135(0.4962) Grad: 41830.4492  LR: 0.00000500  \n","Epoch: [4][200/2470] Elapsed 0m 50s (remain 9m 31s) Loss: 0.5219(0.4988) Grad: 13186.3408  LR: 0.00000482  \n","Epoch: [4][300/2470] Elapsed 1m 15s (remain 9m 6s) Loss: 0.4948(0.4978) Grad: 40081.8945  LR: 0.00000465  \n","Epoch: [4][400/2470] Elapsed 1m 40s (remain 8m 40s) Loss: 0.4480(0.4962) Grad: 49341.5938  LR: 0.00000447  \n","Epoch: [4][500/2470] Elapsed 2m 5s (remain 8m 14s) Loss: 0.5955(0.4979) Grad: 59177.5664  LR: 0.00000430  \n","Epoch: [4][600/2470] Elapsed 2m 30s (remain 7m 48s) Loss: 0.5543(0.4961) Grad: 486828.6250  LR: 0.00000413  \n","Epoch: [4][700/2470] Elapsed 2m 55s (remain 7m 23s) Loss: 0.4288(0.4970) Grad: 106076.9844  LR: 0.00000396  \n","Epoch: [4][800/2470] Elapsed 3m 20s (remain 6m 58s) Loss: 0.4837(0.4966) Grad: 78663.8047  LR: 0.00000379  \n","Epoch: [4][900/2470] Elapsed 3m 45s (remain 6m 33s) Loss: 0.4635(0.4960) Grad: 41345.6055  LR: 0.00000363  \n","Epoch: [4][1000/2470] Elapsed 4m 10s (remain 6m 7s) Loss: 0.4572(0.4962) Grad: 27574.7832  LR: 0.00000346  \n","Epoch: [4][1100/2470] Elapsed 4m 35s (remain 5m 42s) Loss: 0.5147(0.4961) Grad: 37842.6797  LR: 0.00000330  \n","Epoch: [4][1200/2470] Elapsed 5m 0s (remain 5m 17s) Loss: 0.4991(0.4957) Grad: 44885.2539  LR: 0.00000315  \n","Epoch: [4][1300/2470] Elapsed 5m 25s (remain 4m 52s) Loss: 0.5809(0.4954) Grad: 66018.7031  LR: 0.00000299  \n","Epoch: [4][1400/2470] Elapsed 5m 50s (remain 4m 27s) Loss: 0.6295(0.4955) Grad: 50699.7852  LR: 0.00000284  \n","Epoch: [4][1500/2470] Elapsed 6m 16s (remain 4m 2s) Loss: 0.2851(0.4962) Grad: 30274.3418  LR: 0.00000269  \n","Epoch: [4][1600/2470] Elapsed 6m 41s (remain 3m 37s) Loss: 0.6153(0.4959) Grad: 84926.2891  LR: 0.00000255  \n","Epoch: [4][1700/2470] Elapsed 7m 6s (remain 3m 12s) Loss: 0.4467(0.4957) Grad: 35462.8711  LR: 0.00000241  \n","Epoch: [4][1800/2470] Elapsed 7m 31s (remain 2m 47s) Loss: 0.3788(0.4960) Grad: 42582.4414  LR: 0.00000227  \n","Epoch: [4][1900/2470] Elapsed 7m 55s (remain 2m 22s) Loss: 0.6333(0.4960) Grad: 19031.5625  LR: 0.00000213  \n","Epoch: [4][2000/2470] Elapsed 8m 20s (remain 1m 57s) Loss: 0.5414(0.4968) Grad: 22563.4648  LR: 0.00000200  \n","Epoch: [4][2100/2470] Elapsed 8m 45s (remain 1m 32s) Loss: 0.6514(0.4968) Grad: 28676.8008  LR: 0.00000188  \n","Epoch: [4][2200/2470] Elapsed 9m 10s (remain 1m 7s) Loss: 0.4707(0.4965) Grad: 22338.5234  LR: 0.00000175  \n","Epoch: [4][2300/2470] Elapsed 9m 35s (remain 0m 42s) Loss: 0.6098(0.4967) Grad: 41930.2188  LR: 0.00000163  \n","Epoch: [4][2400/2470] Elapsed 10m 0s (remain 0m 17s) Loss: 0.4849(0.4965) Grad: 26762.3066  LR: 0.00000151  \n","Epoch: [4][2469/2470] Elapsed 10m 18s (remain 0m 0s) Loss: 0.5109(0.4965) Grad: 18011.1738  LR: 0.00000144  \n","EVAL: [0/135] Elapsed 0m 0s (remain 0m 53s) Loss: 0.5170(0.5170) \n","EVAL: [100/135] Elapsed 0m 13s (remain 0m 4s) Loss: 1.3162(0.5865) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4965  avg_val_loss: 0.5768  time: 636s\n","Epoch 4 - Score: 0.8191\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [134/135] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6693(0.5768) \n","Epoch: [5][0/2470] Elapsed 0m 0s (remain 25m 15s) Loss: 0.5424(0.5424) Grad: 38061.5195  LR: 0.00000143  \n","Epoch: [5][100/2470] Elapsed 0m 25s (remain 10m 0s) Loss: 0.4812(0.4864) Grad: 213294.9531  LR: 0.00000132  \n","Epoch: [5][200/2470] Elapsed 0m 50s (remain 9m 31s) Loss: 0.5815(0.4922) Grad: 40088.3242  LR: 0.00000122  \n","Epoch: [5][300/2470] Elapsed 1m 15s (remain 9m 5s) Loss: 0.4550(0.4891) Grad: 44334.0859  LR: 0.00000112  \n","Epoch: [5][400/2470] Elapsed 1m 40s (remain 8m 39s) Loss: 0.5227(0.4899) Grad: 813593.0000  LR: 0.00000102  \n","Epoch: [5][500/2470] Elapsed 2m 5s (remain 8m 13s) Loss: 0.5665(0.4906) Grad: 96895.7578  LR: 0.00000092  \n","Epoch: [5][600/2470] Elapsed 2m 30s (remain 7m 48s) Loss: 0.3447(0.4899) Grad: 72049.7266  LR: 0.00000083  \n","Epoch: [5][700/2470] Elapsed 2m 55s (remain 7m 23s) Loss: 0.5544(0.4890) Grad: 33284.2578  LR: 0.00000075  \n","Epoch: [5][800/2470] Elapsed 3m 20s (remain 6m 58s) Loss: 0.5511(0.4886) Grad: 30849.4824  LR: 0.00000067  \n","Epoch: [5][900/2470] Elapsed 3m 45s (remain 6m 33s) Loss: 0.4231(0.4880) Grad: 27088.2559  LR: 0.00000059  \n","Epoch: [5][1000/2470] Elapsed 4m 10s (remain 6m 8s) Loss: 0.4761(0.4896) Grad: 34893.5000  LR: 0.00000052  \n","Epoch: [5][1100/2470] Elapsed 4m 35s (remain 5m 42s) Loss: 0.5500(0.4904) Grad: 41480.7461  LR: 0.00000045  \n","Epoch: [5][1200/2470] Elapsed 5m 0s (remain 5m 17s) Loss: 0.4728(0.4904) Grad: 42176.5234  LR: 0.00000039  \n","Epoch: [5][1300/2470] Elapsed 5m 25s (remain 4m 52s) Loss: 0.5459(0.4921) Grad: 32060.8789  LR: 0.00000033  \n","Epoch: [5][1400/2470] Elapsed 5m 51s (remain 4m 27s) Loss: 0.4756(0.4929) Grad: 47077.3047  LR: 0.00000028  \n","Epoch: [5][1500/2470] Elapsed 6m 16s (remain 4m 2s) Loss: 0.4781(0.4931) Grad: 47117.1250  LR: 0.00000023  \n","Epoch: [5][1600/2470] Elapsed 6m 41s (remain 3m 37s) Loss: 0.3695(0.4925) Grad: 179723.8906  LR: 0.00000018  \n","Epoch: [5][1700/2470] Elapsed 7m 6s (remain 3m 12s) Loss: 0.5926(0.4927) Grad: 85070.1641  LR: 0.00000014  \n","Epoch: [5][1800/2470] Elapsed 7m 31s (remain 2m 47s) Loss: 0.4509(0.4933) Grad: 149738.4062  LR: 0.00000011  \n","Epoch: [5][1900/2470] Elapsed 7m 56s (remain 2m 22s) Loss: 0.4639(0.4929) Grad: 27624.7930  LR: 0.00000008  \n","Epoch: [5][2000/2470] Elapsed 8m 21s (remain 1m 57s) Loss: 0.4559(0.4928) Grad: 124244.9062  LR: 0.00000005  \n","Epoch: [5][2100/2470] Elapsed 8m 46s (remain 1m 32s) Loss: 0.4923(0.4928) Grad: 45542.3828  LR: 0.00000003  \n","Epoch: [5][2200/2470] Elapsed 9m 11s (remain 1m 7s) Loss: 0.4411(0.4927) Grad: 70128.6875  LR: 0.00000002  \n","Epoch: [5][2300/2470] Elapsed 9m 36s (remain 0m 42s) Loss: 0.5358(0.4923) Grad: 109975.8359  LR: 0.00000001  \n","Epoch: [5][2400/2470] Elapsed 10m 1s (remain 0m 17s) Loss: 0.5677(0.4922) Grad: 58615.3164  LR: 0.00000000  \n","Epoch: [5][2469/2470] Elapsed 10m 18s (remain 0m 0s) Loss: 0.4946(0.4919) Grad: 55050.8945  LR: 0.00000000  \n","EVAL: [0/135] Elapsed 0m 0s (remain 0m 53s) Loss: 0.4976(0.4976) \n","EVAL: [100/135] Elapsed 0m 13s (remain 0m 4s) Loss: 1.2479(0.5814) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4919  avg_val_loss: 0.5727  time: 636s\n","Epoch 5 - Score: 0.8189\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [134/135] Elapsed 0m 17s (remain 0m 0s) Loss: 0.6690(0.5727) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 16 result ==========\n","Score: 0.8374\n","========== fold: 17 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2486] Elapsed 0m 0s (remain 24m 47s) Loss: 0.6918(0.6918) Grad: 99147.3438  LR: 0.00001500  \n","Epoch: [1][100/2486] Elapsed 0m 25s (remain 10m 3s) Loss: 0.6276(0.6379) Grad: 93013.8125  LR: 0.00001500  \n","Epoch: [1][200/2486] Elapsed 0m 50s (remain 9m 35s) Loss: 0.5712(0.6148) Grad: 63658.9844  LR: 0.00001499  \n","Epoch: [1][300/2486] Elapsed 1m 15s (remain 9m 9s) Loss: 0.5246(0.6024) Grad: 43466.9727  LR: 0.00001498  \n","Epoch: [1][400/2486] Elapsed 1m 40s (remain 8m 43s) Loss: 0.4909(0.5952) Grad: 66977.9609  LR: 0.00001496  \n","Epoch: [1][500/2486] Elapsed 2m 5s (remain 8m 17s) Loss: 0.4771(0.5873) Grad: 80285.5703  LR: 0.00001494  \n","Epoch: [1][600/2486] Elapsed 2m 30s (remain 7m 52s) Loss: 0.5399(0.5789) Grad: 62455.7500  LR: 0.00001491  \n","Epoch: [1][700/2486] Elapsed 2m 55s (remain 7m 27s) Loss: 0.6753(0.5761) Grad: 130879.4844  LR: 0.00001488  \n","Epoch: [1][800/2486] Elapsed 3m 20s (remain 7m 1s) Loss: 0.5661(0.5731) Grad: 28369.4023  LR: 0.00001485  \n","Epoch: [1][900/2486] Elapsed 3m 45s (remain 6m 36s) Loss: 0.6212(0.5701) Grad: 48985.3125  LR: 0.00001481  \n","Epoch: [1][1000/2486] Elapsed 4m 10s (remain 6m 11s) Loss: 0.5047(0.5682) Grad: 41301.2734  LR: 0.00001476  \n","Epoch: [1][1100/2486] Elapsed 4m 35s (remain 5m 46s) Loss: 0.5923(0.5653) Grad: 44744.7031  LR: 0.00001471  \n","Epoch: [1][1200/2486] Elapsed 5m 0s (remain 5m 21s) Loss: 0.5334(0.5632) Grad: 64227.9453  LR: 0.00001466  \n","Epoch: [1][1300/2486] Elapsed 5m 25s (remain 4m 56s) Loss: 0.5784(0.5621) Grad: 38249.4180  LR: 0.00001460  \n","Epoch: [1][1400/2486] Elapsed 5m 50s (remain 4m 31s) Loss: 0.5447(0.5614) Grad: 27747.6016  LR: 0.00001453  \n","Epoch: [1][1500/2486] Elapsed 6m 15s (remain 4m 6s) Loss: 0.5736(0.5600) Grad: 139227.0625  LR: 0.00001447  \n","Epoch: [1][1600/2486] Elapsed 6m 40s (remain 3m 41s) Loss: 0.5099(0.5589) Grad: 27966.5137  LR: 0.00001439  \n","Epoch: [1][1700/2486] Elapsed 7m 5s (remain 3m 16s) Loss: 0.6197(0.5577) Grad: 115343.0938  LR: 0.00001432  \n","Epoch: [1][1800/2486] Elapsed 7m 30s (remain 2m 51s) Loss: 0.5851(0.5567) Grad: 28664.5352  LR: 0.00001424  \n","Epoch: [1][1900/2486] Elapsed 7m 55s (remain 2m 26s) Loss: 0.5315(0.5570) Grad: 45758.7109  LR: 0.00001415  \n","Epoch: [1][2000/2486] Elapsed 8m 20s (remain 2m 1s) Loss: 0.5580(0.5557) Grad: 49017.7109  LR: 0.00001406  \n","Epoch: [1][2100/2486] Elapsed 8m 45s (remain 1m 36s) Loss: 0.5625(0.5553) Grad: 61487.1680  LR: 0.00001397  \n","Epoch: [1][2200/2486] Elapsed 9m 10s (remain 1m 11s) Loss: 0.5348(0.5547) Grad: 37336.2578  LR: 0.00001387  \n","Epoch: [1][2300/2486] Elapsed 9m 35s (remain 0m 46s) Loss: 0.5271(0.5539) Grad: 46851.7539  LR: 0.00001377  \n","Epoch: [1][2400/2486] Elapsed 10m 0s (remain 0m 21s) Loss: 0.5095(0.5532) Grad: 133681.8125  LR: 0.00001366  \n","Epoch: [1][2485/2486] Elapsed 10m 22s (remain 0m 0s) Loss: 0.5017(0.5527) Grad: 44601.5820  LR: 0.00001357  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 49s) Loss: 0.7539(0.7539) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.3773(0.5520) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5527  avg_val_loss: 0.5455  time: 638s\n","Epoch 1 - Score: 0.8103\n","Epoch 1 - Save Best Score: 0.8103 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.7547(0.5455) \n","Epoch: [2][0/2486] Elapsed 0m 0s (remain 26m 39s) Loss: 0.4720(0.4720) Grad: 106329.7734  LR: 0.00001357  \n","Epoch: [2][100/2486] Elapsed 0m 26s (remain 10m 16s) Loss: 0.5596(0.5200) Grad: 29131.2129  LR: 0.00001345  \n","Epoch: [2][200/2486] Elapsed 0m 51s (remain 9m 44s) Loss: 0.5354(0.5155) Grad: 95901.2188  LR: 0.00001334  \n","Epoch: [2][300/2486] Elapsed 1m 16s (remain 9m 14s) Loss: 0.5097(0.5166) Grad: 58832.4492  LR: 0.00001322  \n","Epoch: [2][400/2486] Elapsed 1m 41s (remain 8m 47s) Loss: 0.5055(0.5169) Grad: 106389.1953  LR: 0.00001309  \n","Epoch: [2][500/2486] Elapsed 2m 6s (remain 8m 20s) Loss: 0.3515(0.5170) Grad: 57625.6953  LR: 0.00001296  \n","Epoch: [2][600/2486] Elapsed 2m 31s (remain 7m 54s) Loss: 0.4216(0.5176) Grad: 185414.7969  LR: 0.00001283  \n","Epoch: [2][700/2486] Elapsed 2m 56s (remain 7m 28s) Loss: 0.4756(0.5182) Grad: 48721.9531  LR: 0.00001270  \n","Epoch: [2][800/2486] Elapsed 3m 21s (remain 7m 3s) Loss: 0.3931(0.5189) Grad: 33378.9648  LR: 0.00001256  \n","Epoch: [2][900/2486] Elapsed 3m 46s (remain 6m 38s) Loss: 0.4713(0.5185) Grad: 101878.0625  LR: 0.00001242  \n","Epoch: [2][1000/2486] Elapsed 4m 11s (remain 6m 12s) Loss: 0.4924(0.5186) Grad: 67557.1250  LR: 0.00001227  \n","Epoch: [2][1100/2486] Elapsed 4m 36s (remain 5m 47s) Loss: 0.3576(0.5175) Grad: 125329.8438  LR: 0.00001212  \n","Epoch: [2][1200/2486] Elapsed 5m 1s (remain 5m 22s) Loss: 0.5099(0.5179) Grad: 58749.1484  LR: 0.00001197  \n","Epoch: [2][1300/2486] Elapsed 5m 26s (remain 4m 57s) Loss: 0.4696(0.5178) Grad: 53289.5586  LR: 0.00001182  \n","Epoch: [2][1400/2486] Elapsed 5m 51s (remain 4m 32s) Loss: 0.5271(0.5181) Grad: 351802.9688  LR: 0.00001166  \n","Epoch: [2][1500/2486] Elapsed 6m 16s (remain 4m 6s) Loss: 0.5694(0.5182) Grad: 49191.1602  LR: 0.00001150  \n","Epoch: [2][1600/2486] Elapsed 6m 41s (remain 3m 41s) Loss: 0.5989(0.5187) Grad: 55415.9336  LR: 0.00001134  \n","Epoch: [2][1700/2486] Elapsed 7m 6s (remain 3m 16s) Loss: 0.6234(0.5192) Grad: 84961.8594  LR: 0.00001118  \n","Epoch: [2][1800/2486] Elapsed 7m 31s (remain 2m 51s) Loss: 0.5588(0.5191) Grad: 196773.8750  LR: 0.00001101  \n","Epoch: [2][1900/2486] Elapsed 7m 56s (remain 2m 26s) Loss: 0.4502(0.5188) Grad: 86885.2812  LR: 0.00001084  \n","Epoch: [2][2000/2486] Elapsed 8m 21s (remain 2m 1s) Loss: 0.5490(0.5191) Grad: 834421.3125  LR: 0.00001067  \n","Epoch: [2][2100/2486] Elapsed 8m 46s (remain 1m 36s) Loss: 0.5672(0.5190) Grad: 119331.3516  LR: 0.00001050  \n","Epoch: [2][2200/2486] Elapsed 9m 10s (remain 1m 11s) Loss: 0.5261(0.5188) Grad: 38503.5508  LR: 0.00001033  \n","Epoch: [2][2300/2486] Elapsed 9m 35s (remain 0m 46s) Loss: 0.5516(0.5190) Grad: 136431.1094  LR: 0.00001015  \n","Epoch: [2][2400/2486] Elapsed 10m 0s (remain 0m 21s) Loss: 0.5184(0.5191) Grad: 31375.7969  LR: 0.00000997  \n","Epoch: [2][2485/2486] Elapsed 10m 21s (remain 0m 0s) Loss: 0.6511(0.5192) Grad: 92413.8906  LR: 0.00000982  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 50s) Loss: 0.7785(0.7785) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.3161(0.5579) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5192  avg_val_loss: 0.5492  time: 638s\n","Epoch 2 - Score: 0.8153\n","Epoch 2 - Save Best Score: 0.8153 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.7562(0.5492) \n","Epoch: [3][0/2486] Elapsed 0m 0s (remain 25m 29s) Loss: 0.6292(0.6292) Grad: 64886.7305  LR: 0.00000982  \n","Epoch: [3][100/2486] Elapsed 0m 25s (remain 10m 13s) Loss: 0.4317(0.5055) Grad: 172004.4375  LR: 0.00000964  \n","Epoch: [3][200/2486] Elapsed 0m 51s (remain 9m 43s) Loss: 0.5291(0.5068) Grad: 76165.1875  LR: 0.00000945  \n","Epoch: [3][300/2486] Elapsed 1m 16s (remain 9m 14s) Loss: 0.4467(0.5044) Grad: 27716.7500  LR: 0.00000927  \n","Epoch: [3][400/2486] Elapsed 1m 41s (remain 8m 47s) Loss: 0.2682(0.5052) Grad: 64232.3594  LR: 0.00000908  \n","Epoch: [3][500/2486] Elapsed 2m 6s (remain 8m 20s) Loss: 0.4797(0.5069) Grad: 46894.8477  LR: 0.00000890  \n","Epoch: [3][600/2486] Elapsed 2m 31s (remain 7m 54s) Loss: 0.5289(0.5046) Grad: 31975.2324  LR: 0.00000871  \n","Epoch: [3][700/2486] Elapsed 2m 56s (remain 7m 28s) Loss: 0.5690(0.5059) Grad: 28068.3809  LR: 0.00000853  \n","Epoch: [3][800/2486] Elapsed 3m 21s (remain 7m 3s) Loss: 0.5316(0.5064) Grad: 30723.0742  LR: 0.00000834  \n","Epoch: [3][900/2486] Elapsed 3m 46s (remain 6m 38s) Loss: 0.5595(0.5057) Grad: 161174.4219  LR: 0.00000815  \n","Epoch: [3][1000/2486] Elapsed 4m 11s (remain 6m 12s) Loss: 0.5468(0.5057) Grad: 11279.4297  LR: 0.00000796  \n","Epoch: [3][1100/2486] Elapsed 4m 36s (remain 5m 47s) Loss: 0.4175(0.5063) Grad: 23246.9395  LR: 0.00000777  \n","Epoch: [3][1200/2486] Elapsed 5m 1s (remain 5m 22s) Loss: 0.5948(0.5063) Grad: 30673.1133  LR: 0.00000758  \n","Epoch: [3][1300/2486] Elapsed 5m 26s (remain 4m 56s) Loss: 0.5178(0.5069) Grad: 30871.2070  LR: 0.00000739  \n","Epoch: [3][1400/2486] Elapsed 5m 51s (remain 4m 32s) Loss: 0.5015(0.5070) Grad: 21847.7441  LR: 0.00000720  \n","Epoch: [3][1500/2486] Elapsed 6m 16s (remain 4m 6s) Loss: 0.5112(0.5066) Grad: 19651.7539  LR: 0.00000701  \n","Epoch: [3][1600/2486] Elapsed 6m 41s (remain 3m 41s) Loss: 0.4311(0.5080) Grad: 63281.6875  LR: 0.00000682  \n","Epoch: [3][1700/2486] Elapsed 7m 6s (remain 3m 16s) Loss: 0.6084(0.5079) Grad: 62533.3516  LR: 0.00000663  \n","Epoch: [3][1800/2486] Elapsed 7m 31s (remain 2m 51s) Loss: 0.4805(0.5076) Grad: 382059.0625  LR: 0.00000645  \n","Epoch: [3][1900/2486] Elapsed 7m 56s (remain 2m 26s) Loss: 0.5883(0.5083) Grad: 23626.3262  LR: 0.00000626  \n","Epoch: [3][2000/2486] Elapsed 8m 21s (remain 2m 1s) Loss: 0.5567(0.5083) Grad: 17640.7754  LR: 0.00000607  \n","Epoch: [3][2100/2486] Elapsed 8m 46s (remain 1m 36s) Loss: 0.3707(0.5082) Grad: 24290.6328  LR: 0.00000589  \n","Epoch: [3][2200/2486] Elapsed 9m 11s (remain 1m 11s) Loss: 0.4082(0.5079) Grad: 16932.9102  LR: 0.00000570  \n","Epoch: [3][2300/2486] Elapsed 9m 36s (remain 0m 46s) Loss: 0.4126(0.5076) Grad: 18327.3164  LR: 0.00000552  \n","Epoch: [3][2400/2486] Elapsed 10m 1s (remain 0m 21s) Loss: 0.5348(0.5077) Grad: 76235.5078  LR: 0.00000534  \n","Epoch: [3][2485/2486] Elapsed 10m 22s (remain 0m 0s) Loss: 0.2280(0.5076) Grad: 23347.9609  LR: 0.00000518  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 48s) Loss: 0.7880(0.7880) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.3175(0.5735) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5076  avg_val_loss: 0.5659  time: 639s\n","Epoch 3 - Score: 0.8154\n","Epoch 3 - Save Best Score: 0.8154 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.9265(0.5659) \n","Epoch: [4][0/2486] Elapsed 0m 0s (remain 27m 28s) Loss: 0.5014(0.5014) Grad: 59648.6328  LR: 0.00000518  \n","Epoch: [4][100/2486] Elapsed 0m 25s (remain 10m 10s) Loss: 0.5330(0.4994) Grad: 14828.6846  LR: 0.00000500  \n","Epoch: [4][200/2486] Elapsed 0m 51s (remain 9m 41s) Loss: 0.5334(0.4915) Grad: 39221.5195  LR: 0.00000482  \n","Epoch: [4][300/2486] Elapsed 1m 16s (remain 9m 11s) Loss: 0.4680(0.4946) Grad: 64427.3398  LR: 0.00000465  \n","Epoch: [4][400/2486] Elapsed 1m 40s (remain 8m 44s) Loss: 0.4668(0.4971) Grad: 29028.5840  LR: 0.00000447  \n","Epoch: [4][500/2486] Elapsed 2m 5s (remain 8m 18s) Loss: 0.5281(0.4956) Grad: 33858.7383  LR: 0.00000430  \n","Epoch: [4][600/2486] Elapsed 2m 30s (remain 7m 53s) Loss: 0.5099(0.4960) Grad: 53739.9766  LR: 0.00000413  \n","Epoch: [4][700/2486] Elapsed 2m 55s (remain 7m 27s) Loss: 0.5948(0.4947) Grad: 226158.8438  LR: 0.00000396  \n","Epoch: [4][800/2486] Elapsed 3m 20s (remain 7m 2s) Loss: 0.4928(0.4947) Grad: 26861.3730  LR: 0.00000380  \n","Epoch: [4][900/2486] Elapsed 3m 45s (remain 6m 37s) Loss: 0.5295(0.4944) Grad: 45222.6055  LR: 0.00000363  \n","Epoch: [4][1000/2486] Elapsed 4m 10s (remain 6m 11s) Loss: 0.5197(0.4941) Grad: 160710.2812  LR: 0.00000347  \n","Epoch: [4][1100/2486] Elapsed 4m 35s (remain 5m 46s) Loss: 0.5190(0.4966) Grad: 84572.6250  LR: 0.00000331  \n","Epoch: [4][1200/2486] Elapsed 5m 0s (remain 5m 21s) Loss: 0.4752(0.4972) Grad: 50263.7891  LR: 0.00000316  \n","Epoch: [4][1300/2486] Elapsed 5m 25s (remain 4m 56s) Loss: 0.6118(0.4976) Grad: 57408.6953  LR: 0.00000300  \n","Epoch: [4][1400/2486] Elapsed 5m 50s (remain 4m 31s) Loss: 0.5197(0.4970) Grad: 49925.1016  LR: 0.00000285  \n","Epoch: [4][1500/2486] Elapsed 6m 15s (remain 4m 6s) Loss: 0.4575(0.4970) Grad: 114637.5391  LR: 0.00000271  \n","Epoch: [4][1600/2486] Elapsed 6m 40s (remain 3m 41s) Loss: 0.3841(0.4973) Grad: 82440.5781  LR: 0.00000256  \n","Epoch: [4][1700/2486] Elapsed 7m 5s (remain 3m 16s) Loss: 0.5242(0.4975) Grad: 134426.3281  LR: 0.00000242  \n","Epoch: [4][1800/2486] Elapsed 7m 30s (remain 2m 51s) Loss: 0.4947(0.4976) Grad: 79009.0703  LR: 0.00000228  \n","Epoch: [4][1900/2486] Elapsed 7m 55s (remain 2m 26s) Loss: 0.3866(0.4980) Grad: 34185.7773  LR: 0.00000215  \n","Epoch: [4][2000/2486] Elapsed 8m 20s (remain 2m 1s) Loss: 0.5248(0.4977) Grad: 84364.1328  LR: 0.00000202  \n","Epoch: [4][2100/2486] Elapsed 8m 45s (remain 1m 36s) Loss: 0.4708(0.4980) Grad: 122729.7578  LR: 0.00000189  \n","Epoch: [4][2200/2486] Elapsed 9m 10s (remain 1m 11s) Loss: 0.4990(0.4976) Grad: 103378.8438  LR: 0.00000177  \n","Epoch: [4][2300/2486] Elapsed 9m 35s (remain 0m 46s) Loss: 0.5288(0.4976) Grad: 61971.4180  LR: 0.00000165  \n","Epoch: [4][2400/2486] Elapsed 10m 0s (remain 0m 21s) Loss: 0.4036(0.4969) Grad: 74408.4453  LR: 0.00000153  \n","Epoch: [4][2485/2486] Elapsed 10m 21s (remain 0m 0s) Loss: 0.4593(0.4967) Grad: 67021.0156  LR: 0.00000143  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 48s) Loss: 0.8210(0.8210) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.3172(0.5716) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4967  avg_val_loss: 0.5627  time: 638s\n","Epoch 4 - Score: 0.8110\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.9433(0.5627) \n","Epoch: [5][0/2486] Elapsed 0m 0s (remain 27m 18s) Loss: 0.4575(0.4575) Grad: 43159.9102  LR: 0.00000143  \n","Epoch: [5][100/2486] Elapsed 0m 25s (remain 10m 6s) Loss: 0.4986(0.4927) Grad: 73303.0625  LR: 0.00000132  \n","Epoch: [5][200/2486] Elapsed 0m 50s (remain 9m 36s) Loss: 0.5805(0.4944) Grad: 562289.3750  LR: 0.00000122  \n","Epoch: [5][300/2486] Elapsed 1m 15s (remain 9m 9s) Loss: 0.5231(0.4940) Grad: 281184.1250  LR: 0.00000112  \n","Epoch: [5][400/2486] Elapsed 1m 40s (remain 8m 43s) Loss: 0.5053(0.4930) Grad: 32298.4492  LR: 0.00000102  \n","Epoch: [5][500/2486] Elapsed 2m 5s (remain 8m 17s) Loss: 0.6495(0.4931) Grad: 67300.7422  LR: 0.00000092  \n","Epoch: [5][600/2486] Elapsed 2m 30s (remain 7m 52s) Loss: 0.3859(0.4932) Grad: 128000.1172  LR: 0.00000084  \n","Epoch: [5][700/2486] Elapsed 2m 55s (remain 7m 27s) Loss: 0.4646(0.4924) Grad: 25297.5098  LR: 0.00000075  \n","Epoch: [5][800/2486] Elapsed 3m 20s (remain 7m 2s) Loss: 0.4329(0.4931) Grad: 17648.5957  LR: 0.00000067  \n","Epoch: [5][900/2486] Elapsed 3m 45s (remain 6m 36s) Loss: 0.3054(0.4926) Grad: 27230.7480  LR: 0.00000059  \n","Epoch: [5][1000/2486] Elapsed 4m 10s (remain 6m 11s) Loss: 0.4550(0.4925) Grad: 36718.7539  LR: 0.00000052  \n","Epoch: [5][1100/2486] Elapsed 4m 35s (remain 5m 46s) Loss: 0.4987(0.4931) Grad: 77830.6484  LR: 0.00000046  \n","Epoch: [5][1200/2486] Elapsed 5m 0s (remain 5m 21s) Loss: 0.4642(0.4932) Grad: 47486.9883  LR: 0.00000039  \n","Epoch: [5][1300/2486] Elapsed 5m 25s (remain 4m 56s) Loss: 0.5396(0.4932) Grad: 24926.7812  LR: 0.00000033  \n","Epoch: [5][1400/2486] Elapsed 5m 50s (remain 4m 31s) Loss: 0.4263(0.4933) Grad: 51127.1328  LR: 0.00000028  \n","Epoch: [5][1500/2486] Elapsed 6m 15s (remain 4m 6s) Loss: 0.4317(0.4931) Grad: 152448.6406  LR: 0.00000023  \n","Epoch: [5][1600/2486] Elapsed 6m 40s (remain 3m 41s) Loss: 0.4354(0.4931) Grad: 33693.2461  LR: 0.00000019  \n","Epoch: [5][1700/2486] Elapsed 7m 5s (remain 3m 16s) Loss: 0.5651(0.4934) Grad: 27563.2363  LR: 0.00000015  \n","Epoch: [5][1800/2486] Elapsed 7m 30s (remain 2m 51s) Loss: 0.4951(0.4924) Grad: 59343.3555  LR: 0.00000011  \n","Epoch: [5][1900/2486] Elapsed 7m 56s (remain 2m 26s) Loss: 0.5552(0.4928) Grad: 44406.8633  LR: 0.00000008  \n","Epoch: [5][2000/2486] Elapsed 8m 20s (remain 2m 1s) Loss: 0.5037(0.4929) Grad: 94109.3047  LR: 0.00000006  \n","Epoch: [5][2100/2486] Elapsed 8m 45s (remain 1m 36s) Loss: 0.4185(0.4929) Grad: 13960.6641  LR: 0.00000004  \n","Epoch: [5][2200/2486] Elapsed 9m 10s (remain 1m 11s) Loss: 0.5989(0.4928) Grad: 356435.0625  LR: 0.00000002  \n","Epoch: [5][2300/2486] Elapsed 9m 35s (remain 0m 46s) Loss: 0.4593(0.4931) Grad: 152929.2812  LR: 0.00000001  \n","Epoch: [5][2400/2486] Elapsed 10m 1s (remain 0m 21s) Loss: 0.3903(0.4926) Grad: 53675.2812  LR: 0.00000000  \n","Epoch: [5][2485/2486] Elapsed 10m 22s (remain 0m 0s) Loss: 0.5338(0.4931) Grad: 49648.1680  LR: 0.00000000  \n","EVAL: [0/119] Elapsed 0m 0s (remain 0m 51s) Loss: 0.8280(0.8280) \n","EVAL: [100/119] Elapsed 0m 13s (remain 0m 2s) Loss: 0.3177(0.5749) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4931  avg_val_loss: 0.5660  time: 638s\n","Epoch 5 - Score: 0.8111\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [118/119] Elapsed 0m 15s (remain 0m 0s) Loss: 0.9774(0.5660) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 17 result ==========\n","Score: 0.8154\n","========== fold: 18 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2476] Elapsed 0m 0s (remain 21m 13s) Loss: 0.9058(0.9058) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2476] Elapsed 0m 25s (remain 9m 59s) Loss: 0.6222(0.6689) Grad: 53980.1172  LR: 0.00001500  \n","Epoch: [1][200/2476] Elapsed 0m 50s (remain 9m 31s) Loss: 0.6523(0.6385) Grad: 66069.6953  LR: 0.00001499  \n","Epoch: [1][300/2476] Elapsed 1m 15s (remain 9m 5s) Loss: 0.6517(0.6172) Grad: 70790.9297  LR: 0.00001498  \n","Epoch: [1][400/2476] Elapsed 1m 40s (remain 8m 39s) Loss: 0.5441(0.6081) Grad: 40593.5000  LR: 0.00001496  \n","Epoch: [1][500/2476] Elapsed 2m 5s (remain 8m 14s) Loss: 0.4348(0.5988) Grad: 36156.6484  LR: 0.00001494  \n","Epoch: [1][600/2476] Elapsed 2m 30s (remain 7m 49s) Loss: 0.5000(0.5920) Grad: 62465.9844  LR: 0.00001491  \n","Epoch: [1][700/2476] Elapsed 2m 55s (remain 7m 24s) Loss: 0.5084(0.5869) Grad: 35593.6719  LR: 0.00001488  \n","Epoch: [1][800/2476] Elapsed 3m 20s (remain 6m 59s) Loss: 0.6535(0.5841) Grad: 34903.1641  LR: 0.00001485  \n","Epoch: [1][900/2476] Elapsed 3m 45s (remain 6m 33s) Loss: 0.4723(0.5803) Grad: 128751.7109  LR: 0.00001480  \n","Epoch: [1][1000/2476] Elapsed 4m 10s (remain 6m 8s) Loss: 0.6309(0.5772) Grad: 325502.2500  LR: 0.00001476  \n","Epoch: [1][1100/2476] Elapsed 4m 35s (remain 5m 43s) Loss: 0.6178(0.5751) Grad: 75796.2500  LR: 0.00001471  \n","Epoch: [1][1200/2476] Elapsed 5m 0s (remain 5m 18s) Loss: 0.5792(0.5727) Grad: 43569.7734  LR: 0.00001465  \n","Epoch: [1][1300/2476] Elapsed 5m 25s (remain 4m 53s) Loss: 0.5203(0.5700) Grad: 26638.1133  LR: 0.00001460  \n","Epoch: [1][1400/2476] Elapsed 5m 50s (remain 4m 28s) Loss: 0.4651(0.5678) Grad: 26040.0918  LR: 0.00001453  \n","Epoch: [1][1500/2476] Elapsed 6m 15s (remain 4m 3s) Loss: 0.5599(0.5665) Grad: 23756.7246  LR: 0.00001446  \n","Epoch: [1][1600/2476] Elapsed 6m 40s (remain 3m 38s) Loss: 0.5150(0.5650) Grad: 27366.1484  LR: 0.00001439  \n","Epoch: [1][1700/2476] Elapsed 7m 5s (remain 3m 13s) Loss: 0.4705(0.5641) Grad: 19908.4844  LR: 0.00001431  \n","Epoch: [1][1800/2476] Elapsed 7m 30s (remain 2m 48s) Loss: 0.5750(0.5629) Grad: 31311.7168  LR: 0.00001423  \n","Epoch: [1][1900/2476] Elapsed 7m 55s (remain 2m 23s) Loss: 0.5104(0.5626) Grad: 30173.0312  LR: 0.00001414  \n","Epoch: [1][2000/2476] Elapsed 8m 20s (remain 1m 58s) Loss: 0.5587(0.5618) Grad: 44202.0586  LR: 0.00001405  \n","Epoch: [1][2100/2476] Elapsed 8m 45s (remain 1m 33s) Loss: 0.4790(0.5607) Grad: 65549.3672  LR: 0.00001396  \n","Epoch: [1][2200/2476] Elapsed 9m 10s (remain 1m 8s) Loss: 0.5429(0.5595) Grad: 115140.3594  LR: 0.00001386  \n","Epoch: [1][2300/2476] Elapsed 9m 34s (remain 0m 43s) Loss: 0.5705(0.5584) Grad: 82544.3359  LR: 0.00001376  \n","Epoch: [1][2400/2476] Elapsed 9m 59s (remain 0m 18s) Loss: 0.5944(0.5571) Grad: 24899.6621  LR: 0.00001365  \n","Epoch: [1][2475/2476] Elapsed 10m 18s (remain 0m 0s) Loss: 0.5628(0.5563) Grad: 16721.0352  LR: 0.00001357  \n","EVAL: [0/129] Elapsed 0m 0s (remain 0m 55s) Loss: 0.8419(0.8419) \n","EVAL: [100/129] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4489(0.5562) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5563  avg_val_loss: 0.5452  time: 636s\n","Epoch 1 - Score: 0.8314\n","Epoch 1 - Save Best Score: 0.8314 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [128/129] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5790(0.5452) \n","Epoch: [2][0/2476] Elapsed 0m 0s (remain 26m 8s) Loss: 0.5462(0.5462) Grad: 74571.5234  LR: 0.00001357  \n","Epoch: [2][100/2476] Elapsed 0m 25s (remain 10m 10s) Loss: 0.4624(0.5177) Grad: 16232.0742  LR: 0.00001345  \n","Epoch: [2][200/2476] Elapsed 0m 51s (remain 9m 41s) Loss: 0.6075(0.5210) Grad: 21355.8047  LR: 0.00001334  \n","Epoch: [2][300/2476] Elapsed 1m 16s (remain 9m 11s) Loss: 0.5193(0.5215) Grad: 53535.5820  LR: 0.00001321  \n","Epoch: [2][400/2476] Elapsed 1m 41s (remain 8m 44s) Loss: 0.4902(0.5205) Grad: 20716.7012  LR: 0.00001309  \n","Epoch: [2][500/2476] Elapsed 2m 6s (remain 8m 18s) Loss: 0.4703(0.5205) Grad: 21307.0625  LR: 0.00001296  \n","Epoch: [2][600/2476] Elapsed 2m 31s (remain 7m 52s) Loss: 0.4954(0.5214) Grad: 32874.6094  LR: 0.00001283  \n","Epoch: [2][700/2476] Elapsed 2m 56s (remain 7m 26s) Loss: 0.4978(0.5219) Grad: 20028.2578  LR: 0.00001269  \n","Epoch: [2][800/2476] Elapsed 3m 21s (remain 7m 0s) Loss: 0.4087(0.5195) Grad: 14462.0176  LR: 0.00001255  \n","Epoch: [2][900/2476] Elapsed 3m 46s (remain 6m 35s) Loss: 0.5887(0.5186) Grad: 24204.3652  LR: 0.00001241  \n","Epoch: [2][1000/2476] Elapsed 4m 11s (remain 6m 10s) Loss: 0.5431(0.5185) Grad: 60113.0859  LR: 0.00001227  \n","Epoch: [2][1100/2476] Elapsed 4m 36s (remain 5m 44s) Loss: 0.4487(0.5190) Grad: 39854.6172  LR: 0.00001212  \n","Epoch: [2][1200/2476] Elapsed 5m 1s (remain 5m 19s) Loss: 0.5871(0.5186) Grad: 61224.0664  LR: 0.00001197  \n","Epoch: [2][1300/2476] Elapsed 5m 26s (remain 4m 54s) Loss: 0.5277(0.5191) Grad: 37943.0273  LR: 0.00001181  \n","Epoch: [2][1400/2476] Elapsed 5m 51s (remain 4m 29s) Loss: 0.3761(0.5193) Grad: 17382.4023  LR: 0.00001165  \n","Epoch: [2][1500/2476] Elapsed 6m 16s (remain 4m 4s) Loss: 0.5285(0.5193) Grad: 26435.2012  LR: 0.00001149  \n","Epoch: [2][1600/2476] Elapsed 6m 41s (remain 3m 39s) Loss: 0.5807(0.5195) Grad: 57492.3359  LR: 0.00001133  \n","Epoch: [2][1700/2476] Elapsed 7m 6s (remain 3m 14s) Loss: 0.5270(0.5192) Grad: 16317.5264  LR: 0.00001117  \n","Epoch: [2][1800/2476] Elapsed 7m 31s (remain 2m 49s) Loss: 0.6629(0.5190) Grad: 139518.9062  LR: 0.00001100  \n","Epoch: [2][1900/2476] Elapsed 7m 56s (remain 2m 24s) Loss: 0.5823(0.5191) Grad: 26109.3672  LR: 0.00001083  \n","Epoch: [2][2000/2476] Elapsed 8m 21s (remain 1m 59s) Loss: 0.5693(0.5188) Grad: 153047.0938  LR: 0.00001066  \n","Epoch: [2][2100/2476] Elapsed 8m 46s (remain 1m 33s) Loss: 0.5240(0.5194) Grad: 168475.9844  LR: 0.00001049  \n","Epoch: [2][2200/2476] Elapsed 9m 11s (remain 1m 8s) Loss: 0.5556(0.5196) Grad: 54861.6367  LR: 0.00001031  \n","Epoch: [2][2300/2476] Elapsed 9m 36s (remain 0m 43s) Loss: 0.5158(0.5193) Grad: 132864.8906  LR: 0.00001013  \n","Epoch: [2][2400/2476] Elapsed 10m 1s (remain 0m 18s) Loss: 0.4956(0.5194) Grad: 51640.3164  LR: 0.00000995  \n","Epoch: [2][2475/2476] Elapsed 10m 20s (remain 0m 0s) Loss: 0.3901(0.5191) Grad: 42174.7227  LR: 0.00000982  \n","EVAL: [0/129] Elapsed 0m 0s (remain 0m 50s) Loss: 0.8075(0.8075) \n","EVAL: [100/129] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4347(0.5595) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5191  avg_val_loss: 0.5462  time: 638s\n","Epoch 2 - Score: 0.8382\n","Epoch 2 - Save Best Score: 0.8382 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [128/129] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5535(0.5462) \n","Epoch: [3][0/2476] Elapsed 0m 0s (remain 25m 1s) Loss: 0.5117(0.5117) Grad: 181978.0938  LR: 0.00000982  \n","Epoch: [3][100/2476] Elapsed 0m 25s (remain 10m 9s) Loss: 0.6103(0.5103) Grad: 36593.2305  LR: 0.00000963  \n","Epoch: [3][200/2476] Elapsed 0m 51s (remain 9m 41s) Loss: 0.4390(0.5104) Grad: 56037.4531  LR: 0.00000945  \n","Epoch: [3][300/2476] Elapsed 1m 16s (remain 9m 11s) Loss: 0.4417(0.5100) Grad: 90838.2578  LR: 0.00000927  \n","Epoch: [3][400/2476] Elapsed 1m 41s (remain 8m 44s) Loss: 0.4083(0.5093) Grad: 78284.6562  LR: 0.00000908  \n","Epoch: [3][500/2476] Elapsed 2m 6s (remain 8m 18s) Loss: 0.5371(0.5070) Grad: 252790.2188  LR: 0.00000890  \n","Epoch: [3][600/2476] Elapsed 2m 31s (remain 7m 52s) Loss: 0.5021(0.5081) Grad: 41727.2695  LR: 0.00000871  \n","Epoch: [3][700/2476] Elapsed 2m 56s (remain 7m 26s) Loss: 0.4847(0.5077) Grad: 35223.8438  LR: 0.00000852  \n","Epoch: [3][800/2476] Elapsed 3m 21s (remain 7m 1s) Loss: 0.5147(0.5078) Grad: 38304.6836  LR: 0.00000833  \n","Epoch: [3][900/2476] Elapsed 3m 46s (remain 6m 35s) Loss: 0.4997(0.5073) Grad: 94552.2109  LR: 0.00000814  \n","Epoch: [3][1000/2476] Elapsed 4m 11s (remain 6m 10s) Loss: 0.4913(0.5072) Grad: 45182.4414  LR: 0.00000795  \n","Epoch: [3][1100/2476] Elapsed 4m 36s (remain 5m 45s) Loss: 0.4896(0.5062) Grad: 67023.3516  LR: 0.00000776  \n","Epoch: [3][1200/2476] Elapsed 5m 1s (remain 5m 19s) Loss: 0.4858(0.5070) Grad: 371759.9375  LR: 0.00000757  \n","Epoch: [3][1300/2476] Elapsed 5m 26s (remain 4m 54s) Loss: 0.5351(0.5062) Grad: 61329.5078  LR: 0.00000738  \n","Epoch: [3][1400/2476] Elapsed 5m 51s (remain 4m 29s) Loss: 0.4919(0.5062) Grad: 23250.3301  LR: 0.00000719  \n","Epoch: [3][1500/2476] Elapsed 6m 16s (remain 4m 4s) Loss: 0.4826(0.5063) Grad: 46567.3438  LR: 0.00000700  \n","Epoch: [3][1600/2476] Elapsed 6m 41s (remain 3m 39s) Loss: 0.4486(0.5058) Grad: 32693.4551  LR: 0.00000681  \n","Epoch: [3][1700/2476] Elapsed 7m 6s (remain 3m 14s) Loss: 0.4922(0.5058) Grad: 44522.8945  LR: 0.00000662  \n","Epoch: [3][1800/2476] Elapsed 7m 31s (remain 2m 49s) Loss: 0.5842(0.5057) Grad: 22052.1621  LR: 0.00000643  \n","Epoch: [3][1900/2476] Elapsed 7m 56s (remain 2m 24s) Loss: 0.4134(0.5060) Grad: 21520.4707  LR: 0.00000625  \n","Epoch: [3][2000/2476] Elapsed 8m 21s (remain 1m 59s) Loss: 0.6647(0.5053) Grad: 502215.0000  LR: 0.00000606  \n","Epoch: [3][2100/2476] Elapsed 8m 46s (remain 1m 33s) Loss: 0.4012(0.5053) Grad: 51032.7734  LR: 0.00000587  \n","Epoch: [3][2200/2476] Elapsed 9m 11s (remain 1m 8s) Loss: 0.3906(0.5049) Grad: 25212.1953  LR: 0.00000569  \n","Epoch: [3][2300/2476] Elapsed 9m 36s (remain 0m 43s) Loss: 0.4291(0.5046) Grad: 19372.9395  LR: 0.00000550  \n","Epoch: [3][2400/2476] Elapsed 10m 1s (remain 0m 18s) Loss: 0.4308(0.5043) Grad: 67236.7812  LR: 0.00000532  \n","Epoch: [3][2475/2476] Elapsed 10m 20s (remain 0m 0s) Loss: 0.4985(0.5046) Grad: 248922.8438  LR: 0.00000518  \n","EVAL: [0/129] Elapsed 0m 0s (remain 0m 49s) Loss: 1.0325(1.0325) \n","EVAL: [100/129] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4326(0.5848) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5046  avg_val_loss: 0.5654  time: 638s\n","Epoch 3 - Score: 0.8329\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [128/129] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5598(0.5654) \n","Epoch: [4][0/2476] Elapsed 0m 0s (remain 24m 47s) Loss: 0.5050(0.5050) Grad: 61707.5664  LR: 0.00000518  \n","Epoch: [4][100/2476] Elapsed 0m 25s (remain 10m 2s) Loss: 0.4304(0.4917) Grad: 27664.2871  LR: 0.00000500  \n","Epoch: [4][200/2476] Elapsed 0m 50s (remain 9m 33s) Loss: 0.5076(0.4949) Grad: 29370.7129  LR: 0.00000482  \n","Epoch: [4][300/2476] Elapsed 1m 15s (remain 9m 7s) Loss: 0.4764(0.4963) Grad: 41722.5820  LR: 0.00000465  \n","Epoch: [4][400/2476] Elapsed 1m 40s (remain 8m 41s) Loss: 0.5933(0.4954) Grad: 317296.5625  LR: 0.00000447  \n","Epoch: [4][500/2476] Elapsed 2m 5s (remain 8m 15s) Loss: 0.3971(0.4965) Grad: 72169.9219  LR: 0.00000430  \n","Epoch: [4][600/2476] Elapsed 2m 30s (remain 7m 50s) Loss: 0.5700(0.4966) Grad: 68723.4609  LR: 0.00000413  \n","Epoch: [4][700/2476] Elapsed 2m 55s (remain 7m 25s) Loss: 0.4364(0.4968) Grad: 32822.2617  LR: 0.00000396  \n","Epoch: [4][800/2476] Elapsed 3m 20s (remain 6m 59s) Loss: 0.4716(0.4978) Grad: 43224.9336  LR: 0.00000379  \n","Epoch: [4][900/2476] Elapsed 3m 45s (remain 6m 34s) Loss: 0.4179(0.4986) Grad: 140677.3750  LR: 0.00000363  \n","Epoch: [4][1000/2476] Elapsed 4m 10s (remain 6m 9s) Loss: 0.3749(0.4987) Grad: 81754.8750  LR: 0.00000347  \n","Epoch: [4][1100/2476] Elapsed 4m 35s (remain 5m 44s) Loss: 0.4653(0.4976) Grad: 90554.3984  LR: 0.00000331  \n","Epoch: [4][1200/2476] Elapsed 5m 0s (remain 5m 19s) Loss: 0.5314(0.4971) Grad: 93714.4219  LR: 0.00000315  \n","Epoch: [4][1300/2476] Elapsed 5m 25s (remain 4m 54s) Loss: 0.5188(0.4978) Grad: 89339.1641  LR: 0.00000300  \n","Epoch: [4][1400/2476] Elapsed 5m 51s (remain 4m 29s) Loss: 0.5097(0.4978) Grad: 38310.5625  LR: 0.00000285  \n","Epoch: [4][1500/2476] Elapsed 6m 16s (remain 4m 4s) Loss: 0.4771(0.4970) Grad: 170048.0469  LR: 0.00000270  \n","Epoch: [4][1600/2476] Elapsed 6m 41s (remain 3m 39s) Loss: 0.5737(0.4970) Grad: 20647.9004  LR: 0.00000255  \n","Epoch: [4][1700/2476] Elapsed 7m 6s (remain 3m 14s) Loss: 0.6223(0.4972) Grad: 643974.4375  LR: 0.00000241  \n","Epoch: [4][1800/2476] Elapsed 7m 31s (remain 2m 49s) Loss: 0.5664(0.4977) Grad: 43917.7383  LR: 0.00000227  \n","Epoch: [4][1900/2476] Elapsed 7m 56s (remain 2m 24s) Loss: 0.4482(0.4970) Grad: 62244.8438  LR: 0.00000214  \n","Epoch: [4][2000/2476] Elapsed 8m 21s (remain 1m 58s) Loss: 0.4228(0.4966) Grad: 600760.2500  LR: 0.00000201  \n","Epoch: [4][2100/2476] Elapsed 8m 46s (remain 1m 33s) Loss: 0.5623(0.4968) Grad: 549159.1875  LR: 0.00000188  \n","Epoch: [4][2200/2476] Elapsed 9m 11s (remain 1m 8s) Loss: 0.6129(0.4962) Grad: 92702.6797  LR: 0.00000176  \n","Epoch: [4][2300/2476] Elapsed 9m 36s (remain 0m 43s) Loss: 0.5118(0.4964) Grad: 31491.2578  LR: 0.00000163  \n","Epoch: [4][2400/2476] Elapsed 10m 1s (remain 0m 18s) Loss: 0.3248(0.4967) Grad: 64127.1758  LR: 0.00000152  \n","Epoch: [4][2475/2476] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5538(0.4968) Grad: 70973.5625  LR: 0.00000143  \n","EVAL: [0/129] Elapsed 0m 0s (remain 0m 50s) Loss: 0.9502(0.9502) \n","EVAL: [100/129] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4366(0.5878) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4968  avg_val_loss: 0.5682  time: 637s\n","Epoch 4 - Score: 0.8343\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [128/129] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5558(0.5682) \n","Epoch: [5][0/2476] Elapsed 0m 0s (remain 24m 44s) Loss: 0.5186(0.5186) Grad: 68690.3750  LR: 0.00000143  \n","Epoch: [5][100/2476] Elapsed 0m 25s (remain 10m 3s) Loss: 0.5064(0.5020) Grad: 41530.6875  LR: 0.00000132  \n","Epoch: [5][200/2476] Elapsed 0m 50s (remain 9m 34s) Loss: 0.4635(0.4988) Grad: 49301.6445  LR: 0.00000122  \n","Epoch: [5][300/2476] Elapsed 1m 15s (remain 9m 7s) Loss: 0.5107(0.5000) Grad: 414802.5938  LR: 0.00000111  \n","Epoch: [5][400/2476] Elapsed 1m 40s (remain 8m 41s) Loss: 0.5438(0.4980) Grad: 94422.7422  LR: 0.00000102  \n","Epoch: [5][500/2476] Elapsed 2m 5s (remain 8m 15s) Loss: 0.5696(0.4999) Grad: 42228.2695  LR: 0.00000092  \n","Epoch: [5][600/2476] Elapsed 2m 30s (remain 7m 50s) Loss: 0.4466(0.4984) Grad: 48444.6094  LR: 0.00000083  \n","Epoch: [5][700/2476] Elapsed 2m 55s (remain 7m 25s) Loss: 0.4728(0.4973) Grad: 28250.5332  LR: 0.00000075  \n","Epoch: [5][800/2476] Elapsed 3m 20s (remain 6m 59s) Loss: 0.4686(0.4968) Grad: 40192.1719  LR: 0.00000067  \n","Epoch: [5][900/2476] Elapsed 3m 45s (remain 6m 34s) Loss: 0.5353(0.4938) Grad: 116024.9531  LR: 0.00000059  \n","Epoch: [5][1000/2476] Elapsed 4m 11s (remain 6m 9s) Loss: 0.4539(0.4934) Grad: 30016.0645  LR: 0.00000052  \n","Epoch: [5][1100/2476] Elapsed 4m 36s (remain 5m 44s) Loss: 0.4092(0.4934) Grad: 134415.6719  LR: 0.00000045  \n","Epoch: [5][1200/2476] Elapsed 5m 1s (remain 5m 19s) Loss: 0.3611(0.4926) Grad: 14671.7158  LR: 0.00000039  \n","Epoch: [5][1300/2476] Elapsed 5m 26s (remain 4m 54s) Loss: 0.3839(0.4919) Grad: 36597.8359  LR: 0.00000033  \n","Epoch: [5][1400/2476] Elapsed 5m 51s (remain 4m 29s) Loss: 0.5651(0.4917) Grad: 37837.9688  LR: 0.00000028  \n","Epoch: [5][1500/2476] Elapsed 6m 16s (remain 4m 4s) Loss: 0.5128(0.4913) Grad: 25030.8359  LR: 0.00000023  \n","Epoch: [5][1600/2476] Elapsed 6m 41s (remain 3m 39s) Loss: 0.5532(0.4914) Grad: 47901.0117  LR: 0.00000018  \n","Epoch: [5][1700/2476] Elapsed 7m 6s (remain 3m 14s) Loss: 0.5011(0.4912) Grad: 43770.7070  LR: 0.00000014  \n","Epoch: [5][1800/2476] Elapsed 7m 31s (remain 2m 49s) Loss: 0.5406(0.4915) Grad: 41663.3516  LR: 0.00000011  \n","Epoch: [5][1900/2476] Elapsed 7m 56s (remain 2m 24s) Loss: 0.5883(0.4923) Grad: 52423.4141  LR: 0.00000008  \n","Epoch: [5][2000/2476] Elapsed 8m 21s (remain 1m 59s) Loss: 0.5148(0.4929) Grad: 127877.5625  LR: 0.00000005  \n","Epoch: [5][2100/2476] Elapsed 8m 46s (remain 1m 34s) Loss: 0.4851(0.4929) Grad: 40286.9062  LR: 0.00000003  \n","Epoch: [5][2200/2476] Elapsed 9m 11s (remain 1m 8s) Loss: 0.5329(0.4928) Grad: 22311.1504  LR: 0.00000002  \n","Epoch: [5][2300/2476] Elapsed 9m 36s (remain 0m 43s) Loss: 0.4887(0.4930) Grad: 32807.2344  LR: 0.00000001  \n","Epoch: [5][2400/2476] Elapsed 10m 1s (remain 0m 18s) Loss: 0.5006(0.4929) Grad: 56210.3359  LR: 0.00000000  \n","Epoch: [5][2475/2476] Elapsed 10m 20s (remain 0m 0s) Loss: 0.4307(0.4926) Grad: 88986.1953  LR: 0.00000000  \n","EVAL: [0/129] Elapsed 0m 0s (remain 0m 51s) Loss: 0.9315(0.9315) \n","EVAL: [100/129] Elapsed 0m 13s (remain 0m 3s) Loss: 0.4467(0.5857) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4926  avg_val_loss: 0.5665  time: 638s\n","Epoch 5 - Score: 0.8361\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [128/129] Elapsed 0m 17s (remain 0m 0s) Loss: 0.5578(0.5665) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 18 result ==========\n","Score: 0.8382\n","========== fold: 19 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2480] Elapsed 0m 0s (remain 22m 32s) Loss: 0.7313(0.7313) Grad: inf  LR: 0.00001500  \n","Epoch: [1][100/2480] Elapsed 0m 25s (remain 10m 4s) Loss: 0.5169(0.6435) Grad: 80351.7031  LR: 0.00001500  \n","Epoch: [1][200/2480] Elapsed 0m 50s (remain 9m 33s) Loss: 0.6552(0.6231) Grad: 46679.7031  LR: 0.00001499  \n","Epoch: [1][300/2480] Elapsed 1m 15s (remain 9m 7s) Loss: 0.5356(0.6139) Grad: 51543.6914  LR: 0.00001498  \n","Epoch: [1][400/2480] Elapsed 1m 40s (remain 8m 41s) Loss: 0.5883(0.6000) Grad: 199068.7812  LR: 0.00001496  \n","Epoch: [1][500/2480] Elapsed 2m 5s (remain 8m 15s) Loss: 0.5219(0.5910) Grad: 45053.2734  LR: 0.00001494  \n","Epoch: [1][600/2480] Elapsed 2m 30s (remain 7m 50s) Loss: 0.6469(0.5836) Grad: 24845.2949  LR: 0.00001491  \n","Epoch: [1][700/2480] Elapsed 2m 55s (remain 7m 25s) Loss: 0.6828(0.5791) Grad: 50869.3203  LR: 0.00001488  \n","Epoch: [1][800/2480] Elapsed 3m 20s (remain 7m 0s) Loss: 0.5222(0.5754) Grad: 28301.0137  LR: 0.00001485  \n","Epoch: [1][900/2480] Elapsed 3m 45s (remain 6m 35s) Loss: 0.5762(0.5725) Grad: 30266.8496  LR: 0.00001481  \n","Epoch: [1][1000/2480] Elapsed 4m 10s (remain 6m 10s) Loss: 0.5103(0.5718) Grad: 46871.0312  LR: 0.00001476  \n","Epoch: [1][1100/2480] Elapsed 4m 35s (remain 5m 44s) Loss: 0.5053(0.5697) Grad: 18416.3652  LR: 0.00001471  \n","Epoch: [1][1200/2480] Elapsed 5m 0s (remain 5m 19s) Loss: 0.4248(0.5677) Grad: 29242.8320  LR: 0.00001466  \n","Epoch: [1][1300/2480] Elapsed 5m 25s (remain 4m 55s) Loss: 0.6214(0.5656) Grad: 52067.8164  LR: 0.00001460  \n","Epoch: [1][1400/2480] Elapsed 5m 50s (remain 4m 30s) Loss: 0.6455(0.5643) Grad: 48629.2188  LR: 0.00001453  \n","Epoch: [1][1500/2480] Elapsed 6m 15s (remain 4m 5s) Loss: 0.5958(0.5618) Grad: 27844.9609  LR: 0.00001446  \n","Epoch: [1][1600/2480] Elapsed 6m 40s (remain 3m 40s) Loss: 0.5212(0.5609) Grad: 27117.2344  LR: 0.00001439  \n","Epoch: [1][1700/2480] Elapsed 7m 5s (remain 3m 14s) Loss: 0.4700(0.5606) Grad: 21346.4023  LR: 0.00001431  \n","Epoch: [1][1800/2480] Elapsed 7m 30s (remain 2m 49s) Loss: 0.5320(0.5604) Grad: 20198.4277  LR: 0.00001423  \n","Epoch: [1][1900/2480] Elapsed 7m 55s (remain 2m 24s) Loss: 0.7317(0.5591) Grad: 70908.0781  LR: 0.00001415  \n","Epoch: [1][2000/2480] Elapsed 8m 20s (remain 1m 59s) Loss: 0.4618(0.5583) Grad: 18969.2539  LR: 0.00001406  \n","Epoch: [1][2100/2480] Elapsed 8m 45s (remain 1m 34s) Loss: 0.5186(0.5570) Grad: 48532.3711  LR: 0.00001396  \n","Epoch: [1][2200/2480] Elapsed 9m 10s (remain 1m 9s) Loss: 0.4495(0.5560) Grad: 28309.5391  LR: 0.00001386  \n","Epoch: [1][2300/2480] Elapsed 9m 35s (remain 0m 44s) Loss: 0.5489(0.5549) Grad: 21411.0254  LR: 0.00001376  \n","Epoch: [1][2400/2480] Elapsed 10m 0s (remain 0m 19s) Loss: 0.6257(0.5538) Grad: 149031.1406  LR: 0.00001365  \n","Epoch: [1][2479/2480] Elapsed 10m 20s (remain 0m 0s) Loss: 0.5538(0.5538) Grad: 94793.0938  LR: 0.00001357  \n","EVAL: [0/125] Elapsed 0m 0s (remain 0m 52s) Loss: 0.6377(0.6377) \n","EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5216(0.5527) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5538  avg_val_loss: 0.5513  time: 637s\n","Epoch 1 - Score: 0.8065\n","Epoch 1 - Save Best Score: 0.8065 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [124/125] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4557(0.5513) \n","Epoch: [2][0/2480] Elapsed 0m 0s (remain 26m 36s) Loss: 0.4905(0.4905) Grad: 101505.5234  LR: 0.00001357  \n","Epoch: [2][100/2480] Elapsed 0m 25s (remain 10m 11s) Loss: 0.5653(0.5184) Grad: 82499.9922  LR: 0.00001345  \n","Epoch: [2][200/2480] Elapsed 0m 51s (remain 9m 42s) Loss: 0.4629(0.5188) Grad: 25060.7148  LR: 0.00001334  \n","Epoch: [2][300/2480] Elapsed 1m 16s (remain 9m 13s) Loss: 0.5611(0.5163) Grad: 105277.4922  LR: 0.00001321  \n","Epoch: [2][400/2480] Elapsed 1m 41s (remain 8m 46s) Loss: 0.5325(0.5163) Grad: 164601.4219  LR: 0.00001309  \n","Epoch: [2][500/2480] Elapsed 2m 6s (remain 8m 20s) Loss: 0.5031(0.5167) Grad: 256304.4844  LR: 0.00001296  \n","Epoch: [2][600/2480] Elapsed 2m 31s (remain 7m 54s) Loss: 0.3973(0.5158) Grad: 59261.6992  LR: 0.00001283  \n","Epoch: [2][700/2480] Elapsed 2m 56s (remain 7m 28s) Loss: 0.5307(0.5172) Grad: 130646.6250  LR: 0.00001269  \n","Epoch: [2][800/2480] Elapsed 3m 21s (remain 7m 3s) Loss: 0.5864(0.5163) Grad: 47173.8281  LR: 0.00001255  \n","Epoch: [2][900/2480] Elapsed 3m 46s (remain 6m 37s) Loss: 0.5095(0.5172) Grad: 45758.7305  LR: 0.00001241  \n","Epoch: [2][1000/2480] Elapsed 4m 11s (remain 6m 12s) Loss: 0.3852(0.5169) Grad: 53460.7539  LR: 0.00001227  \n","Epoch: [2][1100/2480] Elapsed 4m 37s (remain 5m 46s) Loss: 0.5069(0.5162) Grad: 62885.2227  LR: 0.00001212  \n","Epoch: [2][1200/2480] Elapsed 5m 2s (remain 5m 21s) Loss: 0.6260(0.5158) Grad: 166987.6094  LR: 0.00001197  \n","Epoch: [2][1300/2480] Elapsed 5m 27s (remain 4m 56s) Loss: 0.4001(0.5148) Grad: 56667.8984  LR: 0.00001181  \n","Epoch: [2][1400/2480] Elapsed 5m 52s (remain 4m 31s) Loss: 0.6184(0.5153) Grad: 34977.0156  LR: 0.00001166  \n","Epoch: [2][1500/2480] Elapsed 6m 17s (remain 4m 6s) Loss: 0.3228(0.5150) Grad: 12740.3809  LR: 0.00001150  \n","Epoch: [2][1600/2480] Elapsed 6m 42s (remain 3m 40s) Loss: 0.5109(0.5157) Grad: 29643.7812  LR: 0.00001134  \n","Epoch: [2][1700/2480] Elapsed 7m 7s (remain 3m 15s) Loss: 0.4895(0.5162) Grad: 26768.4609  LR: 0.00001117  \n","Epoch: [2][1800/2480] Elapsed 7m 32s (remain 2m 50s) Loss: 0.4110(0.5164) Grad: 129581.5859  LR: 0.00001101  \n","Epoch: [2][1900/2480] Elapsed 7m 57s (remain 2m 25s) Loss: 0.5463(0.5163) Grad: 18137.9297  LR: 0.00001084  \n","Epoch: [2][2000/2480] Elapsed 8m 22s (remain 2m 0s) Loss: 0.5353(0.5163) Grad: 30522.2305  LR: 0.00001066  \n","Epoch: [2][2100/2480] Elapsed 8m 47s (remain 1m 35s) Loss: 0.4597(0.5163) Grad: 89040.5234  LR: 0.00001049  \n","Epoch: [2][2200/2480] Elapsed 9m 12s (remain 1m 10s) Loss: 0.5155(0.5165) Grad: 21052.2871  LR: 0.00001032  \n","Epoch: [2][2300/2480] Elapsed 9m 37s (remain 0m 44s) Loss: 0.5188(0.5163) Grad: 17797.0723  LR: 0.00001014  \n","Epoch: [2][2400/2480] Elapsed 10m 2s (remain 0m 19s) Loss: 0.4462(0.5164) Grad: 122552.6641  LR: 0.00000996  \n","Epoch: [2][2479/2480] Elapsed 10m 22s (remain 0m 0s) Loss: 0.5989(0.5168) Grad: 15007.8184  LR: 0.00000982  \n","EVAL: [0/125] Elapsed 0m 0s (remain 0m 50s) Loss: 0.6560(0.6560) \n","EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.5745(0.5692) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5168  avg_val_loss: 0.5692  time: 639s\n","Epoch 2 - Score: 0.8063\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [124/125] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4614(0.5692) \n","Epoch: [3][0/2480] Elapsed 0m 0s (remain 25m 15s) Loss: 0.5482(0.5482) Grad: 195657.4062  LR: 0.00000982  \n","Epoch: [3][100/2480] Elapsed 0m 25s (remain 10m 4s) Loss: 0.5789(0.5074) Grad: 70704.4375  LR: 0.00000964  \n","Epoch: [3][200/2480] Elapsed 0m 50s (remain 9m 35s) Loss: 0.5929(0.5040) Grad: 104121.6094  LR: 0.00000945  \n","Epoch: [3][300/2480] Elapsed 1m 15s (remain 9m 9s) Loss: 0.4939(0.5007) Grad: 36570.7812  LR: 0.00000927  \n","Epoch: [3][400/2480] Elapsed 1m 40s (remain 8m 43s) Loss: 0.5220(0.5001) Grad: 28764.1914  LR: 0.00000908  \n","Epoch: [3][500/2480] Elapsed 2m 6s (remain 8m 17s) Loss: 0.4131(0.5034) Grad: 50486.6641  LR: 0.00000890  \n","Epoch: [3][600/2480] Elapsed 2m 31s (remain 7m 52s) Loss: 0.3981(0.5039) Grad: 26759.4551  LR: 0.00000871  \n","Epoch: [3][700/2480] Elapsed 2m 56s (remain 7m 27s) Loss: 0.4031(0.5050) Grad: 158926.3594  LR: 0.00000852  \n","Epoch: [3][800/2480] Elapsed 3m 21s (remain 7m 2s) Loss: 0.5212(0.5057) Grad: 58078.2656  LR: 0.00000833  \n","Epoch: [3][900/2480] Elapsed 3m 46s (remain 6m 36s) Loss: 0.5201(0.5069) Grad: 46165.6523  LR: 0.00000814  \n","Epoch: [3][1000/2480] Elapsed 4m 11s (remain 6m 11s) Loss: 0.5995(0.5071) Grad: 115784.1641  LR: 0.00000795  \n","Epoch: [3][1100/2480] Elapsed 4m 36s (remain 5m 46s) Loss: 0.5090(0.5074) Grad: 25885.9629  LR: 0.00000776  \n","Epoch: [3][1200/2480] Elapsed 5m 1s (remain 5m 20s) Loss: 0.5102(0.5059) Grad: 23877.9609  LR: 0.00000758  \n","Epoch: [3][1300/2480] Elapsed 5m 26s (remain 4m 55s) Loss: 0.5374(0.5057) Grad: 39241.7734  LR: 0.00000739  \n","Epoch: [3][1400/2480] Elapsed 5m 51s (remain 4m 30s) Loss: 0.5952(0.5072) Grad: 15406.8350  LR: 0.00000720  \n","Epoch: [3][1500/2480] Elapsed 6m 16s (remain 4m 5s) Loss: 0.4923(0.5073) Grad: 19542.0000  LR: 0.00000701  \n","Epoch: [3][1600/2480] Elapsed 6m 41s (remain 3m 40s) Loss: 0.4697(0.5074) Grad: 18474.2656  LR: 0.00000682  \n","Epoch: [3][1700/2480] Elapsed 7m 6s (remain 3m 15s) Loss: 0.4682(0.5071) Grad: 15445.4170  LR: 0.00000663  \n","Epoch: [3][1800/2480] Elapsed 7m 31s (remain 2m 50s) Loss: 0.5362(0.5070) Grad: 23695.1699  LR: 0.00000644  \n","Epoch: [3][1900/2480] Elapsed 7m 56s (remain 2m 25s) Loss: 0.5130(0.5073) Grad: 38462.3438  LR: 0.00000625  \n","Epoch: [3][2000/2480] Elapsed 8m 21s (remain 2m 0s) Loss: 0.4859(0.5067) Grad: 70622.8359  LR: 0.00000606  \n","Epoch: [3][2100/2480] Elapsed 8m 46s (remain 1m 35s) Loss: 0.3788(0.5063) Grad: 28119.0664  LR: 0.00000588  \n","Epoch: [3][2200/2480] Elapsed 9m 11s (remain 1m 9s) Loss: 0.5905(0.5063) Grad: 28112.9160  LR: 0.00000569  \n","Epoch: [3][2300/2480] Elapsed 9m 36s (remain 0m 44s) Loss: 0.5095(0.5060) Grad: 113175.9219  LR: 0.00000551  \n","Epoch: [3][2400/2480] Elapsed 10m 1s (remain 0m 19s) Loss: 0.5694(0.5064) Grad: 50323.9258  LR: 0.00000533  \n","Epoch: [3][2479/2480] Elapsed 10m 21s (remain 0m 0s) Loss: 0.5287(0.5061) Grad: 33847.6445  LR: 0.00000518  \n","EVAL: [0/125] Elapsed 0m 0s (remain 0m 48s) Loss: 0.6433(0.6433) \n","EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6077(0.5754) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5061  avg_val_loss: 0.5774  time: 638s\n","Epoch 3 - Score: 0.8051\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [124/125] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4579(0.5774) \n","Epoch: [4][0/2480] Elapsed 0m 0s (remain 25m 18s) Loss: 0.4852(0.4852) Grad: 32927.2930  LR: 0.00000518  \n","Epoch: [4][100/2480] Elapsed 0m 25s (remain 10m 2s) Loss: 0.4421(0.4929) Grad: 46065.3398  LR: 0.00000500  \n","Epoch: [4][200/2480] Elapsed 0m 50s (remain 9m 33s) Loss: 0.6475(0.4980) Grad: 73200.1641  LR: 0.00000482  \n","Epoch: [4][300/2480] Elapsed 1m 15s (remain 9m 7s) Loss: 0.4814(0.4966) Grad: 22305.1055  LR: 0.00000465  \n","Epoch: [4][400/2480] Elapsed 1m 40s (remain 8m 41s) Loss: 0.4669(0.4973) Grad: 28297.2246  LR: 0.00000447  \n","Epoch: [4][500/2480] Elapsed 2m 5s (remain 8m 15s) Loss: 0.4982(0.4953) Grad: 36989.7812  LR: 0.00000430  \n","Epoch: [4][600/2480] Elapsed 2m 30s (remain 7m 50s) Loss: 0.4245(0.4949) Grad: 34548.7969  LR: 0.00000413  \n","Epoch: [4][700/2480] Elapsed 2m 55s (remain 7m 25s) Loss: 0.4960(0.4975) Grad: 417565.0625  LR: 0.00000396  \n","Epoch: [4][800/2480] Elapsed 3m 20s (remain 7m 0s) Loss: 0.4976(0.4964) Grad: 29663.2422  LR: 0.00000379  \n","Epoch: [4][900/2480] Elapsed 3m 45s (remain 6m 35s) Loss: 0.6355(0.4962) Grad: 217199.0469  LR: 0.00000363  \n","Epoch: [4][1000/2480] Elapsed 4m 10s (remain 6m 10s) Loss: 0.4864(0.4958) Grad: 28496.5977  LR: 0.00000347  \n","Epoch: [4][1100/2480] Elapsed 4m 35s (remain 5m 45s) Loss: 0.3690(0.4961) Grad: 38303.5039  LR: 0.00000331  \n","Epoch: [4][1200/2480] Elapsed 5m 0s (remain 5m 20s) Loss: 0.4254(0.4966) Grad: 19661.5332  LR: 0.00000315  \n","Epoch: [4][1300/2480] Elapsed 5m 25s (remain 4m 55s) Loss: 0.4885(0.4969) Grad: 40794.4531  LR: 0.00000300  \n","Epoch: [4][1400/2480] Elapsed 5m 50s (remain 4m 30s) Loss: 0.4497(0.4977) Grad: 102561.8594  LR: 0.00000285  \n","Epoch: [4][1500/2480] Elapsed 6m 15s (remain 4m 5s) Loss: 0.5067(0.4983) Grad: 55684.1641  LR: 0.00000270  \n","Epoch: [4][1600/2480] Elapsed 6m 41s (remain 3m 40s) Loss: 0.4391(0.4979) Grad: 49913.9961  LR: 0.00000256  \n","Epoch: [4][1700/2480] Elapsed 7m 6s (remain 3m 15s) Loss: 0.6124(0.4980) Grad: 301421.9688  LR: 0.00000242  \n","Epoch: [4][1800/2480] Elapsed 7m 31s (remain 2m 50s) Loss: 0.4617(0.4970) Grad: 34806.3047  LR: 0.00000228  \n","Epoch: [4][1900/2480] Elapsed 7m 55s (remain 2m 24s) Loss: 0.4337(0.4966) Grad: 59196.3125  LR: 0.00000214  \n","Epoch: [4][2000/2480] Elapsed 8m 20s (remain 1m 59s) Loss: 0.5949(0.4962) Grad: 169840.2969  LR: 0.00000201  \n","Epoch: [4][2100/2480] Elapsed 8m 46s (remain 1m 34s) Loss: 0.5504(0.4963) Grad: 44940.1680  LR: 0.00000188  \n","Epoch: [4][2200/2480] Elapsed 9m 11s (remain 1m 9s) Loss: 0.5658(0.4961) Grad: 589121.0000  LR: 0.00000176  \n","Epoch: [4][2300/2480] Elapsed 9m 36s (remain 0m 44s) Loss: 0.4127(0.4958) Grad: 67703.9844  LR: 0.00000164  \n","Epoch: [4][2400/2480] Elapsed 10m 1s (remain 0m 19s) Loss: 0.5186(0.4957) Grad: 115681.9766  LR: 0.00000152  \n","Epoch: [4][2479/2480] Elapsed 10m 20s (remain 0m 0s) Loss: 0.5146(0.4956) Grad: 107079.1562  LR: 0.00000143  \n","EVAL: [0/125] Elapsed 0m 0s (remain 0m 52s) Loss: 0.6320(0.6320) \n","EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6509(0.5821) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4956  avg_val_loss: 0.5814  time: 638s\n","Epoch 4 - Score: 0.8126\n","Epoch 4 - Save Best Score: 0.8126 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [124/125] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4511(0.5814) \n","Epoch: [5][0/2480] Elapsed 0m 0s (remain 24m 39s) Loss: 0.5144(0.5144) Grad: 48858.9688  LR: 0.00000143  \n","Epoch: [5][100/2480] Elapsed 0m 25s (remain 10m 10s) Loss: 0.5909(0.4961) Grad: 97921.6953  LR: 0.00000132  \n","Epoch: [5][200/2480] Elapsed 0m 51s (remain 9m 40s) Loss: 0.5396(0.5006) Grad: 50680.9648  LR: 0.00000122  \n","Epoch: [5][300/2480] Elapsed 1m 16s (remain 9m 11s) Loss: 0.5411(0.4960) Grad: 52991.1289  LR: 0.00000111  \n","Epoch: [5][400/2480] Elapsed 1m 41s (remain 8m 44s) Loss: 0.4296(0.4955) Grad: 22617.1543  LR: 0.00000102  \n","Epoch: [5][500/2480] Elapsed 2m 6s (remain 8m 17s) Loss: 0.4509(0.4947) Grad: 118083.4766  LR: 0.00000092  \n","Epoch: [5][600/2480] Elapsed 2m 30s (remain 7m 51s) Loss: 0.4387(0.4941) Grad: 54819.5664  LR: 0.00000083  \n","Epoch: [5][700/2480] Elapsed 2m 55s (remain 7m 26s) Loss: 0.4640(0.4933) Grad: 16618.1387  LR: 0.00000075  \n","Epoch: [5][800/2480] Elapsed 3m 20s (remain 7m 0s) Loss: 0.5716(0.4950) Grad: 5522.4922  LR: 0.00000067  \n","Epoch: [5][900/2480] Elapsed 3m 45s (remain 6m 35s) Loss: 0.4362(0.4943) Grad: 15763.3877  LR: 0.00000059  \n","Epoch: [5][1000/2480] Elapsed 4m 10s (remain 6m 10s) Loss: 0.4636(0.4944) Grad: 6000.9473  LR: 0.00000052  \n","Epoch: [5][1100/2480] Elapsed 4m 35s (remain 5m 45s) Loss: 0.5929(0.4949) Grad: 27028.6602  LR: 0.00000045  \n","Epoch: [5][1200/2480] Elapsed 5m 0s (remain 5m 20s) Loss: 0.5988(0.4945) Grad: 9597.5449  LR: 0.00000039  \n","Epoch: [5][1300/2480] Elapsed 5m 25s (remain 4m 54s) Loss: 0.5552(0.4934) Grad: 7410.6582  LR: 0.00000033  \n","Epoch: [5][1400/2480] Elapsed 5m 50s (remain 4m 30s) Loss: 0.3923(0.4934) Grad: 3705.5793  LR: 0.00000028  \n","Epoch: [5][1500/2480] Elapsed 6m 15s (remain 4m 4s) Loss: 0.5303(0.4931) Grad: 7916.1494  LR: 0.00000023  \n","Epoch: [5][1600/2480] Elapsed 6m 40s (remain 3m 39s) Loss: 0.6089(0.4926) Grad: 35348.6914  LR: 0.00000019  \n","Epoch: [5][1700/2480] Elapsed 7m 5s (remain 3m 14s) Loss: 0.5402(0.4921) Grad: 10459.2715  LR: 0.00000015  \n","Epoch: [5][1800/2480] Elapsed 7m 30s (remain 2m 49s) Loss: 0.4798(0.4923) Grad: 9862.9307  LR: 0.00000011  \n","Epoch: [5][1900/2480] Elapsed 7m 55s (remain 2m 24s) Loss: 0.4534(0.4926) Grad: 9043.1289  LR: 0.00000008  \n","Epoch: [5][2000/2480] Elapsed 8m 20s (remain 1m 59s) Loss: 0.5207(0.4927) Grad: 8579.1738  LR: 0.00000006  \n","Epoch: [5][2100/2480] Elapsed 8m 45s (remain 1m 34s) Loss: 0.4311(0.4926) Grad: 17095.0996  LR: 0.00000003  \n","Epoch: [5][2200/2480] Elapsed 9m 10s (remain 1m 9s) Loss: 0.4128(0.4924) Grad: 6320.9365  LR: 0.00000002  \n","Epoch: [5][2300/2480] Elapsed 9m 35s (remain 0m 44s) Loss: 0.4572(0.4916) Grad: 19468.8574  LR: 0.00000001  \n","Epoch: [5][2400/2480] Elapsed 10m 0s (remain 0m 19s) Loss: 0.5384(0.4917) Grad: 106536.1484  LR: 0.00000000  \n","Epoch: [5][2479/2480] Elapsed 10m 19s (remain 0m 0s) Loss: 0.5753(0.4920) Grad: 113000.2031  LR: 0.00000000  \n","EVAL: [0/125] Elapsed 0m 0s (remain 0m 44s) Loss: 0.6218(0.6218) \n","EVAL: [100/125] Elapsed 0m 13s (remain 0m 3s) Loss: 0.6885(0.5933) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.4920  avg_val_loss: 0.5928  time: 636s\n","Epoch 5 - Score: 0.8104\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [124/125] Elapsed 0m 16s (remain 0m 0s) Loss: 0.4479(0.5928) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 19 result ==========\n","Score: 0.8126\n","========== CV ==========\n","Score: 0.8307\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e4390436ba34be8be1ad9e788d7fb09","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003e[fold0] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] avg_val_loss\u003c/td\u003e\u003ctd\u003e▂▁▄▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] loss\u003c/td\u003e\u003ctd\u003e▇▂▂▂▇▆█▄▅▂▁▃▄▄▄▂▃▄▅▃▄▂▆▇▃▃▁▃▃▇▄▂▂█▂▅▃▃▄▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] score\u003c/td\u003e\u003ctd\u003e▁▃█▇▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▃▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] avg_val_loss\u003c/td\u003e\u003ctd\u003e▅▁▅██\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] loss\u003c/td\u003e\u003ctd\u003e▅▅▅▇▅▆▄▄▇▅▅▄▇▆▂▅▅▃▂▅▆▅▄█▅▅▂▅█▄▃▆▅█▂▁▃▅▅▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] score\u003c/td\u003e\u003ctd\u003e▁▆▇██\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▃▂▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] avg_val_loss\u003c/td\u003e\u003ctd\u003e▂▁▇▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] loss\u003c/td\u003e\u003ctd\u003e▅▃▇▆▄█▂▄▃▅▅▃▄▄▃▄▄▃▅▂▃▄▂▃▂▂▄▅▆▂▃▁▅▂▃▄▂▄▄▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] score\u003c/td\u003e\u003ctd\u003e▁█▆▅▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▃▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▃█▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] loss\u003c/td\u003e\u003ctd\u003e▇▂▃▃▃▄▃▆▃▃▄▅▄▅█▇█▅▆▆▃▃▄▄▃▆▅▆▄▅▄▆▁▅▃▃▁▂▅▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] score\u003c/td\u003e\u003ctd\u003e▂▆▁█▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] avg_val_loss\u003c/td\u003e\u003ctd\u003e▃▁█▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] loss\u003c/td\u003e\u003ctd\u003e▆▆▆▇▂█▆▇▄▆▆█▅▄▇▅▇▅▅▂▅▁▂▆▃▄▆▆▄▄▆▃▅▄▃▅▅▄▅▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] score\u003c/td\u003e\u003ctd\u003e▁█▁▄▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▁▄▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] loss\u003c/td\u003e\u003ctd\u003e▆▆▆▇▅▃▇▅▅▃▆█▆▃▃▆▆▂▃▄▅▇▄▅▅▄▆▁▅▆▅▄▃▅▄▃▆▅▄▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] score\u003c/td\u003e\u003ctd\u003e▁▄█▆▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] avg_val_loss\u003c/td\u003e\u003ctd\u003e▂▁▃▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] loss\u003c/td\u003e\u003ctd\u003e▅▆▇█▄▆▄▃▆▃▃▃▄▅▃▃▂▄▃▃▅▄▄▄▁▅▄▄▃▃▄▃▃▆▄▁▄▄▅▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] score\u003c/td\u003e\u003ctd\u003e▃█▄▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] avg_val_loss\u003c/td\u003e\u003ctd\u003e▂▁▅█▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] loss\u003c/td\u003e\u003ctd\u003e▇▅▅▅▃▃▄▆▇▂▆▅▄▁▄▇▅▇▆█▅▃▅▅▂▃▅▅▇▃▂▅▆▁▂▂▅▃▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] score\u003c/td\u003e\u003ctd\u003e▄█▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▂█▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] loss\u003c/td\u003e\u003ctd\u003e▅▆▄▃█▆▆▅▅▇▄▄▃▄▅▂▅▃▂▄▄▄▄█▄▃▄▂▆▄▅▅▄▄▃▄▁▆▂▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] score\u003c/td\u003e\u003ctd\u003e▁██▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▁▇█▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] loss\u003c/td\u003e\u003ctd\u003e▇█▅▆▆▅█▅▅▆▃▅▃▁▇▄█▆▆▄▃▃▆▇▆▁▆▂▅▅▃▄▂▃▂▆█▂▂▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] score\u003c/td\u003e\u003ctd\u003e▁█▃▄▆\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▄▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] loss\u003c/td\u003e\u003ctd\u003e█▅▆▅▆▆▅▆▃▆▇▄▆▅▄▃▃▅▄▇▄▇▄▅▆▆▄▇▇▄▇▅▄█▅▆▄▄▅▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] score\u003c/td\u003e\u003ctd\u003e▂▂▁█▆\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▃▂▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] avg_val_loss\u003c/td\u003e\u003ctd\u003e▃▁▁██\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] loss\u003c/td\u003e\u003ctd\u003e▄▄▄▄▅▆▃▅▅▂▂▃▃▅▄▃▅▅▄▄▂▄▃▂▄▄█▄▆▁▂▅▄▅▂▃▄▃▂▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] score\u003c/td\u003e\u003ctd\u003e▁▆█▄▆\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] loss\u003c/td\u003e\u003ctd\u003e▄▄▇▆▅▃▄▆▄▆▃▂▆▅█▁▇▄▅▃▆▃▅▁▆▃▆▄██▆▆▄▆▇▄▄▆▂▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] score\u003c/td\u003e\u003ctd\u003e▃▁█▇▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▃▄▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] loss\u003c/td\u003e\u003ctd\u003e▆▄▅▇█▅▇▅█▇▅█▆▆▁▆▇▇▃▄▅▆█▅▄▆▆█▅▅▅▆▄▆█▇▃▇▄▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] score\u003c/td\u003e\u003ctd\u003e▇██▃▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▄█▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] loss\u003c/td\u003e\u003ctd\u003e█▅█▆▆█▄▇▆▅▆▄▃▆▆▄▆█▄█▆▆▆▅▅▅▂▆▁▆▅▅▆▆▄▅▄▃▅▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] score\u003c/td\u003e\u003ctd\u003e█▆▁▅▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▂▄▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] loss\u003c/td\u003e\u003ctd\u003e▆▄█▅▃▅▄▆▆▆█▄▃▅▄▅▆▁▆▅▆▄▇▅▆▅▄▅▅▄▃▆▄▅█▄▆▅▃▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] score\u003c/td\u003e\u003ctd\u003e▄▁█▅▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▃▆▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] loss\u003c/td\u003e\u003ctd\u003e▆▄▅▆▁▆▆▆▃▆▆▂▅▇▆▁▄▃▁▅▅▃▅▅▄▄▅▃█▃▇▆▃▄▃▅▄▅▇▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] score\u003c/td\u003e\u003ctd\u003e▁▃▆█▇\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▃▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▂▆▇█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] loss\u003c/td\u003e\u003ctd\u003e▅▄▇▃▅▅▃▅▄▅█▄▄▄▃▄▃▂▄▃▃▆▂▄▂▃▂▇▅▃▅▆▅▁▁▃▁▅▃▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] score\u003c/td\u003e\u003ctd\u003e▆█▁▇▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▃▂▂▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▃▆▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] loss\u003c/td\u003e\u003ctd\u003e▆▆▆▅▆▅▄▁▃▇█▁▂▄▂▄▅▃▄▅▂▅▂▅▃▆▆▃▂▅▄▅▆▄▅▂▄▅▆▃\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] score\u003c/td\u003e\u003ctd\u003e▁█▃▇▅\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] avg_train_loss\u003c/td\u003e\u003ctd\u003e█▄▂▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] avg_val_loss\u003c/td\u003e\u003ctd\u003e▁▂▄██\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] epoch\u003c/td\u003e\u003ctd\u003e▁▃▅▆█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] loss\u003c/td\u003e\u003ctd\u003e▆▅▅▄▅▅▅▅▆▄▆█▄▅▇▃▆▄▅▆▂▇▄▄▅▄▄▄▄▁▅▅▅▃▄▄▄▄▅▄\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] lr\u003c/td\u003e\u003ctd\u003e███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] score\u003c/td\u003e\u003ctd\u003e▁█▃▃▄\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003e[fold0] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.4924\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.58038\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] loss\u003c/td\u003e\u003ctd\u003e0.40276\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold0] score\u003c/td\u003e\u003ctd\u003e0.82991\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49391\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.54375\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] loss\u003c/td\u003e\u003ctd\u003e0.41646\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold10] score\u003c/td\u003e\u003ctd\u003e0.85869\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.4952\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.55944\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] loss\u003c/td\u003e\u003ctd\u003e0.50935\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold11] score\u003c/td\u003e\u003ctd\u003e0.84188\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49509\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.56941\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] loss\u003c/td\u003e\u003ctd\u003e0.48055\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold12] score\u003c/td\u003e\u003ctd\u003e0.83484\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.4939\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.55997\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] loss\u003c/td\u003e\u003ctd\u003e0.65476\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold13] score\u003c/td\u003e\u003ctd\u003e0.82996\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49112\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57596\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] loss\u003c/td\u003e\u003ctd\u003e0.54283\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold14] score\u003c/td\u003e\u003ctd\u003e0.83438\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49235\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57625\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] loss\u003c/td\u003e\u003ctd\u003e0.47984\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold15] score\u003c/td\u003e\u003ctd\u003e0.81276\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49186\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57267\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] loss\u003c/td\u003e\u003ctd\u003e0.49458\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold16] score\u003c/td\u003e\u003ctd\u003e0.8189\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49306\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.56601\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] loss\u003c/td\u003e\u003ctd\u003e0.53378\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold17] score\u003c/td\u003e\u003ctd\u003e0.81106\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49256\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.56648\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] loss\u003c/td\u003e\u003ctd\u003e0.43068\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold18] score\u003c/td\u003e\u003ctd\u003e0.83609\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49196\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.59284\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] loss\u003c/td\u003e\u003ctd\u003e0.57528\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold19] score\u003c/td\u003e\u003ctd\u003e0.81042\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49633\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57461\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] loss\u003c/td\u003e\u003ctd\u003e0.38985\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold1] score\u003c/td\u003e\u003ctd\u003e0.82951\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49399\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.5491\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] loss\u003c/td\u003e\u003ctd\u003e0.55831\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold2] score\u003c/td\u003e\u003ctd\u003e0.84664\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49276\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.5719\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] loss\u003c/td\u003e\u003ctd\u003e0.40604\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold3] score\u003c/td\u003e\u003ctd\u003e0.83115\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49288\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.59668\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] loss\u003c/td\u003e\u003ctd\u003e0.46157\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold4] score\u003c/td\u003e\u003ctd\u003e0.81668\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49268\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.58375\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] loss\u003c/td\u003e\u003ctd\u003e0.59006\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold5] score\u003c/td\u003e\u003ctd\u003e0.81467\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49225\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.58722\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] loss\u003c/td\u003e\u003ctd\u003e0.46434\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold6] score\u003c/td\u003e\u003ctd\u003e0.81658\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49154\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57566\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] loss\u003c/td\u003e\u003ctd\u003e0.38604\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold7] score\u003c/td\u003e\u003ctd\u003e0.81084\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49746\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.56801\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] loss\u003c/td\u003e\u003ctd\u003e0.63548\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold8] score\u003c/td\u003e\u003ctd\u003e0.82145\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] avg_train_loss\u003c/td\u003e\u003ctd\u003e0.49179\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] avg_val_loss\u003c/td\u003e\u003ctd\u003e0.57762\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] epoch\u003c/td\u003e\u003ctd\u003e5\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] loss\u003c/td\u003e\u003ctd\u003e0.5059\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] lr\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e[fold9] score\u003c/td\u003e\u003ctd\u003e0.81766\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003emicrosoft/deberta-v3-large\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/bluehills/PPPM-Training/runs/ffslevnp\" target=\"_blank\"\u003ehttps://wandb.ai/bluehills/PPPM-Training/runs/ffslevnp\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: \u003ccode\u003e./wandb/run-20220521_030613-ffslevnp/logs\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["if __name__ == '__main__':\n","    \n","    def get_result(oof_df):\n","        labels = oof_df['score'].values\n","        preds = oof_df['pred'].values\n","        score = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:\u003c.4f}')\n","    \n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                _oof_df = train_loop(train, fold)\n","                oof_df = pd.concat([oof_df, _oof_df])\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        oof_df = oof_df.reset_index(drop=True)\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n","        \n","    if CFG.wandb:\n","        wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tL7g_rhfN1sr"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNt0NrR5I6TYuL29xVKDN/m","background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"PPPM training.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00e09dd38eea4b78828cb13f548bc00c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efa143273fc742b09d0688d0b34091f6","placeholder":"​","style":"IPY_MODEL_2038fafcd0734873acde4554d330a1b2","value":" 833M/833M [00:15\u0026lt;00:00, 68.8MB/s]"}},"04405997cbdb4021a01b79690fa37a2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"085986b038804169aea7d2fed219e269":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adf2c58735734e26bb79a6f388cff53f","IPY_MODEL_7553f91bc30547ab905b652a768e0a52","IPY_MODEL_4219de946ee84456b68526d4efd41c3a"],"layout":"IPY_MODEL_2d0dec1947d148aea2b880a0d7e4fba7"}},"17af81a0de334ce3b5b6de9f0ce6fe85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a7425c4857a4baebe7b41a8323f99cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c3638d42eb44df5bb4621c41f00973b","IPY_MODEL_b0068c017baf44cf9b78145c10ab74cc","IPY_MODEL_1aa3b65f197241399abbb540f4493e33"],"layout":"IPY_MODEL_bd42141d2d9d4a78a412abd5fa2e3a3b"}},"1aa3b65f197241399abbb540f4493e33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9578af886284d5481cce7af34179ab8","placeholder":"​","style":"IPY_MODEL_537106673e50450caa8260c8ea7f01ec","value":" 36473/36473 [00:02\u0026lt;00:00, 13023.14it/s]"}},"1df3806aeb75462bb2182366563f9131":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2038fafcd0734873acde4554d330a1b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21de0591b92b46d086603b6b93d4d3fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2ea7dbb9c184d18812af55cf2f74b78","IPY_MODEL_ec53c47b16054a728c8bcc45e0413c40","IPY_MODEL_f4efa72c68cf4736aaaed9eef53db46b"],"layout":"IPY_MODEL_8715b220327044b9aa8ed08315250032"}},"2bad2d68417d4b63a45d53c28d3a2df3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ce4f3f2678b403b90606febfab3f5f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42d4def1c3e94d1fb93192d80857c7cc","placeholder":"​","style":"IPY_MODEL_efce8f3e0b8544898b5aade7a7bd645b","value":" 136/136 [00:00\u0026lt;00:00, 2600.56it/s]"}},"2d0dec1947d148aea2b880a0d7e4fba7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31a380fc4f694f0b938a01787edd84bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31b86372a267460fb21744fde9854e7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c7bbb4c94d41c19f5b721c418dbb6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"367268e5790443a4a51174af4f5d3035":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3939642a79e14e68b40631a13bb6bdf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fc5680c8a5e4334a8d186c5880fe37c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_475cbabe97214db58e6de37d6759489a","IPY_MODEL_e7d6f25251534d37923c2884b28ed5a3","IPY_MODEL_00e09dd38eea4b78828cb13f548bc00c"],"layout":"IPY_MODEL_1df3806aeb75462bb2182366563f9131"}},"4219de946ee84456b68526d4efd41c3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a28e3710cd284cd3803450d17633206d","placeholder":"​","style":"IPY_MODEL_8a9e51caada9413688eb627222445e5d","value":" 36473/36473 [00:02\u0026lt;00:00, 13498.65it/s]"}},"42d4def1c3e94d1fb93192d80857c7cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44e577c001974e8a8be5c23753a7bee4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"475cbabe97214db58e6de37d6759489a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3939642a79e14e68b40631a13bb6bdf1","placeholder":"​","style":"IPY_MODEL_bcd76cf65e3f4fffb54484050fe894bc","value":"Downloading: 100%"}},"4c3638d42eb44df5bb4621c41f00973b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63244f6f141846749eef2ea652ef71ba","placeholder":"​","style":"IPY_MODEL_367268e5790443a4a51174af4f5d3035","value":"100%"}},"520f9f34417b4d298da6821614ba3f95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fa85b28991f4b3189851d2171fa46ec","max":580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae28b6a8c44b44669ce934e545d61453","value":580}},"537106673e50450caa8260c8ea7f01ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5452a60ac1ab47ee8c91c2ea386a8c46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"618a875122954c42869d78f931214d7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9e3e852d041474ba78d8929f5b44398","placeholder":"​","style":"IPY_MODEL_31b86372a267460fb21744fde9854e7a","value":" 580/580 [00:00\u0026lt;00:00, 22.0kB/s]"}},"61cec5e1d9024a3698a8c7ff234f7558":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_730e4ffbda70484aacfc6a6b035abcd7","IPY_MODEL_520f9f34417b4d298da6821614ba3f95","IPY_MODEL_618a875122954c42869d78f931214d7d"],"layout":"IPY_MODEL_9ff35fe9939046908db74c8bbff6f3e2"}},"622edc72717043b28889be8a82f5a74d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63244f6f141846749eef2ea652ef71ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646a3cd6bee94409a1cf10bf1f7fe8ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64db227891b74adb9d610b4dc8aaf606":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6508d5e30a8e43d19271fee3ed70dfd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d1548af1a8448aab2d0bf9b2c8cd2c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab1fb85ee45440979f4fb6f8dd4a70b2","IPY_MODEL_b38e4afb73544fcfb09a5575f20c9739","IPY_MODEL_fb749c79603941e6b261f001012d7301"],"layout":"IPY_MODEL_cee4f600b3204530a46fd6f7eecc58fa"}},"6d2e088d3f414160bb2c6c682e8af499":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"710a4e57b33d4e8e91614857804bda8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"722e7b3d89de49938335a00d41a533cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730e4ffbda70484aacfc6a6b035abcd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04405997cbdb4021a01b79690fa37a2d","placeholder":"​","style":"IPY_MODEL_7d682c0befff4ea5a8f6be39252b286f","value":"Downloading: 100%"}},"7553f91bc30547ab905b652a768e0a52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c69f8bfe50664aad9200216662cdf8f5","max":36473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94121da7cb9a46608cbd9c59763cc1b7","value":36473}},"78aad57533894f4d8af389605ccc1fad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5bac3372dbd4864bf3f50e5631be060","IPY_MODEL_e12eca600d124ed1b3efef05fd0a824a","IPY_MODEL_2ce4f3f2678b403b90606febfab3f5f6"],"layout":"IPY_MODEL_722e7b3d89de49938335a00d41a533cd"}},"7d667e658a6c4317a6be3744d061ad74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d682c0befff4ea5a8f6be39252b286f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86dedb1f3dca4c6a823031a6404eb586":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8715b220327044b9aa8ed08315250032":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89f3270059c04683946468edc7654134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a9e51caada9413688eb627222445e5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8fa85b28991f4b3189851d2171fa46ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94121da7cb9a46608cbd9c59763cc1b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ff35fe9939046908db74c8bbff6f3e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a28e3710cd284cd3803450d17633206d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab1fb85ee45440979f4fb6f8dd4a70b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64db227891b74adb9d610b4dc8aaf606","placeholder":"​","style":"IPY_MODEL_abd7bb814cf643109559f491f07cd4d0","value":"Downloading: 100%"}},"abd7bb814cf643109559f491f07cd4d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adf2c58735734e26bb79a6f388cff53f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e90d414985cf4c5d830c682c09c7d1bc","placeholder":"​","style":"IPY_MODEL_2bad2d68417d4b63a45d53c28d3a2df3","value":"100%"}},"ae28b6a8c44b44669ce934e545d61453":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0068c017baf44cf9b78145c10ab74cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf7698df5b05401d8e9abd79c789b874","max":36473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d667e658a6c4317a6be3744d061ad74","value":36473}},"b38e4afb73544fcfb09a5575f20c9739":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86dedb1f3dca4c6a823031a6404eb586","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_710a4e57b33d4e8e91614857804bda8c","value":2464616}},"b9578af886284d5481cce7af34179ab8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b971d8972e5447e5b40ed7ed1b884064":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcd76cf65e3f4fffb54484050fe894bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd42141d2d9d4a78a412abd5fa2e3a3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2ea7dbb9c184d18812af55cf2f74b78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5452a60ac1ab47ee8c91c2ea386a8c46","placeholder":"​","style":"IPY_MODEL_6d2e088d3f414160bb2c6c682e8af499","value":"Downloading: 100%"}},"c58f151ae1e24c5d845bbec83435e787":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c69f8bfe50664aad9200216662cdf8f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cee4f600b3204530a46fd6f7eecc58fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf3ce1a223bf48e38531df088b59e491":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf7698df5b05401d8e9abd79c789b874":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7611deee01a454289f6ffc1e8f4c5d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e12eca600d124ed1b3efef05fd0a824a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf3ce1a223bf48e38531df088b59e491","max":136,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c58f151ae1e24c5d845bbec83435e787","value":136}},"e7d6f25251534d37923c2884b28ed5a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_646a3cd6bee94409a1cf10bf1f7fe8ad","max":873673253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_622edc72717043b28889be8a82f5a74d","value":873673253}},"e90d414985cf4c5d830c682c09c7d1bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9e3e852d041474ba78d8929f5b44398":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec53c47b16054a728c8bcc45e0413c40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31a380fc4f694f0b938a01787edd84bf","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89f3270059c04683946468edc7654134","value":52}},"efa143273fc742b09d0688d0b34091f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efce8f3e0b8544898b5aade7a7bd645b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4efa72c68cf4736aaaed9eef53db46b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b971d8972e5447e5b40ed7ed1b884064","placeholder":"​","style":"IPY_MODEL_d7611deee01a454289f6ffc1e8f4c5d6","value":" 52.0/52.0 [00:00\u0026lt;00:00, 2.19kB/s]"}},"f5bac3372dbd4864bf3f50e5631be060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17af81a0de334ce3b5b6de9f0ce6fe85","placeholder":"​","style":"IPY_MODEL_6508d5e30a8e43d19271fee3ed70dfd0","value":"100%"}},"fb749c79603941e6b261f001012d7301":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44e577c001974e8a8be5c23753a7bee4","placeholder":"​","style":"IPY_MODEL_35c7bbb4c94d41c19f5b721c418dbb6d","value":" 2.35M/2.35M [00:00\u0026lt;00:00, 11.5MB/s]"}}}}},"nbformat":4,"nbformat_minor":0}