{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4wtXQc5eILks"},"outputs":[],"source":["# !pip install kaggle\n","# from google.colab import files\n","# files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ybT-kPrIRRF"},"outputs":[],"source":["# !ls -1ha kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18084,"status":"ok","timestamp":1652107855307,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"ngruL3LzILgG","outputId":"ffbe621d-3af3-4cac-f3f2-b0275ee6b041"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-pSk1HrImpC"},"outputs":[],"source":["# !mkdir -p ~/.kaggle\n","# !cp kaggle.json ~/.kaggle/\n","# # Permission Warning 이 일어나지 않도록 \n","# !chmod 600 ~/.kaggle/kaggle.json\n","# # 본인이 참가한 모든 대회 보기 \n","# !kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7904,"status":"ok","timestamp":1652107863206,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"93TLgBfpILdp","outputId":"2cef041b-d2d8-430c-8567-1a4537220158"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tokenizers\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 15.5 MB/s \n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 59.8 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 71.6 MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 66.3 MB/s \n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 64.2 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=1b16b2775688f7de45e39359a37147d9efc130c029b4f40ef1f3e747e08fa5da\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, tokenizers, sentencepiece\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentencepiece-0.1.96 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 tokenizers-0.12.1 wandb-0.12.16\n"]}],"source":["!pip3 install tokenizers wandb sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7496,"status":"ok","timestamp":1652107870693,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"dTDhaP31LevK","outputId":"91822213-182f-4916-c251-ff48c73340fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 14.8 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 68.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 81.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=a85ef549a5c6aa624168cc0f9d423cc2b8cd6020ce073a872986b9d00e69c2d4\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 transformers-4.18.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABCV5MzcILYt"},"outputs":[],"source":["import os\n","os.chdir(\"drive/\")\n","os.chdir('My Drive')\n","os.chdir('Kaggle')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe14zhChIld4"},"outputs":[],"source":["# !kaggle competitions download -c us-patent-phrase-to-phrase-matching\n","# !unzip us-patent-phrase-to-phrase-matching.zip\n","# !ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enX70dDOIlbH"},"outputs":[],"source":["# debert_v3_tokenizer_path = 'deberta-v2-v3-fast-tokenizer'\n","# %env TOKENIZERS_PARALLELISM=true\n","\n","# import shutil\n","# from pathlib import Path\n","\n","# transformers_path = Path('/usr/local/lib/python3.7/dist-packages/transformers')\n","# input_dir = Path('./deberta-v2-v3-fast-tokenizer')\n","\n","# convert_file = input_dir / \"convert_slow_tokenizer.py\"\n","# conversion_path = transformers_path / convert_file.name\n","\n","# if conversion_path.exists():\n","#     conversion_path.unlink()\n","\n","# shutil.copy(convert_file, transformers_path)\n","# deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","# for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","#     filepath = deberta_v2_path/filename\n","    \n","#     if filepath.exists():\n","#         filepath.unlink()\n","#     shutil.copy(input_dir/filename, filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8uVyeNzILWZ"},"outputs":[],"source":["OUTPUT_DIR = './uspppm-deberta-v3-outputs/'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488,"status":"ok","timestamp":1652107871176,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"B9XJVEp-ILTm","outputId":"fa8931ff-5c64-4cbe-9241-d6c8a3590fde"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon May  9 14:51:10 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad4bqKUJILRr"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    wandb=True\n","    competition='PPPM'\n","    _wandb_kernel='bluehills'\n","    debug=False\n","    apex=True\n","    print_freq=100\n","    num_workers=4\n","    model=\"microsoft/deberta-v3-large\"\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=50\n","    epochs=5\n","    encoder_lr=2e-5 #2e-5\n","    decoder_lr=2e-5 #2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size=10\n","    fc_dropout=0.2\n","    target_size=1\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    n_fold=10\n","    trn_fold=[0, 1, 2, 3, 4]\n","    train=True\n","    #------------------------------\n","    # focal loss\n","    alpha=0.3\n","    gamma=3\n","    focal_reduction='mean' # ['mean', 'sum']\n","    lb_smooth=0.1\n","    \n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"elapsed":10806,"status":"ok","timestamp":1652107881978,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"qW8but_GILO1","outputId":"6a7c76c7-89bd-444a-c657-5cb579332573"},"outputs":[{"name":"stdout","output_type":"stream","text":["login to wandb\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbluehills\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.16"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/My Drive/Kaggle/wandb/run-20220509_145116-2wfm1kp2</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/bluehills/PPPM-focal_loss/runs/2wfm1kp2\" target=\"_blank\">microsoft/deberta-v3-large</a></strong> to <a href=\"https://wandb.ai/bluehills/PPPM-focal_loss\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","    import wandb\n","    try:\n","        # from kaggle_secrets import UserSecretsClient\n","        # user_secrets = UserSecretsClient()\n","        # secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        # wandb.login(key=secret_value_0)\n","        print('login to wandb')\n","        wandb.login()\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","\n","    run = wandb.init(project='PPPM-focal_loss', \n","                     name=CFG.model,\n","                     config=class2dict(CFG),\n","                     group=CFG.model,\n","                     job_type=\"train\",\n","                     anonymous=anony)"]},{"cell_type":"markdown","metadata":{"id":"g_kIjiCGLHsk"},"source":["# Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6178,"status":"ok","timestamp":1652107888137,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"F3Ud6NtXILMj","outputId":"2955a7b2-8701-4f63-a714-9753a9967137"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.__version__: 1.11.0+cu113\n","tokenizers.__version__: 0.12.1\n","transformers.__version__: 4.18.0\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import shutil\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedGroupKFold\n","\n","import torch\n","print(f\"torch.__version__: {torch.__version__}\")\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","import torch.cuda.amp as amp\n","\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n","\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"0N-UkOUGLMTx"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO_u0OIhILJo"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def get_score(y_true, y_pred):\n","    score = sp.stats.pearsonr(y_true, y_pred)[0]\n","    return score\n","\n","\n","def get_logger(filename=OUTPUT_DIR+'train'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)\n"]},{"cell_type":"markdown","metadata":{"id":"6epV68-8Lrk7"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"executionInfo":{"elapsed":824,"status":"ok","timestamp":1652107888954,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"-15eijrbILHp","outputId":"b14b101c-fe9e-4a24-e5f4-3205117e4892"},"outputs":[{"name":"stdout","output_type":"stream","text":["train.shape: (36473, 5)\n","test.shape: (36, 4)\n","submission.shape: (36, 2)\n"]},{"data":{"text/html":["\n","  <div id=\"df-d05e538a-31d4-4f9e-a962-a4adcab90e6e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d05e538a-31d4-4f9e-a962-a4adcab90e6e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d05e538a-31d4-4f9e-a962-a4adcab90e6e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d05e538a-31d4-4f9e-a962-a4adcab90e6e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id     anchor                  target context  score\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  <div id=\"df-3ace70d4-bc2f-44d0-a26e-57962dce773d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ace70d4-bc2f-44d0-a26e-57962dce773d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3ace70d4-bc2f-44d0-a26e-57962dce773d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3ace70d4-bc2f-44d0-a26e-57962dce773d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id              anchor                         target context\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n","2  36baf228038e314b      lower trunnion                 lower locating     B60\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  <div id=\"df-06dd3c04-4246-4885-809f-7aabf248fc40\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06dd3c04-4246-4885-809f-7aabf248fc40')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-06dd3c04-4246-4885-809f-7aabf248fc40 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-06dd3c04-4246-4885-809f-7aabf248fc40');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id  score\n","0  4112d61851461f60      0\n","1  09e418c93a776564      0\n","2  36baf228038e314b      0\n","3  1f37ead645e7f0c8      0\n","4  71a5b6ad068d531f      0"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","submission = pd.read_csv('sample_submission.csv')\n","print(f\"train.shape: {train.shape}\")\n","print(f\"test.shape: {test.shape}\")\n","print(f\"submission.shape: {submission.shape}\")\n","display(train.head())\n","display(test.head())\n","display(submission.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":4429,"status":"ok","timestamp":1652107893379,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"6T4B0z0yILFW","outputId":"48b60ba8-158a-4b65-c3b1-28a922927a6d"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-a5b6c089-0b34-47a9-b803-e70327aa7a93\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5b6c089-0b34-47a9-b803-e70327aa7a93')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a5b6c089-0b34-47a9-b803-e70327aa7a93 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a5b6c089-0b34-47a9-b803-e70327aa7a93');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id     anchor                  target context  score                                       context_text\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  <div id=\"df-82b21580-e7fc-4577-8b14-e796adb51a7c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>context_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","      <td>PHYSICS. OPTICS</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82b21580-e7fc-4577-8b14-e796adb51a7c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-82b21580-e7fc-4577-8b14-e796adb51a7c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-82b21580-e7fc-4577-8b14-e796adb51a7c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id              anchor                         target context                                       context_text\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# CPC Data\n","# ====================================================\n","def get_cpc_texts():\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir('./CPCSchemeXML202105'):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(f'./CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        cpc_result = result[0].lstrip(pattern)\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n","    return results\n","\n","\n","cpc_texts = get_cpc_texts()\n","torch.save(cpc_texts, OUTPUT_DIR+\"cpc_texts.pth\")\n","train['context_text'] = train['context'].map(cpc_texts)\n","test['context_text'] = test['context'].map(cpc_texts)\n","display(train.head())\n","display(test.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1652107893380,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"46m7DZuOILDE","outputId":"a911c775-ad73-494e-f4d2-99894ab3680f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-0457edd9-9b01-4dd3-a197-23270018aaa8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]abatement of pollution[SEP]HUMAN...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]act of abating[SEP]HUMAN NECESSI...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]active catalyst[SEP]HUMAN NECESS...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]eliminating process[SEP]HUMAN NE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]forest region[SEP]HUMAN NECESSIT...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0457edd9-9b01-4dd3-a197-23270018aaa8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0457edd9-9b01-4dd3-a197-23270018aaa8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0457edd9-9b01-4dd3-a197-23270018aaa8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id     anchor                  target context  score                                       context_text                                               text\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]abatement of pollution[SEP]HUMAN...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]act of abating[SEP]HUMAN NECESSI...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]active catalyst[SEP]HUMAN NECESS...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]eliminating process[SEP]HUMAN NE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]forest region[SEP]HUMAN NECESSIT..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  <div id=\"df-a4e5315f-9a29-4e12-9411-79e83b9b3a20\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>context_text</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","      <td>PHYSICS. OPTICS</td>\n","      <td>opc drum[SEP]inorganic photoconductor drum[SEP...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n","      <td>adjust gas flow[SEP]altering gas flow[SEP]MECH...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n","      <td>lower trunnion[SEP]lower locating[SEP]PERFORMI...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n","      <td>cap component[SEP]upper portion[SEP]TEXTILES; ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n","      <td>neural stimulation[SEP]artificial neural netwo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4e5315f-9a29-4e12-9411-79e83b9b3a20')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a4e5315f-9a29-4e12-9411-79e83b9b3a20 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a4e5315f-9a29-4e12-9411-79e83b9b3a20');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id              anchor                         target context                                       context_text                                               text\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS  opc drum[SEP]inorganic photoconductor drum[SEP...\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...  adjust gas flow[SEP]altering gas flow[SEP]MECH...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...  lower trunnion[SEP]lower locating[SEP]PERFORMI...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...  cap component[SEP]upper portion[SEP]TEXTILES; ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE  neural stimulation[SEP]artificial neural netwo..."]},"metadata":{},"output_type":"display_data"}],"source":["train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n","test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n","display(train.head())\n","display(test.head())"]},{"cell_type":"markdown","metadata":{"id":"isTSVEuINl2S"},"source":["# EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1652107893380,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"3rsR5QFLILAq","outputId":"0a4acf5e-947e-41a8-98a5-0ebd45c192d0"},"outputs":[{"data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f9b76757710>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train['score'].hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1652107893380,"user":{"displayName":"성연우","userId":"12015659827226767816"},"user_tz":-540},"id":"qco8TIYeIK-W","outputId":"5192437e-d493-48d1-ad62-ccce3030ec53"},"outputs":[{"data":{"text/plain":["B    8019\n","H    6195\n","G    6013\n","C    5288\n","A    4094\n","F    4054\n","E    1531\n","D    1279\n","Name: context, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["display(train['context'].apply(lambda x: x[0]).value_counts())"]},{"cell_type":"markdown","metadata":{"id":"0X9jmLp9NrEE"},"source":["# CV Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnm4sSJdIK73"},"outputs":[],"source":["# # ====================================================\n","# # CV split\n","# # ====================================================\n","# train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","# Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n","#     train.loc[val_index, 'fold'] = int(n)\n","# train['fold'] = train['fold'].astype(int)\n","# display(train.groupby('fold').size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ppg1HHHBjrU7"},"outputs":[],"source":["train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","\n","encoder = LabelEncoder()\n","train['anchor_map'] = encoder.fit_transform(train['anchor'])\n","\n","kf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","for n, (_, valid_index) in enumerate(kf.split(train, train['score_map'], groups=train['anchor_map'])):\n","    train.loc[valid_index, 'fold'] = int(n)\n","\n","train['fold'] = train['fold'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67wpP1q7IK5L"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())"]},{"cell_type":"markdown","metadata":{"id":"bIqxWfHqNzR2"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["5523755f4e834f218fbe84e837b84ac7","ee8b843344a94acdadc3795d3a2b7daf","0988def69b634add88eff82a97a5363a","6c81baee294343449d6f2b78a28f3f36","a10fb752a0444556a965d9949cea130f","63c3036f107e463b892869fd87ba6136","37e8f1beb29f4e3d9b939b37f7416eed","35eb7c4af6024a379cd5dc0d9759b284","d01362ab47974359ad7734e2194e65a6","fb251b5019294289a8f1e3534edc47ad","ad7fe65a41fe486c971a6859e83db9df"]},"id":"0LIvoPmoIFUv","outputId":"378f6d98-7d21-460e-fd02-f97aaf0472e2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5523755f4e834f218fbe84e837b84ac7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"BvhCQypyN2nU"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9KbGQdeN195"},"outputs":[],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","lengths_dict = {}\n","\n","lengths = []\n","tk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","    lengths.append(length)\n","lengths_dict['context_text'] = lengths\n","\n","for text_col in ['anchor', 'target']:\n","    lengths = []\n","    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n","    for text in tk0:\n","        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","        lengths.append(length)\n","    lengths_dict[text_col] = lengths\n","    \n","CFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n","                + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twc1qFyRN17n"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text):\n","    inputs = cfg.tokenizer(text,\n","                           add_special_tokens=True,\n","                           max_length=cfg.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].values\n","        self.labels = df['score'].values\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        return inputs, label"]},{"cell_type":"markdown","metadata":{"id":"m1x8L7BQOKr2"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4Uu80kLN13D"},"outputs":[],"source":["class TransformerHead(nn.Module):\n","    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n","        super().__init__()\n","\n","        self.transformer = nn.TransformerEncoder(\n","            encoder_layer=nn.TransformerEncoderLayer(d_model=in_features, nhead=nhead),\n","            num_layers=num_layers)\n","        self.row_fc = nn.Linear(in_features, 1)\n","        self.out_features = max_length\n","\n","    def forward(self, x):\n","        out = self.transformer(x)\n","        out = self.row_fc(out).squeeze(-1)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYUMhMGIOL4z"},"outputs":[],"source":["# # ====================================================\n","# # Model\n","# # ====================================================\n","# class CustomModel(nn.Module):\n","#     def __init__(self, cfg, config_path=None, pretrained=False):\n","#         super().__init__()\n","#         self.cfg = cfg\n","#         if config_path is None:\n","#             self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","#         else:\n","#             self.config = torch.load(config_path)\n","#         if pretrained:\n","#             self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","#         else:\n","#             self.model = AutoModel.from_config(self.config)\n","        \n","#         self.feature_extractor = AutoModelForTokenClassification.from_pretrained(cfg.model)\n","#         in_features = self.feature_extractor.classifier.in_features\n","#         self.attention = TransformerHead(in_features=in_features, max_length=133, num_layers=1, nhead=8, num_targets=1)\n","#         self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","#         self.fc = nn.Linear(self.attention.out_features, self.cfg.target_size)\n","#         self._init_weights(self.fc)\n","#         self._init_weights(self.attention)\n","        \n","#     def _init_weights(self, module):\n","#         if isinstance(module, nn.Linear):\n","#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","#             if module.bias is not None:\n","#                 module.bias.data.zero_()\n","#         elif isinstance(module, nn.Embedding):\n","#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","#             if module.padding_idx is not None:\n","#                 module.weight.data[module.padding_idx].zero_()\n","#         elif isinstance(module, nn.LayerNorm):\n","#             module.bias.data.zero_()\n","#             module.weight.data.fill_(1.0)\n","        \n","#     def feature(self, inputs):\n","#         outputs = self.model(**inputs)\n","#         last_hidden_states = outputs[0]\n","#         # feature = torch.mean(last_hidden_states, 1)\n","#         feature = self.attention(last_hidden_states)\n","        \n","#         return feature\n","\n","\n","#     def forward(self, inputs):\n","#         feature = self.feature(inputs)\n","#         #print(feature.shape)\n","#         output = self.fc(self.fc_dropout(feature))\n","#         return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPMtgF_NbjBe"},"outputs":[],"source":["# ====================================================\n","# Model\n","# ====================================================\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n","        self._init_weights(self.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self._init_weights(self.attention)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        # feature = torch.mean(last_hidden_states, 1)\n","        weights = self.attention(last_hidden_states)\n","        feature = torch.sum(weights * last_hidden_states, dim=1)\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(self.fc_dropout(feature))\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmtpzWxmRGwb"},"outputs":[],"source":["class FocalLossV1(nn.Module):\n","    def __init__(self,\n","                 alpha=0.1,\n","                 gamma=2,\n","                 reduction='mean',):\n","        super(FocalLossV1, self).__init__()\n","        self.alpha = alpha\n","        # self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.crit = nn.BCEWithLogitsLoss(reduction='none')\n","\n","    def forward(self, logits, label):\n","        '''\n","        Usage is same as nn.BCEWithLogits:\n","            >>> criteria = FocalLossV1()\n","            >>> logits = torch.randn(8, 19, 384, 384)\n","            >>> lbs = torch.randint(0, 2, (8, 19, 384, 384)).float()\n","            >>> loss = criteria(logits, lbs)\n","        '''\n","        probs = torch.sigmoid(logits)\n","        coeff = torch.abs(label - probs).pow(self.gamma).neg()\n","        log_probs = torch.where(logits >= 0,\n","                F.softplus(logits, -1, 50),\n","                logits - F.softplus(logits, 1, 50))\n","        log_1_probs = torch.where(logits >= 0,\n","                -logits + F.softplus(logits, -1, 50),\n","                -F.softplus(logits, 1, 50))\n","        loss = label * self.alpha * log_probs + (1. - label) * (1. - self.alpha) * log_1_probs\n","        loss = loss * coeff\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","        if self.reduction == 'sum':\n","            loss = loss.sum()\n","        return loss\n","        # BCE_loss = F.binary_cross_entropy_with_logits(logits, label, reduction='none')\n","        # targets = label.type(torch.long)\n","        # at = self.alpha.gather(0, targets.data.view(-1))\n","        # pt = torch.exp(-BCE_loss)\n","        # F_loss = at*(1-pt)**self.gamma * BCE_loss\n","        # return F_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XWMmXjX3W0a"},"outputs":[],"source":["class FocalLossWithSmoothing(nn.Module):\n","    def __init__(\n","            self,\n","            num_classes: int,\n","            gamma: int = 1,\n","            lb_smooth: float = 0.1,\n","            size_average: bool = True,\n","            ignore_index: int = None,\n","            alpha: float = None):\n","        \"\"\"\n","        :param gamma:\n","        :param lb_smooth:\n","        :param ignore_index:\n","        :param size_average:\n","        :param alpha:\n","        \"\"\"\n","        super(FocalLossWithSmoothing, self).__init__()\n","        self._num_classes = num_classes\n","        self._gamma = gamma\n","        self._lb_smooth = lb_smooth\n","        self._size_average = size_average\n","        self._ignore_index = ignore_index\n","        self._log_softmax = nn.LogSoftmax(dim=1)\n","        self._alpha = alpha\n","\n","        if self._num_classes <= 1:\n","            raise ValueError('The number of classes must be 2 or higher')\n","        if self._gamma < 0:\n","            raise ValueError('Gamma must be 0 or higher')\n","        if self._alpha is not None:\n","            if self._alpha <= 0 or self._alpha >= 1:\n","                raise ValueError('Alpha must be 0 <= alpha <= 1')\n","\n","    def forward(self, logits, label):\n","        \"\"\"\n","        :param logits: (batch_size, class, height, width)\n","        :param label:\n","        :return:\n","        \"\"\"\n","        logits = logits.float()\n","        difficulty_level = self._estimate_difficulty_level(logits, label)\n","\n","        with torch.no_grad():\n","            label = label.clone().detach()\n","            if self._ignore_index is not None:\n","                ignore = label.eq(self._ignore_index)\n","                label[ignore] = 0\n","            lb_pos, lb_neg = 1. - self._lb_smooth, self._lb_smooth / (self._num_classes - 1)\n","            lb_one_hot = torch.empty_like(logits).fill_(\n","                lb_neg).scatter_(1, label.unsqueeze(1), lb_pos).detach()\n","        logs = self._log_softmax(logits)\n","        loss = -torch.sum(difficulty_level * logs * lb_one_hot, dim=1)\n","        if self._ignore_index is not None:\n","            loss[ignore] = 0\n","        return loss.mean()\n","\n","    def _estimate_difficulty_level(self, logits, label):\n","        \"\"\"\n","        :param logits:\n","        :param label:\n","        :return:\n","        \"\"\"\n","        one_hot_key = torch.nn.functional.one_hot(label, num_classes=self._num_classes)\n","        if len(one_hot_key.shape) == 4:\n","            one_hot_key = one_hot_key.permute(0, 3, 1, 2)\n","        if one_hot_key.device != logits.device:\n","            one_hot_key = one_hot_key.to(logits.device)\n","        pt = one_hot_key * F.softmax(logits)\n","        difficulty_level = torch.pow(1 - pt, self._gamma)\n","        return difficulty_level\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kSJ2SoJFUNB"},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 <= smoothing < 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8cD8ThO5cbg"},"outputs":[],"source":["# def criterion_focal_loss(preds, targets):\n","#     targets2 = torch.where(targets >= 0.5, 2*targets -1, 1 - 2*targets)\n","#     preds2 = torch.where(targets >= 0.5, preds, -preds)\n","#     loss = - targets2 * F.logsigmoid(preds2)\n","#     return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8m2hrvX4D4VA"},"outputs":[],"source":["# class LabelSmoothSoftmaxCE(nn.Module):\n","#     '''\n","#     This is the autograd version, you can also try the LabelSmoothSoftmaxCEV2 that uses derived gradients\n","#     '''\n","\n","#     def __init__(self, lb_smooth=0.1, reduction='mean', ignore_index=-100, loss_func=nn.LogSoftmax(dim=1)):\n","#         super(LabelSmoothSoftmaxCE, self).__init__()\n","#         self.lb_smooth = lb_smooth\n","#         self.reduction = reduction\n","#         self.lb_ignore = ignore_index\n","#         self.log_softmax = nn.LogSoftmax(dim=1)\n"," \n","#     def forward(self, logits, label):\n","#         '''\n","#         Same usage method as nn.CrossEntropyLoss:\n","#             >>> criteria = LabelSmoothSoftmaxCEV1()\n","#             >>> logits = torch.randn(8, 19, 384, 384) # nchw, float/half\n","#             >>> lbs = torch.randint(0, 19, (8, 384, 384)) # nhw, int64_t\n","#             >>> loss = criteria(logits, lbs)\n","#         '''\n","#         # overcome ignored label\n","#         logits = logits.float() # use fp32 to avoid nan\n","#         with torch.no_grad():\n","#             num_classes = logits.size(1)\n","#             label = label.clone().detach()\n","#             ignore = label.eq(self.lb_ignore)\n","#             n_valid = ignore.eq(0).sum()\n","#             label[ignore] = 0\n","#             lb_pos, lb_neg = 1. - self.lb_smooth, self.lb_smooth / num_classes\n","#             lb_one_hot = torch.empty_like(logits).fill_(lb_neg).scatter_(1, label.unsqueeze(1), lb_pos).detach()\n","\n","#         logs = self.log_softmax(logits)\n","#         loss = -torch.sum(logs * lb_one_hot, dim=1)\n","#         loss[ignore] = 0\n","#         if self.reduction == 'mean':\n","#             loss = loss.sum() / n_valid\n","#         if self.reduction == 'sum':\n","#             loss = loss.sum()\n","\n","#         return loss\n"]},{"cell_type":"markdown","metadata":{"id":"N80Z0ZF9OcjW"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjzXaGwxN10f"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","        \n","        # loss = criterion(y_preds.sigmoid().view(-1, 1), labels.view(-1, 1))\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        # loss = criterion_focal_loss(y_preds.view(-1, 1), labels.view(-1, 1))\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        # loss = criterion_focal_loss(y_preds.view(-1, 1), labels.view(-1, 1))\n","\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","        # preds.append(y_preds.to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    predictions = np.concatenate(predictions)\n","    return losses.avg, predictions\n","\n","\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9fNekOSN1yT"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","    \n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds['score'].values\n","    \n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    \n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    # criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","    # criterion = FocalLossV1(alpha=CFG.alpha, gamma=CFG.gamma, reduction=CFG.focal_reduction).cuda()\n","    # criterion1 = FocalLossWithSmoothing(5, gamma=CFG.gamma, lb_smooth=CFG.lb_smooth, alpha=CFG.alpha)\n","    # criterion = LabelSmoothSoftmaxCE(lb_smooth=CFG.lb_smooth, loss_func=criterion1)\n","    criterion = SmoothFocalLoss(reduction=CFG.focal_reduction, alpha=CFG.alpha, gamma=CFG.gamma, smoothing=CFG.lb_smooth)\n","\n","    best_score = 0.\n","\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","        \n","        # scoring\n","        score = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n","                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","        \n","        if best_score < score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds['pred'] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return valid_folds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["a5c89f18ef9142cab33157888e4be413","369c8e5b28ab47f2ae3e1969442290e6"]},"id":"GIQnVGWmN1vf","outputId":"8022dc78-df8b-47a5-86e1-a65b325c82be"},"outputs":[{"name":"stderr","output_type":"stream","text":["========== fold: 0 training ==========\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5c89f18ef9142cab33157888e4be413","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/833M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/3291] Elapsed 0m 0s (remain 47m 14s) Loss: 0.0251(0.0251) Grad: 22101.0254  LR: 0.00000040  \n","Epoch: [1][100/3291] Elapsed 0m 26s (remain 13m 41s) Loss: 0.0283(0.0242) Grad: 22606.3262  LR: 0.00002000  \n","Epoch: [1][200/3291] Elapsed 0m 51s (remain 13m 6s) Loss: 0.0231(0.0229) Grad: 20213.4941  LR: 0.00002000  \n","Epoch: [1][300/3291] Elapsed 1m 16s (remain 12m 35s) Loss: 0.0155(0.0222) Grad: 6187.1982  LR: 0.00001999  \n","Epoch: [1][400/3291] Elapsed 1m 40s (remain 12m 6s) Loss: 0.0177(0.0216) Grad: 4166.5762  LR: 0.00001998  \n","Epoch: [1][500/3291] Elapsed 2m 5s (remain 11m 39s) Loss: 0.0169(0.0213) Grad: 6385.3721  LR: 0.00001996  \n","Epoch: [1][600/3291] Elapsed 2m 30s (remain 11m 11s) Loss: 0.0144(0.0210) Grad: 6117.7583  LR: 0.00001994  \n","Epoch: [1][700/3291] Elapsed 2m 54s (remain 10m 45s) Loss: 0.0192(0.0208) Grad: 7379.9224  LR: 0.00001992  \n","Epoch: [1][800/3291] Elapsed 3m 19s (remain 10m 19s) Loss: 0.0242(0.0206) Grad: 13564.5889  LR: 0.00001990  \n","Epoch: [1][900/3291] Elapsed 3m 43s (remain 9m 53s) Loss: 0.0125(0.0204) Grad: 6585.8149  LR: 0.00001987  \n","Epoch: [1][1000/3291] Elapsed 4m 8s (remain 9m 28s) Loss: 0.0215(0.0203) Grad: 6698.2056  LR: 0.00001983  \n","Epoch: [1][1100/3291] Elapsed 4m 32s (remain 9m 3s) Loss: 0.0139(0.0203) Grad: 10070.2158  LR: 0.00001980  \n","Epoch: [1][1200/3291] Elapsed 4m 57s (remain 8m 37s) Loss: 0.0187(0.0202) Grad: 4131.4727  LR: 0.00001976  \n","Epoch: [1][1300/3291] Elapsed 5m 22s (remain 8m 12s) Loss: 0.0259(0.0201) Grad: 34703.2305  LR: 0.00001971  \n","Epoch: [1][1400/3291] Elapsed 5m 46s (remain 7m 47s) Loss: 0.0121(0.0200) Grad: 4927.7793  LR: 0.00001967  \n","Epoch: [1][1500/3291] Elapsed 6m 11s (remain 7m 22s) Loss: 0.0230(0.0199) Grad: 31468.7109  LR: 0.00001962  \n","Epoch: [1][1600/3291] Elapsed 6m 36s (remain 6m 58s) Loss: 0.0176(0.0198) Grad: 5754.1982  LR: 0.00001956  \n","Epoch: [1][1700/3291] Elapsed 7m 0s (remain 6m 33s) Loss: 0.0179(0.0198) Grad: 8623.6543  LR: 0.00001950  \n","Epoch: [1][1800/3291] Elapsed 7m 25s (remain 6m 8s) Loss: 0.0169(0.0197) Grad: 14508.1006  LR: 0.00001944  \n","Epoch: [1][1900/3291] Elapsed 7m 50s (remain 5m 43s) Loss: 0.0188(0.0197) Grad: 3321.5735  LR: 0.00001938  \n","Epoch: [1][2000/3291] Elapsed 8m 15s (remain 5m 19s) Loss: 0.0223(0.0197) Grad: 21754.3926  LR: 0.00001931  \n","Epoch: [1][2100/3291] Elapsed 8m 39s (remain 4m 54s) Loss: 0.0124(0.0196) Grad: 13558.5195  LR: 0.00001924  \n","Epoch: [1][2200/3291] Elapsed 9m 4s (remain 4m 29s) Loss: 0.0189(0.0196) Grad: 18834.0000  LR: 0.00001916  \n","Epoch: [1][2300/3291] Elapsed 9m 29s (remain 4m 4s) Loss: 0.0146(0.0195) Grad: 6847.7119  LR: 0.00001909  \n","Epoch: [1][2400/3291] Elapsed 9m 53s (remain 3m 40s) Loss: 0.0212(0.0195) Grad: 7790.7065  LR: 0.00001900  \n","Epoch: [1][2500/3291] Elapsed 10m 18s (remain 3m 15s) Loss: 0.0196(0.0195) Grad: 13308.2373  LR: 0.00001892  \n","Epoch: [1][2600/3291] Elapsed 10m 42s (remain 2m 50s) Loss: 0.0124(0.0194) Grad: 6425.8750  LR: 0.00001883  \n","Epoch: [1][2700/3291] Elapsed 11m 8s (remain 2m 25s) Loss: 0.0186(0.0194) Grad: 12074.4844  LR: 0.00001874  \n","Epoch: [1][2800/3291] Elapsed 11m 32s (remain 2m 1s) Loss: 0.0180(0.0194) Grad: 9927.8799  LR: 0.00001864  \n","Epoch: [1][2900/3291] Elapsed 11m 56s (remain 1m 36s) Loss: 0.0193(0.0193) Grad: 8404.2988  LR: 0.00001855  \n","Epoch: [1][3000/3291] Elapsed 12m 21s (remain 1m 11s) Loss: 0.0225(0.0193) Grad: 13566.6523  LR: 0.00001845  \n","Epoch: [1][3100/3291] Elapsed 12m 45s (remain 0m 46s) Loss: 0.0180(0.0193) Grad: 12006.3408  LR: 0.00001834  \n","Epoch: [1][3200/3291] Elapsed 13m 10s (remain 0m 22s) Loss: 0.0207(0.0193) Grad: 15618.2139  LR: 0.00001823  \n","Epoch: [1][3290/3291] Elapsed 13m 32s (remain 0m 0s) Loss: 0.0114(0.0192) Grad: 11182.4580  LR: 0.00001814  \n","EVAL: [0/356] Elapsed 0m 0s (remain 2m 2s) Loss: 0.0175(0.0175) \n","EVAL: [100/356] Elapsed 0m 11s (remain 0m 27s) Loss: 0.0242(0.0197) \n","EVAL: [200/356] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0172(0.0198) \n","EVAL: [300/356] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0158(0.0197) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0192  avg_val_loss: 0.0198  time: 850s\n","Epoch 1 - Score: 0.8147\n","Epoch 1 - Save Best Score: 0.8147 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [355/356] Elapsed 0m 38s (remain 0m 0s) Loss: 0.0265(0.0198) \n","Epoch: [2][0/3291] Elapsed 0m 0s (remain 32m 9s) Loss: 0.0165(0.0165) Grad: 4078.5024  LR: 0.00001813  \n","Epoch: [2][100/3291] Elapsed 0m 26s (remain 14m 1s) Loss: 0.0138(0.0180) Grad: 4379.3188  LR: 0.00001802  \n","Epoch: [2][200/3291] Elapsed 0m 51s (remain 13m 13s) Loss: 0.0143(0.0180) Grad: 2913.8406  LR: 0.00001791  \n","Epoch: [2][300/3291] Elapsed 1m 16s (remain 12m 37s) Loss: 0.0106(0.0178) Grad: 4123.9033  LR: 0.00001779  \n","Epoch: [2][400/3291] Elapsed 1m 40s (remain 12m 6s) Loss: 0.0179(0.0180) Grad: 7827.3438  LR: 0.00001767  \n","Epoch: [2][500/3291] Elapsed 2m 5s (remain 11m 37s) Loss: 0.0200(0.0179) Grad: 4511.3730  LR: 0.00001754  \n","Epoch: [2][600/3291] Elapsed 2m 29s (remain 11m 10s) Loss: 0.0146(0.0179) Grad: 4304.8013  LR: 0.00001741  \n","Epoch: [2][700/3291] Elapsed 2m 54s (remain 10m 44s) Loss: 0.0198(0.0179) Grad: 2623.2856  LR: 0.00001728  \n","Epoch: [2][800/3291] Elapsed 3m 18s (remain 10m 18s) Loss: 0.0169(0.0179) Grad: 5150.0884  LR: 0.00001715  \n","Epoch: [2][900/3291] Elapsed 3m 43s (remain 9m 52s) Loss: 0.0136(0.0179) Grad: 6253.2788  LR: 0.00001702  \n","Epoch: [2][1000/3291] Elapsed 4m 7s (remain 9m 27s) Loss: 0.0130(0.0180) Grad: 5825.1807  LR: 0.00001688  \n","Epoch: [2][1100/3291] Elapsed 4m 32s (remain 9m 2s) Loss: 0.0160(0.0180) Grad: 8736.9375  LR: 0.00001674  \n","Epoch: [2][1200/3291] Elapsed 4m 57s (remain 8m 36s) Loss: 0.0197(0.0180) Grad: 4558.9883  LR: 0.00001660  \n","Epoch: [2][1300/3291] Elapsed 5m 22s (remain 8m 12s) Loss: 0.0126(0.0180) Grad: 3491.2744  LR: 0.00001645  \n","Epoch: [2][1400/3291] Elapsed 5m 46s (remain 7m 47s) Loss: 0.0165(0.0180) Grad: 7151.0474  LR: 0.00001630  \n","Epoch: [2][1500/3291] Elapsed 6m 11s (remain 7m 23s) Loss: 0.0190(0.0180) Grad: 4423.8433  LR: 0.00001615  \n","Epoch: [2][1600/3291] Elapsed 6m 36s (remain 6m 58s) Loss: 0.0175(0.0180) Grad: 5621.6934  LR: 0.00001600  \n","Epoch: [2][1700/3291] Elapsed 7m 1s (remain 6m 33s) Loss: 0.0169(0.0180) Grad: 3640.0642  LR: 0.00001585  \n","Epoch: [2][1800/3291] Elapsed 7m 26s (remain 6m 9s) Loss: 0.0156(0.0180) Grad: 5729.3428  LR: 0.00001569  \n","Epoch: [2][1900/3291] Elapsed 7m 50s (remain 5m 44s) Loss: 0.0192(0.0180) Grad: 7989.5815  LR: 0.00001553  \n","Epoch: [2][2000/3291] Elapsed 8m 15s (remain 5m 19s) Loss: 0.0223(0.0180) Grad: 13746.9609  LR: 0.00001537  \n","Epoch: [2][2100/3291] Elapsed 8m 39s (remain 4m 54s) Loss: 0.0207(0.0180) Grad: 11948.2656  LR: 0.00001521  \n","Epoch: [2][2200/3291] Elapsed 9m 4s (remain 4m 29s) Loss: 0.0097(0.0180) Grad: 4592.4478  LR: 0.00001504  \n","Epoch: [2][2300/3291] Elapsed 9m 28s (remain 4m 4s) Loss: 0.0244(0.0180) Grad: 14394.8213  LR: 0.00001488  \n","Epoch: [2][2400/3291] Elapsed 9m 52s (remain 3m 39s) Loss: 0.0188(0.0180) Grad: 6229.1382  LR: 0.00001471  \n","Epoch: [2][2500/3291] Elapsed 10m 17s (remain 3m 14s) Loss: 0.0183(0.0180) Grad: 11742.0771  LR: 0.00001454  \n","Epoch: [2][2600/3291] Elapsed 10m 41s (remain 2m 50s) Loss: 0.0164(0.0180) Grad: 21448.1016  LR: 0.00001437  \n","Epoch: [2][2700/3291] Elapsed 11m 5s (remain 2m 25s) Loss: 0.0188(0.0180) Grad: 7995.4639  LR: 0.00001420  \n","Epoch: [2][2800/3291] Elapsed 11m 30s (remain 2m 0s) Loss: 0.0216(0.0180) Grad: 7460.2861  LR: 0.00001402  \n","Epoch: [2][2900/3291] Elapsed 11m 54s (remain 1m 36s) Loss: 0.0133(0.0179) Grad: 4991.0029  LR: 0.00001384  \n","Epoch: [2][3000/3291] Elapsed 12m 19s (remain 1m 11s) Loss: 0.0205(0.0179) Grad: 23323.4805  LR: 0.00001367  \n","Epoch: [2][3100/3291] Elapsed 12m 43s (remain 0m 46s) Loss: 0.0121(0.0179) Grad: 14053.9033  LR: 0.00001349  \n","Epoch: [2][3200/3291] Elapsed 13m 7s (remain 0m 22s) Loss: 0.0133(0.0179) Grad: 4392.5161  LR: 0.00001331  \n","Epoch: [2][3290/3291] Elapsed 13m 29s (remain 0m 0s) Loss: 0.0177(0.0179) Grad: 6298.6602  LR: 0.00001315  \n","EVAL: [0/356] Elapsed 0m 0s (remain 1m 53s) Loss: 0.0176(0.0176) \n","EVAL: [100/356] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0230(0.0200) \n","EVAL: [200/356] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0163(0.0200) \n","EVAL: [300/356] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0156(0.0198) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0179  avg_val_loss: 0.0197  time: 848s\n","Epoch 2 - Score: 0.8203\n","Epoch 2 - Save Best Score: 0.8203 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [355/356] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0247(0.0197) \n","Epoch: [3][0/3291] Elapsed 0m 0s (remain 32m 22s) Loss: 0.0198(0.0198) Grad: 3803.5908  LR: 0.00001314  \n","Epoch: [3][100/3291] Elapsed 0m 26s (remain 13m 48s) Loss: 0.0162(0.0173) Grad: 3997.4282  LR: 0.00001296  \n","Epoch: [3][200/3291] Elapsed 0m 51s (remain 13m 4s) Loss: 0.0160(0.0172) Grad: 1700.9592  LR: 0.00001278  \n","Epoch: [3][300/3291] Elapsed 1m 15s (remain 12m 29s) Loss: 0.0145(0.0170) Grad: 3331.5979  LR: 0.00001259  \n","Epoch: [3][400/3291] Elapsed 1m 39s (remain 11m 58s) Loss: 0.0181(0.0170) Grad: 8667.0439  LR: 0.00001241  \n","Epoch: [3][500/3291] Elapsed 2m 4s (remain 11m 31s) Loss: 0.0088(0.0169) Grad: 4831.6636  LR: 0.00001222  \n","Epoch: [3][600/3291] Elapsed 2m 28s (remain 11m 5s) Loss: 0.0109(0.0169) Grad: 8013.0469  LR: 0.00001203  \n","Epoch: [3][700/3291] Elapsed 2m 53s (remain 10m 40s) Loss: 0.0175(0.0169) Grad: 2318.6128  LR: 0.00001185  \n","Epoch: [3][800/3291] Elapsed 3m 18s (remain 10m 16s) Loss: 0.0151(0.0171) Grad: 4173.9790  LR: 0.00001166  \n","Epoch: [3][900/3291] Elapsed 3m 43s (remain 9m 52s) Loss: 0.0119(0.0171) Grad: 1426.3306  LR: 0.00001147  \n","Epoch: [3][1000/3291] Elapsed 4m 8s (remain 9m 27s) Loss: 0.0159(0.0171) Grad: 2523.0569  LR: 0.00001128  \n","Epoch: [3][1100/3291] Elapsed 4m 33s (remain 9m 3s) Loss: 0.0120(0.0171) Grad: 10611.2559  LR: 0.00001109  \n","Epoch: [3][1200/3291] Elapsed 4m 58s (remain 8m 38s) Loss: 0.0182(0.0171) Grad: 6666.0493  LR: 0.00001090  \n","Epoch: [3][1300/3291] Elapsed 5m 23s (remain 8m 14s) Loss: 0.0166(0.0171) Grad: 4514.7334  LR: 0.00001071  \n","Epoch: [3][1400/3291] Elapsed 5m 48s (remain 7m 49s) Loss: 0.0312(0.0171) Grad: 46654.7227  LR: 0.00001052  \n","Epoch: [3][1500/3291] Elapsed 6m 13s (remain 7m 24s) Loss: 0.0163(0.0171) Grad: 9060.0527  LR: 0.00001033  \n","Epoch: [3][1600/3291] Elapsed 6m 38s (remain 7m 0s) Loss: 0.0123(0.0171) Grad: 1278.9814  LR: 0.00001013  \n","Epoch: [3][1700/3291] Elapsed 7m 2s (remain 6m 35s) Loss: 0.0205(0.0171) Grad: 24857.7324  LR: 0.00000994  \n","Epoch: [3][1800/3291] Elapsed 7m 27s (remain 6m 10s) Loss: 0.0138(0.0171) Grad: 3504.1101  LR: 0.00000975  \n","Epoch: [3][1900/3291] Elapsed 7m 52s (remain 5m 45s) Loss: 0.0126(0.0171) Grad: 5086.3687  LR: 0.00000956  \n","Epoch: [3][2000/3291] Elapsed 8m 17s (remain 5m 20s) Loss: 0.0150(0.0171) Grad: 4194.8828  LR: 0.00000937  \n","Epoch: [3][2100/3291] Elapsed 8m 42s (remain 4m 55s) Loss: 0.0153(0.0171) Grad: 18658.5605  LR: 0.00000918  \n","Epoch: [3][2200/3291] Elapsed 9m 7s (remain 4m 31s) Loss: 0.0221(0.0171) Grad: 14907.2471  LR: 0.00000899  \n","Epoch: [3][2300/3291] Elapsed 9m 31s (remain 4m 6s) Loss: 0.0205(0.0171) Grad: 18869.8828  LR: 0.00000880  \n","Epoch: [3][2400/3291] Elapsed 9m 56s (remain 3m 41s) Loss: 0.0147(0.0171) Grad: 7784.2686  LR: 0.00000861  \n","Epoch: [3][2500/3291] Elapsed 10m 21s (remain 3m 16s) Loss: 0.0136(0.0171) Grad: 7176.6182  LR: 0.00000842  \n","Epoch: [3][2600/3291] Elapsed 10m 46s (remain 2m 51s) Loss: 0.0179(0.0171) Grad: 8485.5723  LR: 0.00000823  \n","Epoch: [3][2700/3291] Elapsed 11m 10s (remain 2m 26s) Loss: 0.0152(0.0171) Grad: 17448.7910  LR: 0.00000804  \n","Epoch: [3][2800/3291] Elapsed 11m 35s (remain 2m 1s) Loss: 0.0121(0.0171) Grad: 7101.1104  LR: 0.00000785  \n","Epoch: [3][2900/3291] Elapsed 12m 0s (remain 1m 36s) Loss: 0.0140(0.0171) Grad: 7220.3389  LR: 0.00000767  \n","Epoch: [3][3000/3291] Elapsed 12m 25s (remain 1m 12s) Loss: 0.0156(0.0171) Grad: 2988.9927  LR: 0.00000748  \n","Epoch: [3][3100/3291] Elapsed 12m 50s (remain 0m 47s) Loss: 0.0186(0.0171) Grad: 10675.7715  LR: 0.00000730  \n","Epoch: [3][3200/3291] Elapsed 13m 14s (remain 0m 22s) Loss: 0.0180(0.0171) Grad: 8395.6250  LR: 0.00000711  \n","Epoch: [3][3290/3291] Elapsed 13m 37s (remain 0m 0s) Loss: 0.0188(0.0171) Grad: 8266.2832  LR: 0.00000695  \n","EVAL: [0/356] Elapsed 0m 0s (remain 1m 54s) Loss: 0.0174(0.0174) \n","EVAL: [100/356] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0250(0.0199) \n","EVAL: [200/356] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0161(0.0201) \n","EVAL: [300/356] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0168(0.0199) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0171  avg_val_loss: 0.0201  time: 855s\n","Epoch 3 - Score: 0.8263\n","Epoch 3 - Save Best Score: 0.8263 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [355/356] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0274(0.0201) \n","Epoch: [4][0/3291] Elapsed 0m 0s (remain 31m 58s) Loss: 0.0187(0.0187) Grad: 3836.1038  LR: 0.00000695  \n","Epoch: [4][100/3291] Elapsed 0m 26s (remain 13m 50s) Loss: 0.0140(0.0169) Grad: 2222.6440  LR: 0.00000676  \n","Epoch: [4][200/3291] Elapsed 0m 51s (remain 13m 14s) Loss: 0.0198(0.0168) Grad: 5518.2812  LR: 0.00000658  \n","Epoch: [4][300/3291] Elapsed 1m 16s (remain 12m 38s) Loss: 0.0215(0.0168) Grad: 6443.7256  LR: 0.00000640  \n","Epoch: [4][400/3291] Elapsed 1m 41s (remain 12m 8s) Loss: 0.0184(0.0168) Grad: 8080.0571  LR: 0.00000623  \n","Epoch: [4][500/3291] Elapsed 2m 5s (remain 11m 39s) Loss: 0.0216(0.0167) Grad: 7563.5776  LR: 0.00000605  \n","Epoch: [4][600/3291] Elapsed 2m 30s (remain 11m 13s) Loss: 0.0188(0.0167) Grad: 3400.2495  LR: 0.00000587  \n","Epoch: [4][700/3291] Elapsed 2m 54s (remain 10m 46s) Loss: 0.0132(0.0166) Grad: 3478.6519  LR: 0.00000570  \n","Epoch: [4][800/3291] Elapsed 3m 19s (remain 10m 20s) Loss: 0.0193(0.0167) Grad: 5821.6582  LR: 0.00000553  \n","Epoch: [4][900/3291] Elapsed 3m 44s (remain 9m 54s) Loss: 0.0174(0.0167) Grad: 2081.5273  LR: 0.00000536  \n","Epoch: [4][1000/3291] Elapsed 4m 9s (remain 9m 29s) Loss: 0.0197(0.0167) Grad: 6889.6416  LR: 0.00000519  \n","Epoch: [4][1100/3291] Elapsed 4m 33s (remain 9m 4s) Loss: 0.0205(0.0167) Grad: 6145.1216  LR: 0.00000502  \n","Epoch: [4][1200/3291] Elapsed 4m 58s (remain 8m 39s) Loss: 0.0172(0.0167) Grad: 3614.5781  LR: 0.00000486  \n","Epoch: [4][1300/3291] Elapsed 5m 23s (remain 8m 14s) Loss: 0.0121(0.0167) Grad: 3035.7471  LR: 0.00000469  \n","Epoch: [4][1400/3291] Elapsed 5m 48s (remain 7m 49s) Loss: 0.0118(0.0167) Grad: 1866.4967  LR: 0.00000453  \n","Epoch: [4][1500/3291] Elapsed 6m 12s (remain 7m 24s) Loss: 0.0158(0.0167) Grad: 1496.7667  LR: 0.00000437  \n","Epoch: [4][1600/3291] Elapsed 6m 37s (remain 6m 59s) Loss: 0.0159(0.0167) Grad: 4530.5889  LR: 0.00000422  \n","Epoch: [4][1700/3291] Elapsed 7m 1s (remain 6m 34s) Loss: 0.0164(0.0167) Grad: 4406.1108  LR: 0.00000406  \n","Epoch: [4][1800/3291] Elapsed 7m 26s (remain 6m 9s) Loss: 0.0179(0.0167) Grad: 3390.9780  LR: 0.00000391  \n","Epoch: [4][1900/3291] Elapsed 7m 51s (remain 5m 44s) Loss: 0.0138(0.0167) Grad: 4243.9097  LR: 0.00000376  \n","Epoch: [4][2000/3291] Elapsed 8m 16s (remain 5m 19s) Loss: 0.0210(0.0166) Grad: 3999.1938  LR: 0.00000361  \n","Epoch: [4][2100/3291] Elapsed 8m 40s (remain 4m 55s) Loss: 0.0123(0.0166) Grad: 8928.0469  LR: 0.00000346  \n","Epoch: [4][2200/3291] Elapsed 9m 5s (remain 4m 30s) Loss: 0.0102(0.0166) Grad: 6757.1284  LR: 0.00000332  \n","Epoch: [4][2300/3291] Elapsed 9m 30s (remain 4m 5s) Loss: 0.0190(0.0166) Grad: 4177.8101  LR: 0.00000318  \n","Epoch: [4][2400/3291] Elapsed 9m 55s (remain 3m 40s) Loss: 0.0091(0.0166) Grad: 1730.5330  LR: 0.00000304  \n","Epoch: [4][2500/3291] Elapsed 10m 19s (remain 3m 15s) Loss: 0.0193(0.0166) Grad: 11342.6582  LR: 0.00000290  \n","Epoch: [4][2600/3291] Elapsed 10m 44s (remain 2m 51s) Loss: 0.0178(0.0166) Grad: 1246.6373  LR: 0.00000277  \n","Epoch: [4][2700/3291] Elapsed 11m 9s (remain 2m 26s) Loss: 0.0171(0.0166) Grad: 9335.1689  LR: 0.00000264  \n","Epoch: [4][2800/3291] Elapsed 11m 34s (remain 2m 1s) Loss: 0.0218(0.0166) Grad: 24050.6504  LR: 0.00000251  \n","Epoch: [4][2900/3291] Elapsed 11m 58s (remain 1m 36s) Loss: 0.0202(0.0166) Grad: 4260.7397  LR: 0.00000238  \n","Epoch: [4][3000/3291] Elapsed 12m 22s (remain 1m 11s) Loss: 0.0093(0.0166) Grad: 5150.5332  LR: 0.00000226  \n","Epoch: [4][3100/3291] Elapsed 12m 47s (remain 0m 46s) Loss: 0.0216(0.0166) Grad: 26234.9414  LR: 0.00000214  \n","Epoch: [4][3200/3291] Elapsed 13m 11s (remain 0m 22s) Loss: 0.0155(0.0166) Grad: 7088.2617  LR: 0.00000202  \n","Epoch: [4][3290/3291] Elapsed 13m 33s (remain 0m 0s) Loss: 0.0167(0.0166) Grad: 7810.5962  LR: 0.00000192  \n","EVAL: [0/356] Elapsed 0m 0s (remain 1m 53s) Loss: 0.0178(0.0178) \n","EVAL: [100/356] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0236(0.0198) \n","EVAL: [200/356] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0161(0.0201) \n","EVAL: [300/356] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0163(0.0199) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0166  avg_val_loss: 0.0201  time: 852s\n","Epoch 4 - Score: 0.8242\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [355/356] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0272(0.0201) \n","Epoch: [5][0/3291] Elapsed 0m 0s (remain 32m 34s) Loss: 0.0125(0.0125) Grad: 2906.0417  LR: 0.00000192  \n","Epoch: [5][100/3291] Elapsed 0m 25s (remain 13m 26s) Loss: 0.0165(0.0165) Grad: 2267.2971  LR: 0.00000181  \n","Epoch: [5][200/3291] Elapsed 0m 50s (remain 12m 55s) Loss: 0.0150(0.0162) Grad: 1543.8174  LR: 0.00000170  \n","Epoch: [5][300/3291] Elapsed 1m 15s (remain 12m 28s) Loss: 0.0133(0.0165) Grad: 3680.7012  LR: 0.00000160  \n","Epoch: [5][400/3291] Elapsed 1m 39s (remain 12m 0s) Loss: 0.0193(0.0164) Grad: 2993.8901  LR: 0.00000149  \n","Epoch: [5][500/3291] Elapsed 2m 4s (remain 11m 33s) Loss: 0.0143(0.0165) Grad: 1582.6245  LR: 0.00000139  \n","Epoch: [5][600/3291] Elapsed 2m 29s (remain 11m 8s) Loss: 0.0154(0.0165) Grad: 3737.0513  LR: 0.00000130  \n","Epoch: [5][700/3291] Elapsed 2m 54s (remain 10m 43s) Loss: 0.0102(0.0164) Grad: 2982.9150  LR: 0.00000121  \n","Epoch: [5][800/3291] Elapsed 3m 18s (remain 10m 17s) Loss: 0.0192(0.0164) Grad: 3807.2512  LR: 0.00000112  \n","Epoch: [5][900/3291] Elapsed 3m 43s (remain 9m 52s) Loss: 0.0147(0.0164) Grad: 5164.7637  LR: 0.00000103  \n","Epoch: [5][1000/3291] Elapsed 4m 8s (remain 9m 27s) Loss: 0.0135(0.0164) Grad: 5068.2290  LR: 0.00000095  \n","Epoch: [5][1100/3291] Elapsed 4m 33s (remain 9m 3s) Loss: 0.0206(0.0164) Grad: 3290.5676  LR: 0.00000087  \n","Epoch: [5][1200/3291] Elapsed 4m 58s (remain 8m 39s) Loss: 0.0161(0.0164) Grad: 2438.4443  LR: 0.00000079  \n","Epoch: [5][1300/3291] Elapsed 5m 23s (remain 8m 14s) Loss: 0.0146(0.0164) Grad: 1475.3647  LR: 0.00000072  \n","Epoch: [5][1400/3291] Elapsed 5m 48s (remain 7m 49s) Loss: 0.0163(0.0164) Grad: 6614.4072  LR: 0.00000065  \n","Epoch: [5][1500/3291] Elapsed 6m 12s (remain 7m 24s) Loss: 0.0165(0.0164) Grad: 1110.1202  LR: 0.00000058  \n","Epoch: [5][1600/3291] Elapsed 6m 37s (remain 6m 59s) Loss: 0.0156(0.0164) Grad: 2448.0320  LR: 0.00000052  \n","Epoch: [5][1700/3291] Elapsed 7m 2s (remain 6m 34s) Loss: 0.0230(0.0164) Grad: 5258.0952  LR: 0.00000046  \n","Epoch: [5][1800/3291] Elapsed 7m 26s (remain 6m 9s) Loss: 0.0168(0.0164) Grad: 1859.9049  LR: 0.00000040  \n","Epoch: [5][1900/3291] Elapsed 7m 51s (remain 5m 44s) Loss: 0.0123(0.0164) Grad: 2356.3279  LR: 0.00000035  \n","Epoch: [5][2000/3291] Elapsed 8m 15s (remain 5m 19s) Loss: 0.0134(0.0164) Grad: 3876.1895  LR: 0.00000030  \n","Epoch: [5][2100/3291] Elapsed 8m 40s (remain 4m 54s) Loss: 0.0161(0.0164) Grad: 8911.9873  LR: 0.00000026  \n","Epoch: [5][2200/3291] Elapsed 9m 5s (remain 4m 29s) Loss: 0.0142(0.0164) Grad: 10482.6504  LR: 0.00000022  \n","Epoch: [5][2300/3291] Elapsed 9m 29s (remain 4m 5s) Loss: 0.0143(0.0164) Grad: 4683.8657  LR: 0.00000018  \n","Epoch: [5][2400/3291] Elapsed 9m 54s (remain 3m 40s) Loss: 0.0168(0.0164) Grad: 1343.3904  LR: 0.00000015  \n","Epoch: [5][2500/3291] Elapsed 10m 19s (remain 3m 15s) Loss: 0.0179(0.0164) Grad: 10252.5283  LR: 0.00000011  \n","Epoch: [5][2600/3291] Elapsed 10m 44s (remain 2m 50s) Loss: 0.0179(0.0164) Grad: 1214.9083  LR: 0.00000009  \n","Epoch: [5][2700/3291] Elapsed 11m 9s (remain 2m 26s) Loss: 0.0178(0.0164) Grad: 1415.1201  LR: 0.00000006  \n","Epoch: [5][2800/3291] Elapsed 11m 33s (remain 2m 1s) Loss: 0.0199(0.0164) Grad: 13609.6045  LR: 0.00000004  \n","Epoch: [5][2900/3291] Elapsed 11m 58s (remain 1m 36s) Loss: 0.0189(0.0164) Grad: 2986.8523  LR: 0.00000003  \n","Epoch: [5][3000/3291] Elapsed 12m 23s (remain 1m 11s) Loss: 0.0185(0.0164) Grad: 6410.8320  LR: 0.00000002  \n","Epoch: [5][3100/3291] Elapsed 12m 47s (remain 0m 47s) Loss: 0.0179(0.0164) Grad: 2791.5996  LR: 0.00000001  \n","Epoch: [5][3200/3291] Elapsed 13m 12s (remain 0m 22s) Loss: 0.0184(0.0164) Grad: 4992.2393  LR: 0.00000000  \n","Epoch: [5][3290/3291] Elapsed 13m 34s (remain 0m 0s) Loss: 0.0182(0.0164) Grad: 7678.2573  LR: 0.00000000  \n","EVAL: [0/356] Elapsed 0m 0s (remain 1m 53s) Loss: 0.0179(0.0179) \n","EVAL: [100/356] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0236(0.0199) \n","EVAL: [200/356] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0158(0.0203) \n","EVAL: [300/356] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0164(0.0201) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0164  avg_val_loss: 0.0203  time: 853s\n","Epoch 5 - Score: 0.8227\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [355/356] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0270(0.0203) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 0 result ==========\n","Score: 0.8263\n","========== fold: 1 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/3247] Elapsed 0m 0s (remain 28m 5s) Loss: 0.0581(0.0581) Grad: 93435.6406  LR: 0.00000040  \n","Epoch: [1][100/3247] Elapsed 0m 25s (remain 13m 2s) Loss: 0.0304(0.0317) Grad: 26448.5156  LR: 0.00002000  \n","Epoch: [1][200/3247] Elapsed 0m 49s (remain 12m 33s) Loss: 0.0206(0.0268) Grad: 7907.8369  LR: 0.00002000  \n","Epoch: [1][300/3247] Elapsed 1m 14s (remain 12m 7s) Loss: 0.0244(0.0248) Grad: 17253.2344  LR: 0.00001999  \n","Epoch: [1][400/3247] Elapsed 1m 39s (remain 11m 42s) Loss: 0.0178(0.0237) Grad: 8469.4473  LR: 0.00001998  \n","Epoch: [1][500/3247] Elapsed 2m 3s (remain 11m 17s) Loss: 0.0141(0.0231) Grad: 7788.2295  LR: 0.00001996  \n","Epoch: [1][600/3247] Elapsed 2m 28s (remain 10m 53s) Loss: 0.0184(0.0225) Grad: 6083.7754  LR: 0.00001994  \n","Epoch: [1][700/3247] Elapsed 2m 53s (remain 10m 28s) Loss: 0.0202(0.0222) Grad: 6475.0464  LR: 0.00001992  \n","Epoch: [1][800/3247] Elapsed 3m 17s (remain 10m 3s) Loss: 0.0109(0.0218) Grad: 4957.2227  LR: 0.00001989  \n","Epoch: [1][900/3247] Elapsed 3m 42s (remain 9m 39s) Loss: 0.0139(0.0215) Grad: 18765.3438  LR: 0.00001986  \n","Epoch: [1][1000/3247] Elapsed 4m 7s (remain 9m 14s) Loss: 0.0206(0.0214) Grad: 5669.0776  LR: 0.00001983  \n","Epoch: [1][1100/3247] Elapsed 4m 31s (remain 8m 49s) Loss: 0.0203(0.0212) Grad: 4764.8540  LR: 0.00001979  \n","Epoch: [1][1200/3247] Elapsed 4m 56s (remain 8m 24s) Loss: 0.0203(0.0210) Grad: 10061.3447  LR: 0.00001975  \n","Epoch: [1][1300/3247] Elapsed 5m 20s (remain 8m 0s) Loss: 0.0155(0.0209) Grad: 10625.1621  LR: 0.00001971  \n","Epoch: [1][1400/3247] Elapsed 5m 45s (remain 7m 35s) Loss: 0.0129(0.0208) Grad: 8974.7041  LR: 0.00001966  \n","Epoch: [1][1500/3247] Elapsed 6m 10s (remain 7m 10s) Loss: 0.0212(0.0207) Grad: 12024.7734  LR: 0.00001961  \n","Epoch: [1][1600/3247] Elapsed 6m 35s (remain 6m 46s) Loss: 0.0152(0.0205) Grad: 5422.0967  LR: 0.00001955  \n","Epoch: [1][1700/3247] Elapsed 7m 0s (remain 6m 22s) Loss: 0.0222(0.0205) Grad: 7819.1475  LR: 0.00001949  \n","Epoch: [1][1800/3247] Elapsed 7m 25s (remain 5m 57s) Loss: 0.0166(0.0204) Grad: 4803.1729  LR: 0.00001943  \n","Epoch: [1][1900/3247] Elapsed 7m 50s (remain 5m 32s) Loss: 0.0125(0.0203) Grad: 6552.0918  LR: 0.00001936  \n","Epoch: [1][2000/3247] Elapsed 8m 14s (remain 5m 8s) Loss: 0.0179(0.0203) Grad: 11864.4844  LR: 0.00001929  \n","Epoch: [1][2100/3247] Elapsed 8m 39s (remain 4m 43s) Loss: 0.0177(0.0202) Grad: 21956.2266  LR: 0.00001922  \n","Epoch: [1][2200/3247] Elapsed 9m 4s (remain 4m 18s) Loss: 0.0151(0.0201) Grad: 10379.7471  LR: 0.00001914  \n","Epoch: [1][2300/3247] Elapsed 9m 29s (remain 3m 54s) Loss: 0.0186(0.0201) Grad: 16144.4551  LR: 0.00001906  \n","Epoch: [1][2400/3247] Elapsed 9m 54s (remain 3m 29s) Loss: 0.0151(0.0200) Grad: 6144.1938  LR: 0.00001898  \n","Epoch: [1][2500/3247] Elapsed 10m 19s (remain 3m 4s) Loss: 0.0231(0.0199) Grad: 16550.3066  LR: 0.00001889  \n","Epoch: [1][2600/3247] Elapsed 10m 44s (remain 2m 40s) Loss: 0.0235(0.0199) Grad: 18877.2324  LR: 0.00001880  \n","Epoch: [1][2700/3247] Elapsed 11m 9s (remain 2m 15s) Loss: 0.0181(0.0198) Grad: 5320.6621  LR: 0.00001871  \n","Epoch: [1][2800/3247] Elapsed 11m 34s (remain 1m 50s) Loss: 0.0256(0.0198) Grad: 25687.1719  LR: 0.00001861  \n","Epoch: [1][2900/3247] Elapsed 11m 59s (remain 1m 25s) Loss: 0.0183(0.0197) Grad: 13828.5371  LR: 0.00001851  \n","Epoch: [1][3000/3247] Elapsed 12m 24s (remain 1m 1s) Loss: 0.0231(0.0197) Grad: 70872.6406  LR: 0.00001840  \n","Epoch: [1][3100/3247] Elapsed 12m 49s (remain 0m 36s) Loss: 0.0162(0.0196) Grad: 16222.7959  LR: 0.00001830  \n","Epoch: [1][3200/3247] Elapsed 13m 14s (remain 0m 11s) Loss: 0.0185(0.0196) Grad: 12304.5967  LR: 0.00001819  \n","Epoch: [1][3246/3247] Elapsed 13m 25s (remain 0m 0s) Loss: 0.0202(0.0196) Grad: 15279.3838  LR: 0.00001814  \n","EVAL: [0/400] Elapsed 0m 0s (remain 2m 19s) Loss: 0.0187(0.0187) \n","EVAL: [100/400] Elapsed 0m 10s (remain 0m 32s) Loss: 0.0154(0.0180) \n","EVAL: [200/400] Elapsed 0m 21s (remain 0m 21s) Loss: 0.0284(0.0190) \n","EVAL: [300/400] Elapsed 0m 32s (remain 0m 10s) Loss: 0.0140(0.0188) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0196  avg_val_loss: 0.0186  time: 849s\n","Epoch 1 - Score: 0.8202\n","Epoch 1 - Save Best Score: 0.8202 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [399/400] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0084(0.0186) \n","Epoch: [2][0/3247] Elapsed 0m 0s (remain 32m 22s) Loss: 0.0166(0.0166) Grad: 10440.9414  LR: 0.00001814  \n","Epoch: [2][100/3247] Elapsed 0m 26s (remain 13m 39s) Loss: 0.0216(0.0177) Grad: 5432.6729  LR: 0.00001802  \n","Epoch: [2][200/3247] Elapsed 0m 51s (remain 13m 4s) Loss: 0.0199(0.0179) Grad: 8215.6045  LR: 0.00001790  \n","Epoch: [2][300/3247] Elapsed 1m 16s (remain 12m 28s) Loss: 0.0193(0.0181) Grad: 8729.0801  LR: 0.00001778  \n","Epoch: [2][400/3247] Elapsed 1m 41s (remain 11m 57s) Loss: 0.0169(0.0180) Grad: 6890.3789  LR: 0.00001766  \n","Epoch: [2][500/3247] Elapsed 2m 6s (remain 11m 30s) Loss: 0.0231(0.0180) Grad: 22334.2891  LR: 0.00001753  \n","Epoch: [2][600/3247] Elapsed 2m 31s (remain 11m 4s) Loss: 0.0145(0.0180) Grad: 3635.9243  LR: 0.00001740  \n","Epoch: [2][700/3247] Elapsed 2m 56s (remain 10m 39s) Loss: 0.0146(0.0181) Grad: 5673.4370  LR: 0.00001727  \n","Epoch: [2][800/3247] Elapsed 3m 20s (remain 10m 12s) Loss: 0.0188(0.0182) Grad: 4125.0142  LR: 0.00001714  \n","Epoch: [2][900/3247] Elapsed 3m 45s (remain 9m 46s) Loss: 0.0225(0.0181) Grad: 14146.2607  LR: 0.00001700  \n","Epoch: [2][1000/3247] Elapsed 4m 9s (remain 9m 20s) Loss: 0.0179(0.0181) Grad: 14359.7832  LR: 0.00001686  \n","Epoch: [2][1100/3247] Elapsed 4m 35s (remain 8m 56s) Loss: 0.0163(0.0182) Grad: 6323.0898  LR: 0.00001672  \n","Epoch: [2][1200/3247] Elapsed 4m 59s (remain 8m 31s) Loss: 0.0207(0.0181) Grad: 6585.9399  LR: 0.00001657  \n","Epoch: [2][1300/3247] Elapsed 5m 25s (remain 8m 6s) Loss: 0.0208(0.0181) Grad: 4411.4531  LR: 0.00001643  \n","Epoch: [2][1400/3247] Elapsed 5m 49s (remain 7m 41s) Loss: 0.0189(0.0181) Grad: 3565.7412  LR: 0.00001628  \n","Epoch: [2][1500/3247] Elapsed 6m 14s (remain 7m 15s) Loss: 0.0179(0.0181) Grad: 6786.5879  LR: 0.00001612  \n","Epoch: [2][1600/3247] Elapsed 6m 38s (remain 6m 50s) Loss: 0.0175(0.0180) Grad: 2721.9619  LR: 0.00001597  \n","Epoch: [2][1700/3247] Elapsed 7m 3s (remain 6m 25s) Loss: 0.0145(0.0180) Grad: 2237.3467  LR: 0.00001581  \n","Epoch: [2][1800/3247] Elapsed 7m 28s (remain 6m 0s) Loss: 0.0208(0.0180) Grad: 4655.1104  LR: 0.00001565  \n","Epoch: [2][1900/3247] Elapsed 7m 53s (remain 5m 35s) Loss: 0.0237(0.0180) Grad: 15650.1914  LR: 0.00001549  \n","Epoch: [2][2000/3247] Elapsed 8m 19s (remain 5m 10s) Loss: 0.0167(0.0180) Grad: 13939.0635  LR: 0.00001533  \n","Epoch: [2][2100/3247] Elapsed 8m 44s (remain 4m 45s) Loss: 0.0114(0.0180) Grad: 2042.3204  LR: 0.00001516  \n","Epoch: [2][2200/3247] Elapsed 9m 9s (remain 4m 20s) Loss: 0.0153(0.0180) Grad: 12283.3701  LR: 0.00001500  \n","Epoch: [2][2300/3247] Elapsed 9m 33s (remain 3m 55s) Loss: 0.0211(0.0180) Grad: 8592.9365  LR: 0.00001483  \n","Epoch: [2][2400/3247] Elapsed 9m 58s (remain 3m 30s) Loss: 0.0122(0.0180) Grad: 35270.4805  LR: 0.00001466  \n","Epoch: [2][2500/3247] Elapsed 10m 23s (remain 3m 5s) Loss: 0.0221(0.0180) Grad: 5352.1133  LR: 0.00001448  \n","Epoch: [2][2600/3247] Elapsed 10m 48s (remain 2m 41s) Loss: 0.0175(0.0180) Grad: 12482.9199  LR: 0.00001431  \n","Epoch: [2][2700/3247] Elapsed 11m 13s (remain 2m 16s) Loss: 0.0126(0.0180) Grad: 10041.8145  LR: 0.00001413  \n","Epoch: [2][2800/3247] Elapsed 11m 38s (remain 1m 51s) Loss: 0.0147(0.0180) Grad: 10982.7021  LR: 0.00001396  \n","Epoch: [2][2900/3247] Elapsed 12m 3s (remain 1m 26s) Loss: 0.0134(0.0180) Grad: 7324.7949  LR: 0.00001378  \n","Epoch: [2][3000/3247] Elapsed 12m 28s (remain 1m 1s) Loss: 0.0156(0.0180) Grad: 6499.9912  LR: 0.00001360  \n","Epoch: [2][3100/3247] Elapsed 12m 53s (remain 0m 36s) Loss: 0.0198(0.0180) Grad: 7651.1846  LR: 0.00001342  \n","Epoch: [2][3200/3247] Elapsed 13m 17s (remain 0m 11s) Loss: 0.0158(0.0179) Grad: 7445.9102  LR: 0.00001323  \n","Epoch: [2][3246/3247] Elapsed 13m 28s (remain 0m 0s) Loss: 0.0146(0.0179) Grad: 4027.2588  LR: 0.00001315  \n","EVAL: [0/400] Elapsed 0m 0s (remain 2m 18s) Loss: 0.0182(0.0182) \n","EVAL: [100/400] Elapsed 0m 10s (remain 0m 32s) Loss: 0.0158(0.0184) \n","EVAL: [200/400] Elapsed 0m 21s (remain 0m 21s) Loss: 0.0259(0.0193) \n","EVAL: [300/400] Elapsed 0m 32s (remain 0m 10s) Loss: 0.0141(0.0189) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0179  avg_val_loss: 0.0188  time: 851s\n","Epoch 2 - Score: 0.8286\n","Epoch 2 - Save Best Score: 0.8286 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [399/400] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0053(0.0188) \n","Epoch: [3][0/3247] Elapsed 0m 3s (remain 208m 24s) Loss: 0.0220(0.0220) Grad: 5547.8784  LR: 0.00001315  \n","Epoch: [3][100/3247] Elapsed 0m 29s (remain 15m 19s) Loss: 0.0114(0.0169) Grad: 2018.4346  LR: 0.00001296  \n","Epoch: [3][200/3247] Elapsed 0m 54s (remain 13m 51s) Loss: 0.0189(0.0171) Grad: 14936.1768  LR: 0.00001278  \n","Epoch: [3][300/3247] Elapsed 1m 19s (remain 12m 59s) Loss: 0.0100(0.0172) Grad: 12599.3242  LR: 0.00001259  \n","Epoch: [3][400/3247] Elapsed 1m 44s (remain 12m 19s) Loss: 0.0180(0.0172) Grad: 3296.8718  LR: 0.00001240  \n","Epoch: [3][500/3247] Elapsed 2m 8s (remain 11m 46s) Loss: 0.0136(0.0172) Grad: 4726.8521  LR: 0.00001221  \n","Epoch: [3][600/3247] Elapsed 2m 33s (remain 11m 16s) Loss: 0.0174(0.0172) Grad: 2374.8210  LR: 0.00001202  \n","Epoch: [3][700/3247] Elapsed 2m 58s (remain 10m 48s) Loss: 0.0158(0.0173) Grad: 6590.3477  LR: 0.00001183  \n","Epoch: [3][800/3247] Elapsed 3m 23s (remain 10m 20s) Loss: 0.0193(0.0172) Grad: 10835.8135  LR: 0.00001164  \n","Epoch: [3][900/3247] Elapsed 3m 48s (remain 9m 53s) Loss: 0.0235(0.0173) Grad: 4610.5225  LR: 0.00001145  \n","Epoch: [3][1000/3247] Elapsed 4m 12s (remain 9m 27s) Loss: 0.0154(0.0173) Grad: 9494.8633  LR: 0.00001126  \n","Epoch: [3][1100/3247] Elapsed 4m 37s (remain 9m 0s) Loss: 0.0204(0.0173) Grad: 4725.9697  LR: 0.00001106  \n","Epoch: [3][1200/3247] Elapsed 5m 2s (remain 8m 34s) Loss: 0.0094(0.0173) Grad: 2956.8054  LR: 0.00001087  \n","Epoch: [3][1300/3247] Elapsed 5m 26s (remain 8m 8s) Loss: 0.0154(0.0173) Grad: 5832.6353  LR: 0.00001068  \n","Epoch: [3][1400/3247] Elapsed 5m 51s (remain 7m 42s) Loss: 0.0134(0.0173) Grad: 2563.5610  LR: 0.00001048  \n","Epoch: [3][1500/3247] Elapsed 6m 15s (remain 7m 17s) Loss: 0.0194(0.0173) Grad: 2871.4114  LR: 0.00001029  \n","Epoch: [3][1600/3247] Elapsed 6m 40s (remain 6m 51s) Loss: 0.0129(0.0173) Grad: 13520.1611  LR: 0.00001010  \n","Epoch: [3][1700/3247] Elapsed 7m 5s (remain 6m 26s) Loss: 0.0187(0.0173) Grad: 2279.0811  LR: 0.00000990  \n","Epoch: [3][1800/3247] Elapsed 7m 30s (remain 6m 1s) Loss: 0.0143(0.0173) Grad: 6814.6333  LR: 0.00000971  \n","Epoch: [3][1900/3247] Elapsed 7m 54s (remain 5m 36s) Loss: 0.0153(0.0173) Grad: 4867.5171  LR: 0.00000951  \n","Epoch: [3][2000/3247] Elapsed 8m 19s (remain 5m 11s) Loss: 0.0207(0.0173) Grad: 6889.2964  LR: 0.00000932  \n","Epoch: [3][2100/3247] Elapsed 8m 44s (remain 4m 45s) Loss: 0.0120(0.0173) Grad: 13316.3613  LR: 0.00000913  \n","Epoch: [3][2200/3247] Elapsed 9m 9s (remain 4m 20s) Loss: 0.0167(0.0173) Grad: 6339.0181  LR: 0.00000893  \n","Epoch: [3][2300/3247] Elapsed 9m 33s (remain 3m 55s) Loss: 0.0127(0.0173) Grad: 17072.5156  LR: 0.00000874  \n","Epoch: [3][2400/3247] Elapsed 9m 58s (remain 3m 30s) Loss: 0.0144(0.0173) Grad: 6703.2593  LR: 0.00000855  \n","Epoch: [3][2500/3247] Elapsed 10m 22s (remain 3m 5s) Loss: 0.0115(0.0173) Grad: 7635.1514  LR: 0.00000836  \n","Epoch: [3][2600/3247] Elapsed 10m 47s (remain 2m 40s) Loss: 0.0146(0.0172) Grad: 6444.1084  LR: 0.00000817  \n","Epoch: [3][2700/3247] Elapsed 11m 12s (remain 2m 15s) Loss: 0.0169(0.0172) Grad: 6224.8931  LR: 0.00000798  \n","Epoch: [3][2800/3247] Elapsed 11m 36s (remain 1m 50s) Loss: 0.0165(0.0172) Grad: 7966.4482  LR: 0.00000779  \n","Epoch: [3][2900/3247] Elapsed 12m 1s (remain 1m 26s) Loss: 0.0192(0.0172) Grad: 13328.0781  LR: 0.00000760  \n","Epoch: [3][3000/3247] Elapsed 12m 25s (remain 1m 1s) Loss: 0.0199(0.0172) Grad: 13022.6748  LR: 0.00000741  \n","Epoch: [3][3100/3247] Elapsed 12m 50s (remain 0m 36s) Loss: 0.0160(0.0172) Grad: 6936.9946  LR: 0.00000722  \n","Epoch: [3][3200/3247] Elapsed 13m 15s (remain 0m 11s) Loss: 0.0139(0.0172) Grad: 8267.6992  LR: 0.00000704  \n","Epoch: [3][3246/3247] Elapsed 13m 26s (remain 0m 0s) Loss: 0.0218(0.0172) Grad: 13795.7822  LR: 0.00000695  \n","EVAL: [0/400] Elapsed 0m 0s (remain 2m 20s) Loss: 0.0208(0.0208) \n","EVAL: [100/400] Elapsed 0m 10s (remain 0m 32s) Loss: 0.0153(0.0187) \n","EVAL: [200/400] Elapsed 0m 21s (remain 0m 21s) Loss: 0.0255(0.0195) \n","EVAL: [300/400] Elapsed 0m 32s (remain 0m 10s) Loss: 0.0141(0.0192) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0172  avg_val_loss: 0.0190  time: 850s\n","Epoch 3 - Score: 0.8324\n","Epoch 3 - Save Best Score: 0.8324 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [399/400] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0059(0.0190) \n","Epoch: [4][0/3247] Elapsed 0m 0s (remain 32m 48s) Loss: 0.0145(0.0145) Grad: 4581.9844  LR: 0.00000695  \n","Epoch: [4][100/3247] Elapsed 0m 26s (remain 13m 37s) Loss: 0.0150(0.0170) Grad: 1856.3745  LR: 0.00000677  \n","Epoch: [4][200/3247] Elapsed 0m 51s (remain 13m 1s) Loss: 0.0196(0.0171) Grad: 9049.6523  LR: 0.00000658  \n","Epoch: [4][300/3247] Elapsed 1m 16s (remain 12m 25s) Loss: 0.0184(0.0169) Grad: 5077.5337  LR: 0.00000640  \n","Epoch: [4][400/3247] Elapsed 1m 41s (remain 12m 0s) Loss: 0.0156(0.0169) Grad: 2034.2955  LR: 0.00000622  \n","Epoch: [4][500/3247] Elapsed 2m 6s (remain 11m 33s) Loss: 0.0123(0.0169) Grad: 2365.8513  LR: 0.00000604  \n","Epoch: [4][600/3247] Elapsed 2m 31s (remain 11m 7s) Loss: 0.0201(0.0169) Grad: 1944.8336  LR: 0.00000586  \n","Epoch: [4][700/3247] Elapsed 2m 56s (remain 10m 42s) Loss: 0.0158(0.0168) Grad: 995.9092  LR: 0.00000569  \n","Epoch: [4][800/3247] Elapsed 3m 21s (remain 10m 16s) Loss: 0.0183(0.0169) Grad: 4049.7434  LR: 0.00000551  \n","Epoch: [4][900/3247] Elapsed 3m 47s (remain 9m 51s) Loss: 0.0174(0.0169) Grad: 1792.4456  LR: 0.00000534  \n","Epoch: [4][1000/3247] Elapsed 4m 11s (remain 9m 25s) Loss: 0.0202(0.0168) Grad: 6504.1367  LR: 0.00000517  \n","Epoch: [4][1100/3247] Elapsed 4m 36s (remain 8m 59s) Loss: 0.0197(0.0169) Grad: 5674.3569  LR: 0.00000500  \n","Epoch: [4][1200/3247] Elapsed 5m 1s (remain 8m 33s) Loss: 0.0164(0.0169) Grad: 2043.0844  LR: 0.00000483  \n","Epoch: [4][1300/3247] Elapsed 5m 26s (remain 8m 8s) Loss: 0.0147(0.0169) Grad: 4939.1626  LR: 0.00000467  \n","Epoch: [4][1400/3247] Elapsed 5m 51s (remain 7m 43s) Loss: 0.0188(0.0168) Grad: 2825.8164  LR: 0.00000451  \n","Epoch: [4][1500/3247] Elapsed 6m 16s (remain 7m 18s) Loss: 0.0100(0.0168) Grad: 3312.5640  LR: 0.00000434  \n","Epoch: [4][1600/3247] Elapsed 6m 41s (remain 6m 52s) Loss: 0.0178(0.0169) Grad: 1936.9709  LR: 0.00000419  \n","Epoch: [4][1700/3247] Elapsed 7m 6s (remain 6m 27s) Loss: 0.0174(0.0169) Grad: 3973.4399  LR: 0.00000403  \n","Epoch: [4][1800/3247] Elapsed 7m 31s (remain 6m 2s) Loss: 0.0193(0.0168) Grad: 6314.8188  LR: 0.00000387  \n","Epoch: [4][1900/3247] Elapsed 7m 56s (remain 5m 37s) Loss: 0.0127(0.0168) Grad: 4007.1934  LR: 0.00000372  \n","Epoch: [4][2000/3247] Elapsed 8m 21s (remain 5m 12s) Loss: 0.0178(0.0168) Grad: 6403.4746  LR: 0.00000357  \n","Epoch: [4][2100/3247] Elapsed 8m 46s (remain 4m 46s) Loss: 0.0146(0.0168) Grad: 3277.3965  LR: 0.00000342  \n","Epoch: [4][2200/3247] Elapsed 9m 10s (remain 4m 21s) Loss: 0.0121(0.0168) Grad: 7450.2417  LR: 0.00000328  \n","Epoch: [4][2300/3247] Elapsed 9m 35s (remain 3m 56s) Loss: 0.0082(0.0168) Grad: 5007.0649  LR: 0.00000314  \n","Epoch: [4][2400/3247] Elapsed 10m 0s (remain 3m 31s) Loss: 0.0245(0.0168) Grad: 33123.2617  LR: 0.00000300  \n","Epoch: [4][2500/3247] Elapsed 10m 25s (remain 3m 6s) Loss: 0.0173(0.0168) Grad: 16164.5615  LR: 0.00000286  \n","Epoch: [4][2600/3247] Elapsed 10m 50s (remain 2m 41s) Loss: 0.0148(0.0168) Grad: 9436.4365  LR: 0.00000273  \n","Epoch: [4][2700/3247] Elapsed 11m 15s (remain 2m 16s) Loss: 0.0092(0.0168) Grad: 1982.6462  LR: 0.00000259  \n","Epoch: [4][2800/3247] Elapsed 11m 39s (remain 1m 51s) Loss: 0.0149(0.0168) Grad: 5336.5898  LR: 0.00000247  \n","Epoch: [4][2900/3247] Elapsed 12m 4s (remain 1m 26s) Loss: 0.0155(0.0168) Grad: 3892.0181  LR: 0.00000234  \n","Epoch: [4][3000/3247] Elapsed 12m 28s (remain 1m 1s) Loss: 0.0191(0.0168) Grad: 2513.0564  LR: 0.00000222  \n","Epoch: [4][3100/3247] Elapsed 12m 53s (remain 0m 36s) Loss: 0.0150(0.0167) Grad: 8762.5664  LR: 0.00000210  \n","Epoch: [4][3200/3247] Elapsed 13m 18s (remain 0m 11s) Loss: 0.0096(0.0167) Grad: 15789.2764  LR: 0.00000198  \n","Epoch: [4][3246/3247] Elapsed 13m 30s (remain 0m 0s) Loss: 0.0173(0.0167) Grad: 11457.4424  LR: 0.00000192  \n","EVAL: [0/400] Elapsed 0m 0s (remain 2m 20s) Loss: 0.0207(0.0207) \n","EVAL: [100/400] Elapsed 0m 10s (remain 0m 32s) Loss: 0.0153(0.0193) \n","EVAL: [200/400] Elapsed 0m 21s (remain 0m 21s) Loss: 0.0264(0.0201) \n","EVAL: [300/400] Elapsed 0m 32s (remain 0m 10s) Loss: 0.0141(0.0197) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0167  avg_val_loss: 0.0197  time: 853s\n","Epoch 4 - Score: 0.8294\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [399/400] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0049(0.0197) \n","Epoch: [5][0/3247] Elapsed 0m 0s (remain 33m 30s) Loss: 0.0137(0.0137) Grad: 6305.7285  LR: 0.00000192  \n","Epoch: [5][100/3247] Elapsed 0m 25s (remain 13m 25s) Loss: 0.0211(0.0163) Grad: 7647.9502  LR: 0.00000181  \n","Epoch: [5][200/3247] Elapsed 0m 51s (remain 12m 56s) Loss: 0.0135(0.0163) Grad: 5377.9604  LR: 0.00000170  \n","Epoch: [5][300/3247] Elapsed 1m 16s (remain 12m 29s) Loss: 0.0169(0.0165) Grad: 48741.4141  LR: 0.00000159  \n","Epoch: [5][400/3247] Elapsed 1m 41s (remain 12m 3s) Loss: 0.0121(0.0163) Grad: 915.2499  LR: 0.00000149  \n","Epoch: [5][500/3247] Elapsed 2m 7s (remain 11m 39s) Loss: 0.0181(0.0164) Grad: 5153.0063  LR: 0.00000139  \n","Epoch: [5][600/3247] Elapsed 2m 33s (remain 11m 13s) Loss: 0.0131(0.0165) Grad: 1825.6902  LR: 0.00000129  \n","Epoch: [5][700/3247] Elapsed 2m 58s (remain 10m 48s) Loss: 0.0139(0.0165) Grad: 2141.3740  LR: 0.00000120  \n","Epoch: [5][800/3247] Elapsed 3m 23s (remain 10m 22s) Loss: 0.0084(0.0164) Grad: 5309.7671  LR: 0.00000111  \n","Epoch: [5][900/3247] Elapsed 3m 49s (remain 9m 57s) Loss: 0.0220(0.0164) Grad: 6275.8379  LR: 0.00000102  \n","Epoch: [5][1000/3247] Elapsed 4m 14s (remain 9m 31s) Loss: 0.0178(0.0165) Grad: 5335.9839  LR: 0.00000094  \n","Epoch: [5][1100/3247] Elapsed 4m 39s (remain 9m 5s) Loss: 0.0225(0.0164) Grad: 10682.4707  LR: 0.00000086  \n","Epoch: [5][1200/3247] Elapsed 5m 4s (remain 8m 39s) Loss: 0.0200(0.0164) Grad: 8601.1729  LR: 0.00000078  \n","Epoch: [5][1300/3247] Elapsed 5m 30s (remain 8m 13s) Loss: 0.0121(0.0164) Grad: 1731.0428  LR: 0.00000071  \n","Epoch: [5][1400/3247] Elapsed 5m 55s (remain 7m 48s) Loss: 0.0152(0.0164) Grad: 3219.0105  LR: 0.00000064  \n","Epoch: [5][1500/3247] Elapsed 6m 21s (remain 7m 23s) Loss: 0.0172(0.0163) Grad: 5105.5000  LR: 0.00000057  \n","Epoch: [5][1600/3247] Elapsed 6m 47s (remain 6m 58s) Loss: 0.0162(0.0164) Grad: 5003.9473  LR: 0.00000051  \n","Epoch: [5][1700/3247] Elapsed 7m 12s (remain 6m 33s) Loss: 0.0144(0.0164) Grad: 5139.2007  LR: 0.00000045  \n","Epoch: [5][1800/3247] Elapsed 7m 37s (remain 6m 7s) Loss: 0.0212(0.0164) Grad: 9837.9883  LR: 0.00000039  \n","Epoch: [5][1900/3247] Elapsed 8m 2s (remain 5m 41s) Loss: 0.0158(0.0164) Grad: 1615.9453  LR: 0.00000034  \n","Epoch: [5][2000/3247] Elapsed 8m 27s (remain 5m 15s) Loss: 0.0174(0.0164) Grad: 76930.6953  LR: 0.00000029  \n","Epoch: [5][2100/3247] Elapsed 8m 52s (remain 4m 50s) Loss: 0.0229(0.0164) Grad: 11552.9551  LR: 0.00000025  \n","Epoch: [5][2200/3247] Elapsed 9m 18s (remain 4m 25s) Loss: 0.0212(0.0164) Grad: 5407.1758  LR: 0.00000021  \n","Epoch: [5][2300/3247] Elapsed 9m 43s (remain 3m 59s) Loss: 0.0211(0.0164) Grad: 12011.5176  LR: 0.00000017  \n","Epoch: [5][2400/3247] Elapsed 10m 8s (remain 3m 34s) Loss: 0.0177(0.0164) Grad: 5797.8184  LR: 0.00000014  \n","Epoch: [5][2500/3247] Elapsed 10m 34s (remain 3m 9s) Loss: 0.0130(0.0164) Grad: 11585.7070  LR: 0.00000011  \n","Epoch: [5][2600/3247] Elapsed 10m 59s (remain 2m 43s) Loss: 0.0204(0.0164) Grad: 8877.0869  LR: 0.00000008  \n","Epoch: [5][2700/3247] Elapsed 11m 24s (remain 2m 18s) Loss: 0.0175(0.0164) Grad: 20648.9473  LR: 0.00000006  \n","Epoch: [5][2800/3247] Elapsed 11m 50s (remain 1m 53s) Loss: 0.0205(0.0164) Grad: 5540.5322  LR: 0.00000004  \n","Epoch: [5][2900/3247] Elapsed 12m 15s (remain 1m 27s) Loss: 0.0140(0.0164) Grad: 15101.8076  LR: 0.00000002  \n","Epoch: [5][3000/3247] Elapsed 12m 40s (remain 1m 2s) Loss: 0.0168(0.0165) Grad: 1316.8787  LR: 0.00000001  \n","Epoch: [5][3100/3247] Elapsed 13m 5s (remain 0m 36s) Loss: 0.0098(0.0165) Grad: 3113.8467  LR: 0.00000000  \n","Epoch: [5][3200/3247] Elapsed 13m 30s (remain 0m 11s) Loss: 0.0142(0.0165) Grad: 1686.5100  LR: 0.00000000  \n","Epoch: [5][3246/3247] Elapsed 13m 42s (remain 0m 0s) Loss: 0.0135(0.0165) Grad: 5377.4624  LR: 0.00000000  \n","EVAL: [0/400] Elapsed 0m 0s (remain 2m 16s) Loss: 0.0193(0.0193) \n","EVAL: [100/400] Elapsed 0m 10s (remain 0m 32s) Loss: 0.0153(0.0195) \n","EVAL: [200/400] Elapsed 0m 21s (remain 0m 21s) Loss: 0.0268(0.0202) \n","EVAL: [300/400] Elapsed 0m 32s (remain 0m 10s) Loss: 0.0141(0.0198) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0165  avg_val_loss: 0.0198  time: 865s\n","Epoch 5 - Score: 0.8290\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [399/400] Elapsed 0m 42s (remain 0m 0s) Loss: 0.0049(0.0198) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 1 result ==========\n","Score: 0.8324\n","========== fold: 2 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/3297] Elapsed 0m 0s (remain 29m 26s) Loss: 0.0217(0.0217) Grad: 10119.4570  LR: 0.00000040  \n","Epoch: [1][100/3297] Elapsed 0m 25s (remain 13m 20s) Loss: 0.0289(0.0247) Grad: 21188.8027  LR: 0.00002000  \n","Epoch: [1][200/3297] Elapsed 0m 49s (remain 12m 49s) Loss: 0.0159(0.0231) Grad: 11408.0850  LR: 0.00002000  \n","Epoch: [1][300/3297] Elapsed 1m 14s (remain 12m 23s) Loss: 0.0268(0.0222) Grad: 17662.6445  LR: 0.00001999  \n","Epoch: [1][400/3297] Elapsed 1m 39s (remain 12m 1s) Loss: 0.0229(0.0218) Grad: 13322.3154  LR: 0.00001998  \n","Epoch: [1][500/3297] Elapsed 2m 5s (remain 11m 39s) Loss: 0.0139(0.0214) Grad: 6233.1157  LR: 0.00001996  \n","Epoch: [1][600/3297] Elapsed 2m 30s (remain 11m 15s) Loss: 0.0138(0.0211) Grad: 5269.3901  LR: 0.00001994  \n","Epoch: [1][700/3297] Elapsed 2m 55s (remain 10m 51s) Loss: 0.0178(0.0209) Grad: 215174.9844  LR: 0.00001992  \n","Epoch: [1][800/3297] Elapsed 3m 20s (remain 10m 26s) Loss: 0.0201(0.0207) Grad: 34123.9336  LR: 0.00001990  \n","Epoch: [1][900/3297] Elapsed 3m 45s (remain 10m 0s) Loss: 0.0207(0.0205) Grad: 11206.2842  LR: 0.00001987  \n","Epoch: [1][1000/3297] Elapsed 4m 11s (remain 9m 35s) Loss: 0.0161(0.0204) Grad: 6388.9409  LR: 0.00001984  \n","Epoch: [1][1100/3297] Elapsed 4m 36s (remain 9m 11s) Loss: 0.0194(0.0203) Grad: 10361.3662  LR: 0.00001980  \n","Epoch: [1][1200/3297] Elapsed 5m 1s (remain 8m 47s) Loss: 0.0231(0.0202) Grad: 13157.0596  LR: 0.00001976  \n","Epoch: [1][1300/3297] Elapsed 5m 27s (remain 8m 23s) Loss: 0.0191(0.0201) Grad: 5190.7803  LR: 0.00001972  \n","Epoch: [1][1400/3297] Elapsed 5m 53s (remain 7m 59s) Loss: 0.0235(0.0201) Grad: 4956.2900  LR: 0.00001967  \n","Epoch: [1][1500/3297] Elapsed 6m 19s (remain 7m 34s) Loss: 0.0176(0.0200) Grad: 6357.5205  LR: 0.00001962  \n","Epoch: [1][1600/3297] Elapsed 6m 44s (remain 7m 8s) Loss: 0.0104(0.0200) Grad: 2994.5850  LR: 0.00001956  \n","Epoch: [1][1700/3297] Elapsed 7m 10s (remain 6m 43s) Loss: 0.0195(0.0199) Grad: 7144.7173  LR: 0.00001951  \n","Epoch: [1][1800/3297] Elapsed 7m 35s (remain 6m 18s) Loss: 0.0208(0.0198) Grad: 4213.8481  LR: 0.00001945  \n","Epoch: [1][1900/3297] Elapsed 8m 1s (remain 5m 53s) Loss: 0.0197(0.0198) Grad: 2261.1616  LR: 0.00001938  \n","Epoch: [1][2000/3297] Elapsed 8m 26s (remain 5m 28s) Loss: 0.0241(0.0197) Grad: 14201.2930  LR: 0.00001931  \n","Epoch: [1][2100/3297] Elapsed 8m 52s (remain 5m 3s) Loss: 0.0121(0.0197) Grad: 6991.2197  LR: 0.00001924  \n","Epoch: [1][2200/3297] Elapsed 9m 17s (remain 4m 37s) Loss: 0.0150(0.0196) Grad: 11375.4502  LR: 0.00001917  \n","Epoch: [1][2300/3297] Elapsed 9m 43s (remain 4m 12s) Loss: 0.0189(0.0196) Grad: 5458.3950  LR: 0.00001909  \n","Epoch: [1][2400/3297] Elapsed 10m 8s (remain 3m 47s) Loss: 0.0145(0.0196) Grad: 20223.6855  LR: 0.00001901  \n","Epoch: [1][2500/3297] Elapsed 10m 34s (remain 3m 21s) Loss: 0.0193(0.0195) Grad: 5570.3457  LR: 0.00001892  \n","Epoch: [1][2600/3297] Elapsed 10m 59s (remain 2m 56s) Loss: 0.0181(0.0195) Grad: 12641.3877  LR: 0.00001883  \n","Epoch: [1][2700/3297] Elapsed 11m 25s (remain 2m 31s) Loss: 0.0190(0.0194) Grad: 8162.2988  LR: 0.00001874  \n","Epoch: [1][2800/3297] Elapsed 11m 50s (remain 2m 5s) Loss: 0.0190(0.0194) Grad: 15857.2324  LR: 0.00001865  \n","Epoch: [1][2900/3297] Elapsed 12m 15s (remain 1m 40s) Loss: 0.0207(0.0193) Grad: 14218.2021  LR: 0.00001855  \n","Epoch: [1][3000/3297] Elapsed 12m 41s (remain 1m 15s) Loss: 0.0170(0.0193) Grad: 10433.2236  LR: 0.00001845  \n","Epoch: [1][3100/3297] Elapsed 13m 6s (remain 0m 49s) Loss: 0.0199(0.0193) Grad: 21716.6406  LR: 0.00001835  \n","Epoch: [1][3200/3297] Elapsed 13m 31s (remain 0m 24s) Loss: 0.0193(0.0192) Grad: 16559.6680  LR: 0.00001824  \n","Epoch: [1][3296/3297] Elapsed 13m 56s (remain 0m 0s) Loss: 0.0164(0.0192) Grad: 11950.6924  LR: 0.00001814  \n","EVAL: [0/351] Elapsed 0m 0s (remain 2m 2s) Loss: 0.0288(0.0288) \n","EVAL: [100/351] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0134(0.0189) \n","EVAL: [200/351] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0178(0.0182) \n","EVAL: [300/351] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0176(0.0188) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0192  avg_val_loss: 0.0187  time: 874s\n","Epoch 1 - Score: 0.8348\n","Epoch 1 - Save Best Score: 0.8348 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [350/351] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0178(0.0187) \n","Epoch: [2][0/3297] Elapsed 0m 0s (remain 33m 39s) Loss: 0.0127(0.0127) Grad: 2243.3818  LR: 0.00001813  \n","Epoch: [2][100/3297] Elapsed 0m 27s (remain 14m 17s) Loss: 0.0207(0.0178) Grad: 4891.6128  LR: 0.00001802  \n","Epoch: [2][200/3297] Elapsed 0m 52s (remain 13m 34s) Loss: 0.0228(0.0178) Grad: 5151.5244  LR: 0.00001791  \n","Epoch: [2][300/3297] Elapsed 1m 17s (remain 12m 55s) Loss: 0.0172(0.0178) Grad: 3747.3657  LR: 0.00001779  \n","Epoch: [2][400/3297] Elapsed 1m 42s (remain 12m 23s) Loss: 0.0182(0.0178) Grad: 5855.0737  LR: 0.00001767  \n","Epoch: [2][500/3297] Elapsed 2m 7s (remain 11m 53s) Loss: 0.0157(0.0178) Grad: 5407.8267  LR: 0.00001754  \n","Epoch: [2][600/3297] Elapsed 2m 32s (remain 11m 25s) Loss: 0.0204(0.0179) Grad: 6547.6914  LR: 0.00001741  \n","Epoch: [2][700/3297] Elapsed 2m 57s (remain 10m 58s) Loss: 0.0218(0.0180) Grad: 11386.0381  LR: 0.00001729  \n","Epoch: [2][800/3297] Elapsed 3m 22s (remain 10m 32s) Loss: 0.0198(0.0179) Grad: 5691.9512  LR: 0.00001715  \n","Epoch: [2][900/3297] Elapsed 3m 48s (remain 10m 6s) Loss: 0.0193(0.0180) Grad: 3697.6492  LR: 0.00001702  \n","Epoch: [2][1000/3297] Elapsed 4m 13s (remain 9m 40s) Loss: 0.0145(0.0179) Grad: 3470.8376  LR: 0.00001688  \n","Epoch: [2][1100/3297] Elapsed 4m 38s (remain 9m 15s) Loss: 0.0159(0.0179) Grad: 3924.8911  LR: 0.00001674  \n","Epoch: [2][1200/3297] Elapsed 5m 3s (remain 8m 49s) Loss: 0.0173(0.0179) Grad: 3867.6433  LR: 0.00001660  \n","Epoch: [2][1300/3297] Elapsed 5m 28s (remain 8m 24s) Loss: 0.0231(0.0179) Grad: 35590.9883  LR: 0.00001645  \n","Epoch: [2][1400/3297] Elapsed 5m 53s (remain 7m 58s) Loss: 0.0192(0.0179) Grad: 11835.7754  LR: 0.00001631  \n","Epoch: [2][1500/3297] Elapsed 6m 18s (remain 7m 33s) Loss: 0.0164(0.0179) Grad: 4406.9893  LR: 0.00001616  \n","Epoch: [2][1600/3297] Elapsed 6m 43s (remain 7m 7s) Loss: 0.0168(0.0179) Grad: 10070.0986  LR: 0.00001601  \n","Epoch: [2][1700/3297] Elapsed 7m 8s (remain 6m 42s) Loss: 0.0169(0.0179) Grad: 4214.6758  LR: 0.00001585  \n","Epoch: [2][1800/3297] Elapsed 7m 33s (remain 6m 16s) Loss: 0.0181(0.0179) Grad: 1412.8287  LR: 0.00001570  \n","Epoch: [2][1900/3297] Elapsed 7m 58s (remain 5m 51s) Loss: 0.0229(0.0179) Grad: 5820.0923  LR: 0.00001554  \n","Epoch: [2][2000/3297] Elapsed 8m 24s (remain 5m 26s) Loss: 0.0190(0.0179) Grad: 8114.3320  LR: 0.00001538  \n","Epoch: [2][2100/3297] Elapsed 8m 49s (remain 5m 1s) Loss: 0.0244(0.0178) Grad: 40437.9609  LR: 0.00001521  \n","Epoch: [2][2200/3297] Elapsed 9m 14s (remain 4m 36s) Loss: 0.0169(0.0178) Grad: 4993.2183  LR: 0.00001505  \n","Epoch: [2][2300/3297] Elapsed 9m 39s (remain 4m 10s) Loss: 0.0199(0.0178) Grad: 7169.8101  LR: 0.00001488  \n","Epoch: [2][2400/3297] Elapsed 10m 4s (remain 3m 45s) Loss: 0.0209(0.0178) Grad: 5849.4521  LR: 0.00001472  \n","Epoch: [2][2500/3297] Elapsed 10m 29s (remain 3m 20s) Loss: 0.0118(0.0178) Grad: 12121.9043  LR: 0.00001455  \n","Epoch: [2][2600/3297] Elapsed 10m 55s (remain 2m 55s) Loss: 0.0133(0.0178) Grad: 8930.3828  LR: 0.00001438  \n","Epoch: [2][2700/3297] Elapsed 11m 20s (remain 2m 30s) Loss: 0.0191(0.0178) Grad: 12066.7383  LR: 0.00001420  \n","Epoch: [2][2800/3297] Elapsed 11m 45s (remain 2m 4s) Loss: 0.0211(0.0178) Grad: 9965.5322  LR: 0.00001403  \n","Epoch: [2][2900/3297] Elapsed 12m 11s (remain 1m 39s) Loss: 0.0169(0.0178) Grad: 17190.4766  LR: 0.00001385  \n","Epoch: [2][3000/3297] Elapsed 12m 36s (remain 1m 14s) Loss: 0.0172(0.0178) Grad: 12755.6094  LR: 0.00001368  \n","Epoch: [2][3100/3297] Elapsed 13m 1s (remain 0m 49s) Loss: 0.0159(0.0178) Grad: 16037.7002  LR: 0.00001350  \n","Epoch: [2][3200/3297] Elapsed 13m 26s (remain 0m 24s) Loss: 0.0160(0.0178) Grad: 9029.6641  LR: 0.00001332  \n","Epoch: [2][3296/3297] Elapsed 13m 51s (remain 0m 0s) Loss: 0.0163(0.0178) Grad: 11424.1865  LR: 0.00001315  \n","EVAL: [0/351] Elapsed 0m 0s (remain 1m 59s) Loss: 0.0302(0.0302) \n","EVAL: [100/351] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0136(0.0202) \n","EVAL: [200/351] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0196(0.0192) \n","EVAL: [300/351] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0166(0.0200) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0178  avg_val_loss: 0.0200  time: 869s\n","Epoch 2 - Score: 0.8308\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [350/351] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0178(0.0200) \n","Epoch: [3][0/3297] Elapsed 0m 0s (remain 32m 13s) Loss: 0.0158(0.0158) Grad: 5569.7017  LR: 0.00001314  \n","Epoch: [3][100/3297] Elapsed 0m 25s (remain 13m 36s) Loss: 0.0171(0.0173) Grad: 4505.2383  LR: 0.00001296  \n","Epoch: [3][200/3297] Elapsed 0m 51s (remain 13m 5s) Loss: 0.0151(0.0172) Grad: 6406.5010  LR: 0.00001278  \n","Epoch: [3][300/3297] Elapsed 1m 16s (remain 12m 36s) Loss: 0.0166(0.0173) Grad: 1680.8833  LR: 0.00001259  \n","Epoch: [3][400/3297] Elapsed 1m 41s (remain 12m 10s) Loss: 0.0191(0.0172) Grad: 2486.8835  LR: 0.00001241  \n","Epoch: [3][500/3297] Elapsed 2m 6s (remain 11m 45s) Loss: 0.0165(0.0173) Grad: 3565.3318  LR: 0.00001222  \n","Epoch: [3][600/3297] Elapsed 2m 31s (remain 11m 20s) Loss: 0.0160(0.0172) Grad: 2884.7917  LR: 0.00001204  \n","Epoch: [3][700/3297] Elapsed 2m 57s (remain 10m 56s) Loss: 0.0152(0.0172) Grad: 3509.2593  LR: 0.00001185  \n","Epoch: [3][800/3297] Elapsed 3m 22s (remain 10m 31s) Loss: 0.0180(0.0172) Grad: 1696.7562  LR: 0.00001166  \n","Epoch: [3][900/3297] Elapsed 3m 48s (remain 10m 6s) Loss: 0.0145(0.0172) Grad: 5024.3882  LR: 0.00001147  \n","Epoch: [3][1000/3297] Elapsed 4m 13s (remain 9m 41s) Loss: 0.0148(0.0172) Grad: 2050.4949  LR: 0.00001128  \n","Epoch: [3][1100/3297] Elapsed 4m 38s (remain 9m 16s) Loss: 0.0176(0.0172) Grad: 2186.4570  LR: 0.00001109  \n","Epoch: [3][1200/3297] Elapsed 5m 4s (remain 8m 50s) Loss: 0.0122(0.0172) Grad: 3161.6743  LR: 0.00001090  \n","Epoch: [3][1300/3297] Elapsed 5m 29s (remain 8m 25s) Loss: 0.0099(0.0172) Grad: 2387.4114  LR: 0.00001071  \n","Epoch: [3][1400/3297] Elapsed 5m 55s (remain 8m 0s) Loss: 0.0143(0.0172) Grad: 5128.4297  LR: 0.00001052  \n","Epoch: [3][1500/3297] Elapsed 6m 20s (remain 7m 35s) Loss: 0.0163(0.0172) Grad: 3147.2432  LR: 0.00001033  \n","Epoch: [3][1600/3297] Elapsed 6m 45s (remain 7m 9s) Loss: 0.0154(0.0172) Grad: 6565.4863  LR: 0.00001014  \n","Epoch: [3][1700/3297] Elapsed 7m 10s (remain 6m 44s) Loss: 0.0194(0.0172) Grad: 857.7812  LR: 0.00000995  \n","Epoch: [3][1800/3297] Elapsed 7m 36s (remain 6m 18s) Loss: 0.0217(0.0172) Grad: 2316.4353  LR: 0.00000976  \n","Epoch: [3][1900/3297] Elapsed 8m 1s (remain 5m 53s) Loss: 0.0170(0.0172) Grad: 2439.7905  LR: 0.00000957  \n","Epoch: [3][2000/3297] Elapsed 8m 26s (remain 5m 28s) Loss: 0.0168(0.0172) Grad: 7898.9458  LR: 0.00000938  \n","Epoch: [3][2100/3297] Elapsed 8m 51s (remain 5m 2s) Loss: 0.0121(0.0172) Grad: 5055.2192  LR: 0.00000918  \n","Epoch: [3][2200/3297] Elapsed 9m 17s (remain 4m 37s) Loss: 0.0197(0.0172) Grad: 13260.2803  LR: 0.00000899  \n","Epoch: [3][2300/3297] Elapsed 9m 42s (remain 4m 12s) Loss: 0.0103(0.0171) Grad: 7476.8364  LR: 0.00000880  \n","Epoch: [3][2400/3297] Elapsed 10m 7s (remain 3m 46s) Loss: 0.0186(0.0171) Grad: 4117.8037  LR: 0.00000861  \n","Epoch: [3][2500/3297] Elapsed 10m 32s (remain 3m 21s) Loss: 0.0166(0.0171) Grad: 10353.1797  LR: 0.00000843  \n","Epoch: [3][2600/3297] Elapsed 10m 58s (remain 2m 56s) Loss: 0.0194(0.0171) Grad: 5539.4077  LR: 0.00000824  \n","Epoch: [3][2700/3297] Elapsed 11m 23s (remain 2m 30s) Loss: 0.0182(0.0171) Grad: 9955.7256  LR: 0.00000805  \n","Epoch: [3][2800/3297] Elapsed 11m 49s (remain 2m 5s) Loss: 0.0164(0.0171) Grad: 9622.0576  LR: 0.00000786  \n","Epoch: [3][2900/3297] Elapsed 12m 14s (remain 1m 40s) Loss: 0.0208(0.0171) Grad: 10087.6162  LR: 0.00000768  \n","Epoch: [3][3000/3297] Elapsed 12m 39s (remain 1m 14s) Loss: 0.0204(0.0171) Grad: 12044.8096  LR: 0.00000749  \n","Epoch: [3][3100/3297] Elapsed 13m 5s (remain 0m 49s) Loss: 0.0120(0.0171) Grad: 5124.3179  LR: 0.00000731  \n","Epoch: [3][3200/3297] Elapsed 13m 30s (remain 0m 24s) Loss: 0.0171(0.0171) Grad: 5872.8345  LR: 0.00000712  \n","Epoch: [3][3296/3297] Elapsed 13m 54s (remain 0m 0s) Loss: 0.0159(0.0171) Grad: 6910.2334  LR: 0.00000695  \n","EVAL: [0/351] Elapsed 0m 0s (remain 1m 58s) Loss: 0.0313(0.0313) \n","EVAL: [100/351] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0148(0.0197) \n","EVAL: [200/351] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0186(0.0189) \n","EVAL: [300/351] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0171(0.0196) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0171  avg_val_loss: 0.0196  time: 872s\n","Epoch 3 - Score: 0.8362\n","Epoch 3 - Save Best Score: 0.8362 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [350/351] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0207(0.0196) \n","Epoch: [4][0/3297] Elapsed 0m 0s (remain 33m 26s) Loss: 0.0159(0.0159) Grad: 3733.7202  LR: 0.00000695  \n","Epoch: [4][100/3297] Elapsed 0m 26s (remain 13m 55s) Loss: 0.0153(0.0165) Grad: 5579.2495  LR: 0.00000676  \n","Epoch: [4][200/3297] Elapsed 0m 51s (remain 13m 19s) Loss: 0.0120(0.0168) Grad: 3504.8904  LR: 0.00000658  \n","Epoch: [4][300/3297] Elapsed 1m 16s (remain 12m 45s) Loss: 0.0184(0.0169) Grad: 2518.1907  LR: 0.00000640  \n","Epoch: [4][400/3297] Elapsed 1m 41s (remain 12m 16s) Loss: 0.0194(0.0168) Grad: 4354.9521  LR: 0.00000623  \n","Epoch: [4][500/3297] Elapsed 2m 6s (remain 11m 47s) Loss: 0.0184(0.0167) Grad: 1452.5770  LR: 0.00000605  \n","Epoch: [4][600/3297] Elapsed 2m 31s (remain 11m 20s) Loss: 0.0066(0.0166) Grad: 936.6990  LR: 0.00000588  \n","Epoch: [4][700/3297] Elapsed 2m 56s (remain 10m 54s) Loss: 0.0163(0.0166) Grad: 3941.7852  LR: 0.00000570  \n","Epoch: [4][800/3297] Elapsed 3m 21s (remain 10m 28s) Loss: 0.0163(0.0165) Grad: 3700.8335  LR: 0.00000553  \n","Epoch: [4][900/3297] Elapsed 3m 46s (remain 10m 3s) Loss: 0.0203(0.0166) Grad: 2946.6111  LR: 0.00000536  \n","Epoch: [4][1000/3297] Elapsed 4m 11s (remain 9m 37s) Loss: 0.0175(0.0165) Grad: 3532.6604  LR: 0.00000519  \n","Epoch: [4][1100/3297] Elapsed 4m 36s (remain 9m 11s) Loss: 0.0119(0.0165) Grad: 1528.3467  LR: 0.00000503  \n","Epoch: [4][1200/3297] Elapsed 5m 1s (remain 8m 46s) Loss: 0.0190(0.0166) Grad: 3975.0693  LR: 0.00000486  \n","Epoch: [4][1300/3297] Elapsed 5m 27s (remain 8m 21s) Loss: 0.0193(0.0166) Grad: 4482.2837  LR: 0.00000470  \n","Epoch: [4][1400/3297] Elapsed 5m 52s (remain 7m 56s) Loss: 0.0182(0.0166) Grad: 5309.1343  LR: 0.00000454  \n","Epoch: [4][1500/3297] Elapsed 6m 17s (remain 7m 31s) Loss: 0.0174(0.0166) Grad: 4452.2778  LR: 0.00000438  \n","Epoch: [4][1600/3297] Elapsed 6m 42s (remain 7m 5s) Loss: 0.0164(0.0166) Grad: 1878.8857  LR: 0.00000422  \n","Epoch: [4][1700/3297] Elapsed 7m 7s (remain 6m 40s) Loss: 0.0195(0.0166) Grad: 7648.3066  LR: 0.00000407  \n","Epoch: [4][1800/3297] Elapsed 7m 32s (remain 6m 15s) Loss: 0.0197(0.0166) Grad: 5524.4131  LR: 0.00000391  \n","Epoch: [4][1900/3297] Elapsed 7m 57s (remain 5m 50s) Loss: 0.0182(0.0166) Grad: 5050.1201  LR: 0.00000376  \n","Epoch: [4][2000/3297] Elapsed 8m 22s (remain 5m 25s) Loss: 0.0119(0.0166) Grad: 2624.2820  LR: 0.00000361  \n","Epoch: [4][2100/3297] Elapsed 8m 46s (remain 4m 59s) Loss: 0.0173(0.0166) Grad: 12499.9805  LR: 0.00000347  \n","Epoch: [4][2200/3297] Elapsed 9m 11s (remain 4m 34s) Loss: 0.0167(0.0166) Grad: 4592.3770  LR: 0.00000332  \n","Epoch: [4][2300/3297] Elapsed 9m 36s (remain 4m 9s) Loss: 0.0173(0.0167) Grad: 7691.5171  LR: 0.00000318  \n","Epoch: [4][2400/3297] Elapsed 10m 1s (remain 3m 44s) Loss: 0.0199(0.0166) Grad: 1773.2634  LR: 0.00000304  \n","Epoch: [4][2500/3297] Elapsed 10m 26s (remain 3m 19s) Loss: 0.0164(0.0166) Grad: 5085.8091  LR: 0.00000291  \n","Epoch: [4][2600/3297] Elapsed 10m 51s (remain 2m 54s) Loss: 0.0186(0.0166) Grad: 5854.7192  LR: 0.00000278  \n","Epoch: [4][2700/3297] Elapsed 11m 16s (remain 2m 29s) Loss: 0.0110(0.0166) Grad: 2320.6416  LR: 0.00000264  \n","Epoch: [4][2800/3297] Elapsed 11m 41s (remain 2m 4s) Loss: 0.0207(0.0166) Grad: 9744.7500  LR: 0.00000252  \n","Epoch: [4][2900/3297] Elapsed 12m 6s (remain 1m 39s) Loss: 0.0141(0.0166) Grad: 4814.8691  LR: 0.00000239  \n","Epoch: [4][3000/3297] Elapsed 12m 30s (remain 1m 14s) Loss: 0.0154(0.0166) Grad: 7297.5381  LR: 0.00000227  \n","Epoch: [4][3100/3297] Elapsed 12m 55s (remain 0m 49s) Loss: 0.0220(0.0166) Grad: 9984.5293  LR: 0.00000215  \n","Epoch: [4][3200/3297] Elapsed 13m 20s (remain 0m 24s) Loss: 0.0179(0.0166) Grad: 1279.1359  LR: 0.00000203  \n","Epoch: [4][3296/3297] Elapsed 13m 44s (remain 0m 0s) Loss: 0.0162(0.0166) Grad: 9012.9658  LR: 0.00000192  \n","EVAL: [0/351] Elapsed 0m 0s (remain 2m 1s) Loss: 0.0318(0.0318) \n","EVAL: [100/351] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0157(0.0205) \n","EVAL: [200/351] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0208(0.0196) \n","EVAL: [300/351] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0180(0.0205) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0166  avg_val_loss: 0.0204  time: 862s\n","Epoch 4 - Score: 0.8301\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [350/351] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0204(0.0204) \n","Epoch: [5][0/3297] Elapsed 0m 0s (remain 32m 13s) Loss: 0.0160(0.0160) Grad: 3391.3481  LR: 0.00000192  \n","Epoch: [5][100/3297] Elapsed 0m 25s (remain 13m 28s) Loss: 0.0196(0.0158) Grad: 4252.4727  LR: 0.00000181  \n","Epoch: [5][200/3297] Elapsed 0m 50s (remain 12m 58s) Loss: 0.0183(0.0158) Grad: 1244.6874  LR: 0.00000170  \n","Epoch: [5][300/3297] Elapsed 1m 15s (remain 12m 34s) Loss: 0.0196(0.0160) Grad: 2123.1409  LR: 0.00000160  \n","Epoch: [5][400/3297] Elapsed 1m 40s (remain 12m 8s) Loss: 0.0171(0.0161) Grad: 1049.1761  LR: 0.00000149  \n","Epoch: [5][500/3297] Elapsed 2m 5s (remain 11m 43s) Loss: 0.0173(0.0161) Grad: 5590.5059  LR: 0.00000140  \n","Epoch: [5][600/3297] Elapsed 2m 31s (remain 11m 18s) Loss: 0.0163(0.0162) Grad: 8848.8838  LR: 0.00000130  \n","Epoch: [5][700/3297] Elapsed 2m 56s (remain 10m 52s) Loss: 0.0194(0.0163) Grad: 1055.7623  LR: 0.00000121  \n","Epoch: [5][800/3297] Elapsed 3m 21s (remain 10m 26s) Loss: 0.0196(0.0164) Grad: 4483.7964  LR: 0.00000112  \n","Epoch: [5][900/3297] Elapsed 3m 45s (remain 10m 0s) Loss: 0.0167(0.0164) Grad: 3595.0371  LR: 0.00000103  \n","Epoch: [5][1000/3297] Elapsed 4m 10s (remain 9m 35s) Loss: 0.0184(0.0164) Grad: 2992.7314  LR: 0.00000095  \n","Epoch: [5][1100/3297] Elapsed 4m 35s (remain 9m 10s) Loss: 0.0163(0.0163) Grad: 2830.2957  LR: 0.00000087  \n","Epoch: [5][1200/3297] Elapsed 5m 0s (remain 8m 45s) Loss: 0.0148(0.0164) Grad: 9206.2539  LR: 0.00000079  \n","Epoch: [5][1300/3297] Elapsed 5m 26s (remain 8m 20s) Loss: 0.0185(0.0164) Grad: 4305.4346  LR: 0.00000072  \n","Epoch: [5][1400/3297] Elapsed 5m 50s (remain 7m 54s) Loss: 0.0162(0.0164) Grad: 3094.5713  LR: 0.00000065  \n","Epoch: [5][1500/3297] Elapsed 6m 15s (remain 7m 29s) Loss: 0.0196(0.0163) Grad: 1343.7473  LR: 0.00000058  \n","Epoch: [5][1600/3297] Elapsed 6m 40s (remain 7m 4s) Loss: 0.0218(0.0164) Grad: 1459.8766  LR: 0.00000052  \n","Epoch: [5][1700/3297] Elapsed 7m 5s (remain 6m 39s) Loss: 0.0181(0.0164) Grad: 3044.2979  LR: 0.00000046  \n","Epoch: [5][1800/3297] Elapsed 7m 30s (remain 6m 14s) Loss: 0.0175(0.0164) Grad: 3817.1401  LR: 0.00000041  \n","Epoch: [5][1900/3297] Elapsed 7m 55s (remain 5m 49s) Loss: 0.0097(0.0164) Grad: 1052.6381  LR: 0.00000035  \n","Epoch: [5][2000/3297] Elapsed 8m 20s (remain 5m 24s) Loss: 0.0185(0.0164) Grad: 5354.5869  LR: 0.00000031  \n","Epoch: [5][2100/3297] Elapsed 8m 45s (remain 4m 59s) Loss: 0.0119(0.0164) Grad: 2416.6663  LR: 0.00000026  \n","Epoch: [5][2200/3297] Elapsed 9m 10s (remain 4m 34s) Loss: 0.0163(0.0164) Grad: 11368.8984  LR: 0.00000022  \n","Epoch: [5][2300/3297] Elapsed 9m 35s (remain 4m 9s) Loss: 0.0156(0.0164) Grad: 5711.7900  LR: 0.00000018  \n","Epoch: [5][2400/3297] Elapsed 10m 0s (remain 3m 43s) Loss: 0.0144(0.0164) Grad: 5488.6914  LR: 0.00000015  \n","Epoch: [5][2500/3297] Elapsed 10m 24s (remain 3m 18s) Loss: 0.0178(0.0164) Grad: 2788.0667  LR: 0.00000012  \n","Epoch: [5][2600/3297] Elapsed 10m 49s (remain 2m 53s) Loss: 0.0195(0.0164) Grad: 9747.3340  LR: 0.00000009  \n","Epoch: [5][2700/3297] Elapsed 11m 14s (remain 2m 28s) Loss: 0.0180(0.0164) Grad: 12166.3105  LR: 0.00000007  \n","Epoch: [5][2800/3297] Elapsed 11m 39s (remain 2m 3s) Loss: 0.0189(0.0164) Grad: 2041.0306  LR: 0.00000005  \n","Epoch: [5][2900/3297] Elapsed 12m 4s (remain 1m 38s) Loss: 0.0141(0.0164) Grad: 15249.8320  LR: 0.00000003  \n","Epoch: [5][3000/3297] Elapsed 12m 29s (remain 1m 13s) Loss: 0.0124(0.0164) Grad: 6210.2290  LR: 0.00000002  \n","Epoch: [5][3100/3297] Elapsed 12m 54s (remain 0m 48s) Loss: 0.0139(0.0164) Grad: 8617.7158  LR: 0.00000001  \n","Epoch: [5][3200/3297] Elapsed 13m 19s (remain 0m 23s) Loss: 0.0189(0.0164) Grad: 11916.9014  LR: 0.00000000  \n","Epoch: [5][3296/3297] Elapsed 13m 43s (remain 0m 0s) Loss: 0.0213(0.0164) Grad: 9836.5234  LR: 0.00000000  \n","EVAL: [0/351] Elapsed 0m 0s (remain 2m 1s) Loss: 0.0310(0.0310) \n","EVAL: [100/351] Elapsed 0m 10s (remain 0m 27s) Loss: 0.0154(0.0205) \n","EVAL: [200/351] Elapsed 0m 21s (remain 0m 16s) Loss: 0.0208(0.0196) \n","EVAL: [300/351] Elapsed 0m 32s (remain 0m 5s) Loss: 0.0175(0.0204) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0164  avg_val_loss: 0.0203  time: 861s\n","Epoch 5 - Score: 0.8307\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [350/351] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0204(0.0203) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 2 result ==========\n","Score: 0.8362\n","========== fold: 3 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/3278] Elapsed 0m 0s (remain 28m 38s) Loss: 0.0604(0.0604) Grad: 125819.2031  LR: 0.00000040  \n","Epoch: [1][100/3278] Elapsed 0m 25s (remain 13m 18s) Loss: 0.0223(0.0302) Grad: 22921.8047  LR: 0.00002000  \n","Epoch: [1][200/3278] Elapsed 0m 50s (remain 12m 49s) Loss: 0.0230(0.0257) Grad: 21572.8984  LR: 0.00002000  \n","Epoch: [1][300/3278] Elapsed 1m 15s (remain 12m 22s) Loss: 0.0241(0.0242) Grad: 14496.3994  LR: 0.00001999  \n","Epoch: [1][400/3278] Elapsed 1m 39s (remain 11m 56s) Loss: 0.0206(0.0231) Grad: 16179.8945  LR: 0.00001998  \n","Epoch: [1][500/3278] Elapsed 2m 4s (remain 11m 31s) Loss: 0.0178(0.0226) Grad: 5428.9722  LR: 0.00001996  \n","Epoch: [1][600/3278] Elapsed 2m 29s (remain 11m 6s) Loss: 0.0197(0.0221) Grad: 9325.6025  LR: 0.00001994  \n","Epoch: [1][700/3278] Elapsed 2m 54s (remain 10m 40s) Loss: 0.0216(0.0218) Grad: 8593.9883  LR: 0.00001992  \n","Epoch: [1][800/3278] Elapsed 3m 18s (remain 10m 15s) Loss: 0.0166(0.0215) Grad: 8543.9551  LR: 0.00001990  \n","Epoch: [1][900/3278] Elapsed 3m 43s (remain 9m 50s) Loss: 0.0209(0.0213) Grad: 8861.6514  LR: 0.00001987  \n","Epoch: [1][1000/3278] Elapsed 4m 8s (remain 9m 24s) Loss: 0.0217(0.0211) Grad: 6539.9556  LR: 0.00001983  \n","Epoch: [1][1100/3278] Elapsed 4m 32s (remain 8m 59s) Loss: 0.0273(0.0209) Grad: 22901.7715  LR: 0.00001980  \n","Epoch: [1][1200/3278] Elapsed 4m 58s (remain 8m 35s) Loss: 0.0217(0.0208) Grad: 11687.6191  LR: 0.00001976  \n","Epoch: [1][1300/3278] Elapsed 5m 23s (remain 8m 11s) Loss: 0.0188(0.0207) Grad: 6848.5176  LR: 0.00001971  \n","Epoch: [1][1400/3278] Elapsed 5m 48s (remain 7m 46s) Loss: 0.0188(0.0206) Grad: 3730.1829  LR: 0.00001966  \n","Epoch: [1][1500/3278] Elapsed 6m 12s (remain 7m 21s) Loss: 0.0228(0.0205) Grad: 2801.9324  LR: 0.00001961  \n","Epoch: [1][1600/3278] Elapsed 6m 37s (remain 6m 56s) Loss: 0.0252(0.0204) Grad: 19517.1641  LR: 0.00001956  \n","Epoch: [1][1700/3278] Elapsed 7m 2s (remain 6m 31s) Loss: 0.0155(0.0203) Grad: 3167.9778  LR: 0.00001950  \n","Epoch: [1][1800/3278] Elapsed 7m 27s (remain 6m 6s) Loss: 0.0183(0.0202) Grad: 11160.1475  LR: 0.00001944  \n","Epoch: [1][1900/3278] Elapsed 7m 51s (remain 5m 41s) Loss: 0.0215(0.0201) Grad: 2420.6267  LR: 0.00001937  \n","Epoch: [1][2000/3278] Elapsed 8m 16s (remain 5m 16s) Loss: 0.0175(0.0201) Grad: 14978.8574  LR: 0.00001930  \n","Epoch: [1][2100/3278] Elapsed 8m 41s (remain 4m 51s) Loss: 0.0195(0.0200) Grad: 11008.1914  LR: 0.00001923  \n","Epoch: [1][2200/3278] Elapsed 9m 5s (remain 4m 27s) Loss: 0.0246(0.0199) Grad: 24859.6191  LR: 0.00001916  \n","Epoch: [1][2300/3278] Elapsed 9m 30s (remain 4m 2s) Loss: 0.0178(0.0199) Grad: 19647.7207  LR: 0.00001908  \n","Epoch: [1][2400/3278] Elapsed 9m 54s (remain 3m 37s) Loss: 0.0170(0.0198) Grad: 11096.3223  LR: 0.00001900  \n","Epoch: [1][2500/3278] Elapsed 10m 19s (remain 3m 12s) Loss: 0.0157(0.0198) Grad: 20153.4355  LR: 0.00001891  \n","Epoch: [1][2600/3278] Elapsed 10m 44s (remain 2m 47s) Loss: 0.0206(0.0197) Grad: 14473.0488  LR: 0.00001882  \n","Epoch: [1][2700/3278] Elapsed 11m 8s (remain 2m 22s) Loss: 0.0168(0.0197) Grad: 4919.1353  LR: 0.00001873  \n","Epoch: [1][2800/3278] Elapsed 11m 33s (remain 1m 58s) Loss: 0.0205(0.0197) Grad: 8442.7959  LR: 0.00001863  \n","Epoch: [1][2900/3278] Elapsed 11m 58s (remain 1m 33s) Loss: 0.0210(0.0196) Grad: 32512.3691  LR: 0.00001854  \n","Epoch: [1][3000/3278] Elapsed 12m 22s (remain 1m 8s) Loss: 0.0204(0.0196) Grad: 11958.3457  LR: 0.00001843  \n","Epoch: [1][3100/3278] Elapsed 12m 47s (remain 0m 43s) Loss: 0.0250(0.0195) Grad: 19374.5840  LR: 0.00001833  \n","Epoch: [1][3200/3278] Elapsed 13m 12s (remain 0m 19s) Loss: 0.0189(0.0195) Grad: 46988.8984  LR: 0.00001822  \n","Epoch: [1][3277/3278] Elapsed 13m 31s (remain 0m 0s) Loss: 0.0162(0.0195) Grad: 7658.6914  LR: 0.00001814  \n","EVAL: [0/370] Elapsed 0m 0s (remain 2m 7s) Loss: 0.0130(0.0130) \n","EVAL: [100/370] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0233(0.0184) \n","EVAL: [200/370] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0114(0.0189) \n","EVAL: [300/370] Elapsed 0m 32s (remain 0m 7s) Loss: 0.0338(0.0193) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0195  avg_val_loss: 0.0192  time: 851s\n","Epoch 1 - Score: 0.8151\n","Epoch 1 - Save Best Score: 0.8151 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [369/370] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0262(0.0192) \n","Epoch: [2][0/3278] Elapsed 0m 0s (remain 33m 13s) Loss: 0.0200(0.0200) Grad: 7176.5171  LR: 0.00001813  \n","Epoch: [2][100/3278] Elapsed 0m 26s (remain 13m 44s) Loss: 0.0206(0.0176) Grad: 6961.8208  LR: 0.00001802  \n","Epoch: [2][200/3278] Elapsed 0m 51s (remain 13m 5s) Loss: 0.0169(0.0179) Grad: 18121.8379  LR: 0.00001790  \n","Epoch: [2][300/3278] Elapsed 1m 15s (remain 12m 30s) Loss: 0.0161(0.0180) Grad: 20387.3164  LR: 0.00001779  \n","Epoch: [2][400/3278] Elapsed 1m 40s (remain 12m 2s) Loss: 0.0181(0.0181) Grad: 7411.7676  LR: 0.00001766  \n","Epoch: [2][500/3278] Elapsed 2m 5s (remain 11m 35s) Loss: 0.0168(0.0180) Grad: 4962.9868  LR: 0.00001754  \n","Epoch: [2][600/3278] Elapsed 2m 30s (remain 11m 9s) Loss: 0.0140(0.0179) Grad: 3159.4880  LR: 0.00001741  \n","Epoch: [2][700/3278] Elapsed 2m 55s (remain 10m 43s) Loss: 0.0151(0.0179) Grad: 6372.5962  LR: 0.00001728  \n","Epoch: [2][800/3278] Elapsed 3m 20s (remain 10m 18s) Loss: 0.0171(0.0180) Grad: 4595.5249  LR: 0.00001715  \n","Epoch: [2][900/3278] Elapsed 3m 45s (remain 9m 54s) Loss: 0.0215(0.0180) Grad: 21362.3906  LR: 0.00001701  \n","Epoch: [2][1000/3278] Elapsed 4m 9s (remain 9m 28s) Loss: 0.0203(0.0180) Grad: 1572.2053  LR: 0.00001687  \n","Epoch: [2][1100/3278] Elapsed 4m 34s (remain 9m 3s) Loss: 0.0204(0.0180) Grad: 4947.3960  LR: 0.00001673  \n","Epoch: [2][1200/3278] Elapsed 4m 59s (remain 8m 38s) Loss: 0.0176(0.0179) Grad: 4232.0220  LR: 0.00001659  \n","Epoch: [2][1300/3278] Elapsed 5m 25s (remain 8m 13s) Loss: 0.0239(0.0180) Grad: 8766.8184  LR: 0.00001644  \n","Epoch: [2][1400/3278] Elapsed 5m 50s (remain 7m 49s) Loss: 0.0160(0.0180) Grad: 3035.9417  LR: 0.00001629  \n","Epoch: [2][1500/3278] Elapsed 6m 15s (remain 7m 24s) Loss: 0.0183(0.0180) Grad: 6691.8311  LR: 0.00001614  \n","Epoch: [2][1600/3278] Elapsed 6m 40s (remain 6m 59s) Loss: 0.0115(0.0180) Grad: 15429.8359  LR: 0.00001599  \n","Epoch: [2][1700/3278] Elapsed 7m 5s (remain 6m 34s) Loss: 0.0280(0.0180) Grad: 37400.9727  LR: 0.00001584  \n","Epoch: [2][1800/3278] Elapsed 7m 29s (remain 6m 9s) Loss: 0.0215(0.0180) Grad: 6428.4443  LR: 0.00001568  \n","Epoch: [2][1900/3278] Elapsed 7m 54s (remain 5m 43s) Loss: 0.0207(0.0180) Grad: 4574.4893  LR: 0.00001552  \n","Epoch: [2][2000/3278] Elapsed 8m 19s (remain 5m 18s) Loss: 0.0169(0.0180) Grad: 13686.8945  LR: 0.00001536  \n","Epoch: [2][2100/3278] Elapsed 8m 44s (remain 4m 53s) Loss: 0.0210(0.0180) Grad: 9912.0752  LR: 0.00001520  \n","Epoch: [2][2200/3278] Elapsed 9m 9s (remain 4m 28s) Loss: 0.0228(0.0180) Grad: 12382.0352  LR: 0.00001503  \n","Epoch: [2][2300/3278] Elapsed 9m 33s (remain 4m 3s) Loss: 0.0186(0.0180) Grad: 9291.2490  LR: 0.00001486  \n","Epoch: [2][2400/3278] Elapsed 9m 58s (remain 3m 38s) Loss: 0.0160(0.0179) Grad: 8861.5850  LR: 0.00001469  \n","Epoch: [2][2500/3278] Elapsed 10m 23s (remain 3m 13s) Loss: 0.0196(0.0179) Grad: 7029.9697  LR: 0.00001452  \n","Epoch: [2][2600/3278] Elapsed 10m 49s (remain 2m 49s) Loss: 0.0156(0.0179) Grad: 5819.1221  LR: 0.00001435  \n","Epoch: [2][2700/3278] Elapsed 11m 14s (remain 2m 24s) Loss: 0.0174(0.0179) Grad: 9057.2764  LR: 0.00001418  \n","Epoch: [2][2800/3278] Elapsed 11m 38s (remain 1m 59s) Loss: 0.0141(0.0179) Grad: 5137.5029  LR: 0.00001400  \n","Epoch: [2][2900/3278] Elapsed 12m 3s (remain 1m 34s) Loss: 0.0182(0.0179) Grad: 6759.9883  LR: 0.00001382  \n","Epoch: [2][3000/3278] Elapsed 12m 28s (remain 1m 9s) Loss: 0.0154(0.0179) Grad: 26965.5879  LR: 0.00001365  \n","Epoch: [2][3100/3278] Elapsed 12m 54s (remain 0m 44s) Loss: 0.0177(0.0179) Grad: 9317.0674  LR: 0.00001347  \n","Epoch: [2][3200/3278] Elapsed 13m 18s (remain 0m 19s) Loss: 0.0120(0.0178) Grad: 4378.9292  LR: 0.00001329  \n","Epoch: [2][3277/3278] Elapsed 13m 37s (remain 0m 0s) Loss: 0.0206(0.0179) Grad: 7468.5088  LR: 0.00001315  \n","EVAL: [0/370] Elapsed 0m 0s (remain 2m 3s) Loss: 0.0136(0.0136) \n","EVAL: [100/370] Elapsed 0m 10s (remain 0m 28s) Loss: 0.0231(0.0181) \n","EVAL: [200/370] Elapsed 0m 21s (remain 0m 17s) Loss: 0.0114(0.0188) \n","EVAL: [300/370] Elapsed 0m 31s (remain 0m 7s) Loss: 0.0272(0.0190) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0179  avg_val_loss: 0.0189  time: 857s\n","Epoch 2 - Score: 0.8187\n","Epoch 2 - Save Best Score: 0.8187 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [369/370] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0271(0.0189) \n","Epoch: [3][0/3278] Elapsed 0m 0s (remain 32m 52s) Loss: 0.0153(0.0153) Grad: 3386.6621  LR: 0.00001314  \n","Epoch: [3][100/3278] Elapsed 0m 26s (remain 14m 3s) Loss: 0.0204(0.0170) Grad: 8218.7441  LR: 0.00001296  \n","Epoch: [3][200/3278] Elapsed 0m 52s (remain 13m 18s) Loss: 0.0209(0.0169) Grad: 10003.3242  LR: 0.00001278  \n","Epoch: [3][300/3278] Elapsed 1m 16s (remain 12m 41s) Loss: 0.0146(0.0168) Grad: 26907.9316  LR: 0.00001259  \n","Epoch: [3][400/3278] Elapsed 1m 41s (remain 12m 9s) Loss: 0.0169(0.0168) Grad: 1784.8500  LR: 0.00001241  \n","Epoch: [3][500/3278] Elapsed 2m 6s (remain 11m 41s) Loss: 0.0161(0.0169) Grad: 4310.8706  LR: 0.00001222  \n","Epoch: [3][600/3278] Elapsed 2m 31s (remain 11m 14s) Loss: 0.0186(0.0169) Grad: 1291.6018  LR: 0.00001203  \n","Epoch: [3][700/3278] Elapsed 2m 56s (remain 10m 47s) Loss: 0.0151(0.0169) Grad: 7659.2539  LR: 0.00001184  \n","Epoch: [3][800/3278] Elapsed 3m 21s (remain 10m 21s) Loss: 0.0201(0.0170) Grad: 5932.2324  LR: 0.00001165  \n","Epoch: [3][900/3278] Elapsed 3m 45s (remain 9m 55s) Loss: 0.0168(0.0170) Grad: 4779.0493  LR: 0.00001146  \n","Epoch: [3][1000/3278] Elapsed 4m 10s (remain 9m 30s) Loss: 0.0110(0.0170) Grad: 3358.2952  LR: 0.00001127  \n","Epoch: [3][1100/3278] Elapsed 4m 36s (remain 9m 6s) Loss: 0.0123(0.0170) Grad: 6532.6411  LR: 0.00001108  \n","Epoch: [3][1200/3278] Elapsed 5m 1s (remain 8m 40s) Loss: 0.0183(0.0170) Grad: 4365.7681  LR: 0.00001089  \n","Epoch: [3][1300/3278] Elapsed 5m 26s (remain 8m 16s) Loss: 0.0169(0.0170) Grad: 2683.9426  LR: 0.00001070  \n","Epoch: [3][1400/3278] Elapsed 5m 51s (remain 7m 51s) Loss: 0.0129(0.0170) Grad: 2086.1807  LR: 0.00001051  \n","Epoch: [3][1500/3278] Elapsed 6m 16s (remain 7m 25s) Loss: 0.0177(0.0170) Grad: 2976.7273  LR: 0.00001031  \n","Epoch: [3][1600/3278] Elapsed 6m 41s (remain 7m 0s) Loss: 0.0194(0.0170) Grad: 3469.6401  LR: 0.00001012  \n","Epoch: [3][1700/3278] Elapsed 7m 6s (remain 6m 35s) Loss: 0.0174(0.0170) Grad: 8227.4951  LR: 0.00000993  \n","Epoch: [3][1800/3278] Elapsed 7m 31s (remain 6m 10s) Loss: 0.0202(0.0170) Grad: 6107.0830  LR: 0.00000974  \n","Epoch: [3][1900/3278] Elapsed 7m 56s (remain 5m 45s) Loss: 0.0190(0.0171) Grad: 2798.1072  LR: 0.00000955  \n","Epoch: [3][2000/3278] Elapsed 8m 21s (remain 5m 20s) Loss: 0.0152(0.0171) Grad: 6508.6777  LR: 0.00000935  \n","Epoch: [3][2100/3278] Elapsed 8m 47s (remain 4m 55s) Loss: 0.0203(0.0171) Grad: 4075.9734  LR: 0.00000916  \n","Epoch: [3][2200/3278] Elapsed 9m 11s (remain 4m 30s) Loss: 0.0197(0.0171) Grad: 13746.1426  LR: 0.00000897  \n","Epoch: [3][2300/3278] Elapsed 9m 36s (remain 4m 4s) Loss: 0.0216(0.0170) Grad: 8983.5859  LR: 0.00000878  \n","Epoch: [3][2400/3278] Elapsed 10m 2s (remain 3m 39s) Loss: 0.0185(0.0170) Grad: 8794.2568  LR: 0.00000859  \n","Epoch: [3][2500/3278] Elapsed 10m 27s (remain 3m 14s) Loss: 0.0213(0.0170) Grad: 20489.9199  LR: 0.00000840  \n","Epoch: [3][2600/3278] Elapsed 10m 52s (remain 2m 49s) Loss: 0.0179(0.0171) Grad: 2731.1262  LR: 0.00000821  \n","Epoch: [3][2700/3278] Elapsed 11m 17s (remain 2m 24s) Loss: 0.0163(0.0171) Grad: 10706.1064  LR: 0.00000802  \n","Epoch: [3][2800/3278] Elapsed 11m 42s (remain 1m 59s) Loss: 0.0139(0.0171) Grad: 5903.7627  LR: 0.00000783  \n","Epoch: [3][2900/3278] Elapsed 12m 6s (remain 1m 34s) Loss: 0.0097(0.0171) Grad: 12387.0439  LR: 0.00000765  \n","Epoch: [3][3000/3278] Elapsed 12m 31s (remain 1m 9s) Loss: 0.0170(0.0171) Grad: 4328.9629  LR: 0.00000746  \n","Epoch: [3][3100/3278] Elapsed 12m 56s (remain 0m 44s) Loss: 0.0173(0.0171) Grad: 2507.3828  LR: 0.00000727  \n","Epoch: [3][3200/3278] Elapsed 13m 20s (remain 0m 19s) Loss: 0.0195(0.0171) Grad: 8055.4595  LR: 0.00000709  \n","Epoch: [3][3277/3278] Elapsed 13m 39s (remain 0m 0s) Loss: 0.0208(0.0171) Grad: 13760.5029  LR: 0.00000695  \n","EVAL: [0/370] Elapsed 0m 0s (remain 2m 4s) Loss: 0.0151(0.0151) \n","EVAL: [100/370] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0229(0.0188) \n","EVAL: [200/370] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0107(0.0195) \n","EVAL: [300/370] Elapsed 0m 32s (remain 0m 7s) Loss: 0.0403(0.0201) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0171  avg_val_loss: 0.0200  time: 859s\n","Epoch 3 - Score: 0.8174\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [369/370] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0263(0.0200) \n","Epoch: [4][0/3278] Elapsed 0m 0s (remain 31m 47s) Loss: 0.0163(0.0163) Grad: 3559.0730  LR: 0.00000695  \n","Epoch: [4][100/3278] Elapsed 0m 25s (remain 13m 21s) Loss: 0.0158(0.0166) Grad: 2268.7795  LR: 0.00000676  \n","Epoch: [4][200/3278] Elapsed 0m 50s (remain 12m 51s) Loss: 0.0219(0.0167) Grad: 12625.1611  LR: 0.00000658  \n","Epoch: [4][300/3278] Elapsed 1m 15s (remain 12m 23s) Loss: 0.0170(0.0167) Grad: 1673.4009  LR: 0.00000640  \n","Epoch: [4][400/3278] Elapsed 1m 39s (remain 11m 57s) Loss: 0.0159(0.0167) Grad: 4865.4819  LR: 0.00000622  \n","Epoch: [4][500/3278] Elapsed 2m 4s (remain 11m 32s) Loss: 0.0165(0.0167) Grad: 2853.6543  LR: 0.00000605  \n","Epoch: [4][600/3278] Elapsed 2m 29s (remain 11m 6s) Loss: 0.0157(0.0167) Grad: 3036.8479  LR: 0.00000587  \n","Epoch: [4][700/3278] Elapsed 2m 54s (remain 10m 41s) Loss: 0.0215(0.0167) Grad: 4021.5952  LR: 0.00000570  \n","Epoch: [4][800/3278] Elapsed 3m 19s (remain 10m 17s) Loss: 0.0196(0.0167) Grad: 3300.0940  LR: 0.00000552  \n","Epoch: [4][900/3278] Elapsed 3m 44s (remain 9m 52s) Loss: 0.0184(0.0167) Grad: 3586.7117  LR: 0.00000535  \n","Epoch: [4][1000/3278] Elapsed 4m 9s (remain 9m 27s) Loss: 0.0108(0.0167) Grad: 1805.1188  LR: 0.00000518  \n","Epoch: [4][1100/3278] Elapsed 4m 34s (remain 9m 2s) Loss: 0.0171(0.0168) Grad: 3989.1643  LR: 0.00000501  \n","Epoch: [4][1200/3278] Elapsed 4m 59s (remain 8m 37s) Loss: 0.0163(0.0167) Grad: 2625.5884  LR: 0.00000485  \n","Epoch: [4][1300/3278] Elapsed 5m 25s (remain 8m 13s) Loss: 0.0229(0.0167) Grad: 11645.6328  LR: 0.00000469  \n","Epoch: [4][1400/3278] Elapsed 5m 50s (remain 7m 49s) Loss: 0.0165(0.0168) Grad: 2928.5791  LR: 0.00000452  \n","Epoch: [4][1500/3278] Elapsed 6m 15s (remain 7m 25s) Loss: 0.0209(0.0168) Grad: 18127.8574  LR: 0.00000436  \n","Epoch: [4][1600/3278] Elapsed 6m 41s (remain 7m 0s) Loss: 0.0121(0.0168) Grad: 4680.2080  LR: 0.00000421  \n","Epoch: [4][1700/3278] Elapsed 7m 6s (remain 6m 35s) Loss: 0.0176(0.0168) Grad: 9386.9092  LR: 0.00000405  \n","Epoch: [4][1800/3278] Elapsed 7m 31s (remain 6m 10s) Loss: 0.0136(0.0168) Grad: 3629.9407  LR: 0.00000390  \n","Epoch: [4][1900/3278] Elapsed 7m 56s (remain 5m 45s) Loss: 0.0224(0.0168) Grad: 3967.2722  LR: 0.00000375  \n","Epoch: [4][2000/3278] Elapsed 8m 21s (remain 5m 20s) Loss: 0.0120(0.0168) Grad: 1878.3263  LR: 0.00000360  \n","Epoch: [4][2100/3278] Elapsed 8m 46s (remain 4m 55s) Loss: 0.0105(0.0168) Grad: 9726.7588  LR: 0.00000345  \n","Epoch: [4][2200/3278] Elapsed 9m 12s (remain 4m 30s) Loss: 0.0129(0.0168) Grad: 2695.5791  LR: 0.00000331  \n","Epoch: [4][2300/3278] Elapsed 9m 36s (remain 4m 4s) Loss: 0.0150(0.0168) Grad: 24455.3359  LR: 0.00000316  \n","Epoch: [4][2400/3278] Elapsed 10m 1s (remain 3m 39s) Loss: 0.0171(0.0167) Grad: 5025.1606  LR: 0.00000303  \n","Epoch: [4][2500/3278] Elapsed 10m 26s (remain 3m 14s) Loss: 0.0146(0.0167) Grad: 4862.6411  LR: 0.00000289  \n","Epoch: [4][2600/3278] Elapsed 10m 51s (remain 2m 49s) Loss: 0.0188(0.0167) Grad: 3501.7815  LR: 0.00000276  \n","Epoch: [4][2700/3278] Elapsed 11m 16s (remain 2m 24s) Loss: 0.0176(0.0167) Grad: 8001.8955  LR: 0.00000262  \n","Epoch: [4][2800/3278] Elapsed 11m 41s (remain 1m 59s) Loss: 0.0183(0.0167) Grad: 20402.6367  LR: 0.00000250  \n","Epoch: [4][2900/3278] Elapsed 12m 6s (remain 1m 34s) Loss: 0.0167(0.0166) Grad: 10720.6514  LR: 0.00000237  \n","Epoch: [4][3000/3278] Elapsed 12m 31s (remain 1m 9s) Loss: 0.0124(0.0166) Grad: 1631.4517  LR: 0.00000225  \n","Epoch: [4][3100/3278] Elapsed 12m 56s (remain 0m 44s) Loss: 0.0195(0.0166) Grad: 10006.5391  LR: 0.00000213  \n","Epoch: [4][3200/3278] Elapsed 13m 21s (remain 0m 19s) Loss: 0.0188(0.0167) Grad: 11520.8691  LR: 0.00000201  \n","Epoch: [4][3277/3278] Elapsed 13m 40s (remain 0m 0s) Loss: 0.0190(0.0167) Grad: 7694.2319  LR: 0.00000192  \n","EVAL: [0/370] Elapsed 0m 0s (remain 2m 7s) Loss: 0.0131(0.0131) \n","EVAL: [100/370] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0231(0.0187) \n","EVAL: [200/370] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0104(0.0197) \n","EVAL: [300/370] Elapsed 0m 32s (remain 0m 7s) Loss: 0.0398(0.0202) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0167  avg_val_loss: 0.0201  time: 860s\n","Epoch 4 - Score: 0.8176\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [369/370] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0260(0.0201) \n","Epoch: [5][0/3278] Elapsed 0m 0s (remain 31m 36s) Loss: 0.0141(0.0141) Grad: 1487.3282  LR: 0.00000192  \n","Epoch: [5][100/3278] Elapsed 0m 25s (remain 13m 23s) Loss: 0.0184(0.0168) Grad: 2990.0039  LR: 0.00000181  \n","Epoch: [5][200/3278] Elapsed 0m 50s (remain 12m 49s) Loss: 0.0153(0.0167) Grad: 1979.4355  LR: 0.00000170  \n","Epoch: [5][300/3278] Elapsed 1m 15s (remain 12m 22s) Loss: 0.0145(0.0165) Grad: 2146.7129  LR: 0.00000159  \n","Epoch: [5][400/3278] Elapsed 1m 39s (remain 11m 56s) Loss: 0.0171(0.0165) Grad: 2857.3462  LR: 0.00000149  \n","Epoch: [5][500/3278] Elapsed 2m 4s (remain 11m 31s) Loss: 0.0159(0.0165) Grad: 2911.5505  LR: 0.00000139  \n","Epoch: [5][600/3278] Elapsed 2m 29s (remain 11m 6s) Loss: 0.0175(0.0165) Grad: 4582.5576  LR: 0.00000130  \n","Epoch: [5][700/3278] Elapsed 2m 54s (remain 10m 41s) Loss: 0.0176(0.0165) Grad: 3379.5081  LR: 0.00000120  \n","Epoch: [5][800/3278] Elapsed 3m 19s (remain 10m 16s) Loss: 0.0184(0.0164) Grad: 1585.2008  LR: 0.00000111  \n","Epoch: [5][900/3278] Elapsed 3m 44s (remain 9m 52s) Loss: 0.0119(0.0164) Grad: 1179.9414  LR: 0.00000103  \n","Epoch: [5][1000/3278] Elapsed 4m 9s (remain 9m 27s) Loss: 0.0195(0.0164) Grad: 13901.7959  LR: 0.00000094  \n","Epoch: [5][1100/3278] Elapsed 4m 34s (remain 9m 2s) Loss: 0.0141(0.0164) Grad: 4588.0146  LR: 0.00000086  \n","Epoch: [5][1200/3278] Elapsed 4m 59s (remain 8m 37s) Loss: 0.0157(0.0164) Grad: 2633.0537  LR: 0.00000079  \n","Epoch: [5][1300/3278] Elapsed 5m 24s (remain 8m 12s) Loss: 0.0166(0.0164) Grad: 4161.1919  LR: 0.00000071  \n","Epoch: [5][1400/3278] Elapsed 5m 49s (remain 7m 47s) Loss: 0.0076(0.0164) Grad: 2066.9609  LR: 0.00000064  \n","Epoch: [5][1500/3278] Elapsed 6m 14s (remain 7m 22s) Loss: 0.0186(0.0164) Grad: 10422.9229  LR: 0.00000058  \n","Epoch: [5][1600/3278] Elapsed 6m 38s (remain 6m 57s) Loss: 0.0153(0.0164) Grad: 7018.9868  LR: 0.00000052  \n","Epoch: [5][1700/3278] Elapsed 7m 3s (remain 6m 32s) Loss: 0.0192(0.0164) Grad: 5698.1460  LR: 0.00000046  \n","Epoch: [5][1800/3278] Elapsed 7m 28s (remain 6m 7s) Loss: 0.0131(0.0164) Grad: 4572.5229  LR: 0.00000040  \n","Epoch: [5][1900/3278] Elapsed 7m 53s (remain 5m 42s) Loss: 0.0155(0.0164) Grad: 5331.2158  LR: 0.00000035  \n","Epoch: [5][2000/3278] Elapsed 8m 17s (remain 5m 17s) Loss: 0.0174(0.0164) Grad: 3481.1455  LR: 0.00000030  \n","Epoch: [5][2100/3278] Elapsed 8m 41s (remain 4m 52s) Loss: 0.0207(0.0164) Grad: 8567.7422  LR: 0.00000026  \n","Epoch: [5][2200/3278] Elapsed 9m 6s (remain 4m 27s) Loss: 0.0191(0.0164) Grad: 8602.9658  LR: 0.00000021  \n","Epoch: [5][2300/3278] Elapsed 9m 30s (remain 4m 2s) Loss: 0.0132(0.0164) Grad: 2052.9836  LR: 0.00000018  \n","Epoch: [5][2400/3278] Elapsed 9m 55s (remain 3m 37s) Loss: 0.0179(0.0164) Grad: 17607.8496  LR: 0.00000014  \n","Epoch: [5][2500/3278] Elapsed 10m 20s (remain 3m 12s) Loss: 0.0192(0.0164) Grad: 26355.1719  LR: 0.00000011  \n","Epoch: [5][2600/3278] Elapsed 10m 45s (remain 2m 48s) Loss: 0.0180(0.0164) Grad: 4221.7373  LR: 0.00000008  \n","Epoch: [5][2700/3278] Elapsed 11m 10s (remain 2m 23s) Loss: 0.0154(0.0164) Grad: 4480.2915  LR: 0.00000006  \n","Epoch: [5][2800/3278] Elapsed 11m 35s (remain 1m 58s) Loss: 0.0187(0.0164) Grad: 7827.7349  LR: 0.00000004  \n","Epoch: [5][2900/3278] Elapsed 11m 59s (remain 1m 33s) Loss: 0.0144(0.0164) Grad: 6095.1484  LR: 0.00000003  \n","Epoch: [5][3000/3278] Elapsed 12m 24s (remain 1m 8s) Loss: 0.0157(0.0164) Grad: 16180.3857  LR: 0.00000001  \n","Epoch: [5][3100/3278] Elapsed 12m 49s (remain 0m 43s) Loss: 0.0223(0.0164) Grad: 10503.5469  LR: 0.00000001  \n","Epoch: [5][3200/3278] Elapsed 13m 14s (remain 0m 19s) Loss: 0.0133(0.0164) Grad: 3513.4949  LR: 0.00000000  \n","Epoch: [5][3277/3278] Elapsed 13m 34s (remain 0m 0s) Loss: 0.0185(0.0164) Grad: 4271.8809  LR: 0.00000000  \n","EVAL: [0/370] Elapsed 0m 0s (remain 2m 6s) Loss: 0.0132(0.0132) \n","EVAL: [100/370] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0232(0.0193) \n","EVAL: [200/370] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0102(0.0203) \n","EVAL: [300/370] Elapsed 0m 32s (remain 0m 7s) Loss: 0.0382(0.0209) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0164  avg_val_loss: 0.0207  time: 854s\n","Epoch 5 - Score: 0.8156\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [369/370] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0261(0.0207) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 3 result ==========\n","Score: 0.8187\n","========== fold: 4 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: [1][0/3268] Elapsed 0m 0s (remain 29m 45s) Loss: 0.0236(0.0236) Grad: 8014.5410  LR: 0.00000040  \n","Epoch: [1][100/3268] Elapsed 0m 26s (remain 13m 41s) Loss: 0.0244(0.0248) Grad: 12622.5527  LR: 0.00002000  \n","Epoch: [1][200/3268] Elapsed 0m 51s (remain 13m 5s) Loss: 0.0186(0.0233) Grad: 10227.2412  LR: 0.00002000  \n","Epoch: [1][300/3268] Elapsed 1m 16s (remain 12m 35s) Loss: 0.0216(0.0227) Grad: 14990.6318  LR: 0.00001999  \n","Epoch: [1][400/3268] Elapsed 1m 41s (remain 12m 9s) Loss: 0.0226(0.0222) Grad: 15362.9541  LR: 0.00001998  \n","Epoch: [1][500/3268] Elapsed 2m 7s (remain 11m 44s) Loss: 0.0237(0.0217) Grad: 21556.3965  LR: 0.00001996  \n","Epoch: [1][600/3268] Elapsed 2m 32s (remain 11m 17s) Loss: 0.0227(0.0214) Grad: 10497.5117  LR: 0.00001994  \n","Epoch: [1][700/3268] Elapsed 2m 58s (remain 10m 52s) Loss: 0.0178(0.0210) Grad: 13688.1357  LR: 0.00001992  \n","Epoch: [1][800/3268] Elapsed 3m 23s (remain 10m 27s) Loss: 0.0157(0.0208) Grad: 12080.8447  LR: 0.00001990  \n","Epoch: [1][900/3268] Elapsed 3m 48s (remain 10m 1s) Loss: 0.0196(0.0206) Grad: 16238.1816  LR: 0.00001987  \n","Epoch: [1][1000/3268] Elapsed 4m 13s (remain 9m 34s) Loss: 0.0187(0.0204) Grad: 4452.6973  LR: 0.00001983  \n","Epoch: [1][1100/3268] Elapsed 4m 38s (remain 9m 7s) Loss: 0.0187(0.0203) Grad: 7034.5391  LR: 0.00001980  \n","Epoch: [1][1200/3268] Elapsed 5m 3s (remain 8m 41s) Loss: 0.0165(0.0202) Grad: 3859.2222  LR: 0.00001975  \n","Epoch: [1][1300/3268] Elapsed 5m 28s (remain 8m 16s) Loss: 0.0195(0.0202) Grad: 6646.9482  LR: 0.00001971  \n","Epoch: [1][1400/3268] Elapsed 5m 53s (remain 7m 50s) Loss: 0.0235(0.0201) Grad: 7013.1172  LR: 0.00001966  \n","Epoch: [1][1500/3268] Elapsed 6m 17s (remain 7m 24s) Loss: 0.0200(0.0201) Grad: 13519.3330  LR: 0.00001961  \n","Epoch: [1][1600/3268] Elapsed 6m 42s (remain 6m 58s) Loss: 0.0131(0.0200) Grad: 7480.2920  LR: 0.00001956  \n","Epoch: [1][1700/3268] Elapsed 7m 6s (remain 6m 32s) Loss: 0.0231(0.0199) Grad: 10154.1455  LR: 0.00001950  \n","Epoch: [1][1800/3268] Elapsed 7m 31s (remain 6m 7s) Loss: 0.0171(0.0198) Grad: 6586.4229  LR: 0.00001944  \n","Epoch: [1][1900/3268] Elapsed 7m 55s (remain 5m 42s) Loss: 0.0190(0.0198) Grad: 1942.3141  LR: 0.00001937  \n","Epoch: [1][2000/3268] Elapsed 8m 20s (remain 5m 16s) Loss: 0.0191(0.0197) Grad: 9100.6475  LR: 0.00001930  \n","Epoch: [1][2100/3268] Elapsed 8m 44s (remain 4m 51s) Loss: 0.0160(0.0197) Grad: 8991.0830  LR: 0.00001923  \n","Epoch: [1][2200/3268] Elapsed 9m 8s (remain 4m 26s) Loss: 0.0176(0.0196) Grad: 10337.0449  LR: 0.00001915  \n","Epoch: [1][2300/3268] Elapsed 9m 33s (remain 4m 0s) Loss: 0.0315(0.0196) Grad: 69969.2969  LR: 0.00001907  \n","Epoch: [1][2400/3268] Elapsed 9m 57s (remain 3m 35s) Loss: 0.0202(0.0195) Grad: 6147.8145  LR: 0.00001899  \n","Epoch: [1][2500/3268] Elapsed 10m 22s (remain 3m 10s) Loss: 0.0184(0.0195) Grad: 12746.3164  LR: 0.00001890  \n","Epoch: [1][2600/3268] Elapsed 10m 46s (remain 2m 45s) Loss: 0.0189(0.0194) Grad: 13990.4355  LR: 0.00001881  \n","Epoch: [1][2700/3268] Elapsed 11m 11s (remain 2m 20s) Loss: 0.0137(0.0194) Grad: 15780.5273  LR: 0.00001872  \n","Epoch: [1][2800/3268] Elapsed 11m 35s (remain 1m 55s) Loss: 0.0188(0.0194) Grad: 8626.6865  LR: 0.00001863  \n","Epoch: [1][2900/3268] Elapsed 11m 59s (remain 1m 31s) Loss: 0.0195(0.0193) Grad: 18569.2812  LR: 0.00001853  \n","Epoch: [1][3000/3268] Elapsed 12m 24s (remain 1m 6s) Loss: 0.0168(0.0193) Grad: 13519.2910  LR: 0.00001842  \n","Epoch: [1][3100/3268] Elapsed 12m 48s (remain 0m 41s) Loss: 0.0143(0.0193) Grad: 11225.2939  LR: 0.00001832  \n","Epoch: [1][3200/3268] Elapsed 13m 12s (remain 0m 16s) Loss: 0.0147(0.0192) Grad: 3816.4180  LR: 0.00001821  \n","Epoch: [1][3267/3268] Elapsed 13m 28s (remain 0m 0s) Loss: 0.0189(0.0192) Grad: 3922.2258  LR: 0.00001814  \n","EVAL: [0/379] Elapsed 0m 0s (remain 2m 10s) Loss: 0.0194(0.0194) \n","EVAL: [100/379] Elapsed 0m 10s (remain 0m 30s) Loss: 0.0215(0.0188) \n","EVAL: [200/379] Elapsed 0m 21s (remain 0m 19s) Loss: 0.0231(0.0192) \n","EVAL: [300/379] Elapsed 0m 32s (remain 0m 8s) Loss: 0.0221(0.0189) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.0192  avg_val_loss: 0.0191  time: 849s\n","Epoch 1 - Score: 0.8227\n","Epoch 1 - Save Best Score: 0.8227 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [378/379] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0356(0.0191) \n","Epoch: [2][0/3268] Elapsed 0m 0s (remain 41m 32s) Loss: 0.0187(0.0187) Grad: 8812.9355  LR: 0.00001813  \n","Epoch: [2][100/3268] Elapsed 0m 26s (remain 13m 49s) Loss: 0.0155(0.0182) Grad: 3876.1360  LR: 0.00001802  \n","Epoch: [2][200/3268] Elapsed 0m 51s (remain 13m 3s) Loss: 0.0148(0.0182) Grad: 4617.2363  LR: 0.00001790  \n","Epoch: [2][300/3268] Elapsed 1m 15s (remain 12m 22s) Loss: 0.0204(0.0181) Grad: 12728.3672  LR: 0.00001778  \n","Epoch: [2][400/3268] Elapsed 1m 39s (remain 11m 50s) Loss: 0.0216(0.0181) Grad: 5143.4819  LR: 0.00001766  \n","Epoch: [2][500/3268] Elapsed 2m 3s (remain 11m 22s) Loss: 0.0237(0.0181) Grad: 14374.2246  LR: 0.00001754  \n","Epoch: [2][600/3268] Elapsed 2m 27s (remain 10m 55s) Loss: 0.0133(0.0180) Grad: 3937.3889  LR: 0.00001741  \n","Epoch: [2][700/3268] Elapsed 2m 51s (remain 10m 28s) Loss: 0.0233(0.0180) Grad: 4379.1357  LR: 0.00001728  \n","Epoch: [2][800/3268] Elapsed 3m 16s (remain 10m 4s) Loss: 0.0190(0.0180) Grad: 11180.0322  LR: 0.00001714  \n","Epoch: [2][900/3268] Elapsed 3m 41s (remain 9m 40s) Loss: 0.0186(0.0180) Grad: 8645.9443  LR: 0.00001701  \n","Epoch: [2][1000/3268] Elapsed 4m 5s (remain 9m 15s) Loss: 0.0187(0.0180) Grad: 5717.3267  LR: 0.00001687  \n","Epoch: [2][1100/3268] Elapsed 4m 29s (remain 8m 49s) Loss: 0.0151(0.0180) Grad: 3696.2515  LR: 0.00001673  \n","Epoch: [2][1200/3268] Elapsed 4m 53s (remain 8m 24s) Loss: 0.0189(0.0180) Grad: 6859.7676  LR: 0.00001658  \n","Epoch: [2][1300/3268] Elapsed 5m 18s (remain 8m 1s) Loss: 0.0175(0.0180) Grad: 2756.9094  LR: 0.00001644  \n","Epoch: [2][1400/3268] Elapsed 5m 42s (remain 7m 36s) Loss: 0.0161(0.0179) Grad: 5973.3062  LR: 0.00001629  \n","Epoch: [2][1500/3268] Elapsed 6m 6s (remain 7m 11s) Loss: 0.0188(0.0179) Grad: 4387.0420  LR: 0.00001614  \n","Epoch: [2][1600/3268] Elapsed 6m 31s (remain 6m 47s) Loss: 0.0177(0.0179) Grad: 7944.7871  LR: 0.00001598  \n","Epoch: [2][1700/3268] Elapsed 6m 55s (remain 6m 22s) Loss: 0.0153(0.0179) Grad: 3955.1199  LR: 0.00001583  \n","Epoch: [2][1800/3268] Elapsed 7m 19s (remain 5m 58s) Loss: 0.0183(0.0179) Grad: 3736.1350  LR: 0.00001567  \n","Epoch: [2][1900/3268] Elapsed 7m 44s (remain 5m 34s) Loss: 0.0154(0.0179) Grad: 2312.7483  LR: 0.00001551  \n","Epoch: [2][2000/3268] Elapsed 8m 9s (remain 5m 9s) Loss: 0.0161(0.0180) Grad: 7310.4883  LR: 0.00001535  \n","Epoch: [2][2100/3268] Elapsed 8m 33s (remain 4m 45s) Loss: 0.0240(0.0179) Grad: 5260.0117  LR: 0.00001519  \n","Epoch: [2][2200/3268] Elapsed 8m 58s (remain 4m 21s) Loss: 0.0203(0.0179) Grad: 4742.1191  LR: 0.00001502  \n","Epoch: [2][2300/3268] Elapsed 9m 23s (remain 3m 56s) Loss: 0.0172(0.0179) Grad: 10079.5674  LR: 0.00001485  \n","Epoch: [2][2400/3268] Elapsed 9m 48s (remain 3m 32s) Loss: 0.0202(0.0179) Grad: 8135.4351  LR: 0.00001468  \n","Epoch: [2][2500/3268] Elapsed 10m 12s (remain 3m 7s) Loss: 0.0190(0.0179) Grad: 15018.3818  LR: 0.00001451  \n","Epoch: [2][2600/3268] Elapsed 10m 37s (remain 2m 43s) Loss: 0.0198(0.0178) Grad: 124945.2188  LR: 0.00001434  \n","Epoch: [2][2700/3268] Elapsed 11m 2s (remain 2m 19s) Loss: 0.0176(0.0178) Grad: 9572.8076  LR: 0.00001416  \n","Epoch: [2][2800/3268] Elapsed 11m 27s (remain 1m 54s) Loss: 0.0159(0.0178) Grad: 12391.4756  LR: 0.00001399  \n","Epoch: [2][2900/3268] Elapsed 11m 52s (remain 1m 30s) Loss: 0.0161(0.0178) Grad: 7091.5103  LR: 0.00001381  \n","Epoch: [2][3000/3268] Elapsed 12m 16s (remain 1m 5s) Loss: 0.0210(0.0178) Grad: 5678.2427  LR: 0.00001363  \n","Epoch: [2][3100/3268] Elapsed 12m 41s (remain 0m 41s) Loss: 0.0192(0.0178) Grad: 17624.2656  LR: 0.00001345  \n","Epoch: [2][3200/3268] Elapsed 13m 6s (remain 0m 16s) Loss: 0.0242(0.0178) Grad: 2936.3308  LR: 0.00001327  \n","Epoch: [2][3267/3268] Elapsed 13m 22s (remain 0m 0s) Loss: 0.0150(0.0178) Grad: 8968.8193  LR: 0.00001315  \n","EVAL: [0/379] Elapsed 0m 0s (remain 2m 15s) Loss: 0.0206(0.0206) \n","EVAL: [100/379] Elapsed 0m 11s (remain 0m 30s) Loss: 0.0237(0.0192) \n","EVAL: [200/379] Elapsed 0m 21s (remain 0m 19s) Loss: 0.0214(0.0204) \n","EVAL: [300/379] Elapsed 0m 32s (remain 0m 8s) Loss: 0.0217(0.0200) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.0178  avg_val_loss: 0.0201  time: 843s\n","Epoch 2 - Score: 0.8253\n","Epoch 2 - Save Best Score: 0.8253 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [378/379] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0405(0.0201) \n","Epoch: [3][0/3268] Elapsed 0m 0s (remain 31m 59s) Loss: 0.0192(0.0192) Grad: 2537.7939  LR: 0.00001315  \n","Epoch: [3][100/3268] Elapsed 0m 26s (remain 13m 40s) Loss: 0.0153(0.0171) Grad: 3985.1084  LR: 0.00001296  \n","Epoch: [3][200/3268] Elapsed 0m 51s (remain 13m 3s) Loss: 0.0177(0.0173) Grad: 4396.3608  LR: 0.00001278  \n","Epoch: [3][300/3268] Elapsed 1m 16s (remain 12m 30s) Loss: 0.0145(0.0172) Grad: 4746.3428  LR: 0.00001259  \n","Epoch: [3][400/3268] Elapsed 1m 40s (remain 12m 1s) Loss: 0.0167(0.0171) Grad: 4748.4307  LR: 0.00001240  \n","Epoch: [3][500/3268] Elapsed 2m 5s (remain 11m 32s) Loss: 0.0219(0.0172) Grad: 7187.2119  LR: 0.00001222  \n","Epoch: [3][600/3268] Elapsed 2m 29s (remain 11m 5s) Loss: 0.0169(0.0173) Grad: 2247.3699  LR: 0.00001203  \n","Epoch: [3][700/3268] Elapsed 2m 54s (remain 10m 39s) Loss: 0.0193(0.0172) Grad: 10063.5068  LR: 0.00001184  \n","Epoch: [3][800/3268] Elapsed 3m 19s (remain 10m 13s) Loss: 0.0205(0.0173) Grad: 6092.3950  LR: 0.00001165  \n","Epoch: [3][900/3268] Elapsed 3m 43s (remain 9m 47s) Loss: 0.0201(0.0173) Grad: 7168.8950  LR: 0.00001146  \n","Epoch: [3][1000/3268] Elapsed 4m 7s (remain 9m 21s) Loss: 0.0144(0.0172) Grad: 3840.5479  LR: 0.00001127  \n","Epoch: [3][1100/3268] Elapsed 4m 32s (remain 8m 56s) Loss: 0.0180(0.0172) Grad: 4799.6558  LR: 0.00001108  \n","Epoch: [3][1200/3268] Elapsed 4m 56s (remain 8m 30s) Loss: 0.0169(0.0172) Grad: 7692.3774  LR: 0.00001088  \n","Epoch: [3][1300/3268] Elapsed 5m 21s (remain 8m 6s) Loss: 0.0175(0.0172) Grad: 2319.4363  LR: 0.00001069  \n","Epoch: [3][1400/3268] Elapsed 5m 45s (remain 7m 40s) Loss: 0.0197(0.0172) Grad: 3000.3408  LR: 0.00001050  \n","Epoch: [3][1500/3268] Elapsed 6m 10s (remain 7m 16s) Loss: 0.0079(0.0172) Grad: 3721.7773  LR: 0.00001031  \n","Epoch: [3][1600/3268] Elapsed 6m 34s (remain 6m 51s) Loss: 0.0196(0.0172) Grad: 1943.1514  LR: 0.00001011  \n","Epoch: [3][1700/3268] Elapsed 6m 58s (remain 6m 25s) Loss: 0.0102(0.0171) Grad: 4528.6392  LR: 0.00000992  \n","Epoch: [3][1800/3268] Elapsed 7m 22s (remain 6m 0s) Loss: 0.0211(0.0171) Grad: 1792.2002  LR: 0.00000973  \n","Epoch: [3][1900/3268] Elapsed 7m 47s (remain 5m 36s) Loss: 0.0222(0.0171) Grad: 5910.0249  LR: 0.00000954  \n","Epoch: [3][2000/3268] Elapsed 8m 11s (remain 5m 11s) Loss: 0.0174(0.0171) Grad: 4126.1021  LR: 0.00000934  \n","Epoch: [3][2100/3268] Elapsed 8m 35s (remain 4m 46s) Loss: 0.0150(0.0171) Grad: 6532.4229  LR: 0.00000915  \n","Epoch: [3][2200/3268] Elapsed 9m 0s (remain 4m 22s) Loss: 0.0188(0.0171) Grad: 26850.1816  LR: 0.00000896  \n","Epoch: [3][2300/3268] Elapsed 9m 25s (remain 3m 57s) Loss: 0.0171(0.0171) Grad: 12269.2842  LR: 0.00000877  \n","Epoch: [3][2400/3268] Elapsed 9m 49s (remain 3m 32s) Loss: 0.0165(0.0171) Grad: 7024.4487  LR: 0.00000858  \n","Epoch: [3][2500/3268] Elapsed 10m 13s (remain 3m 8s) Loss: 0.0168(0.0171) Grad: 3941.5923  LR: 0.00000839  \n","Epoch: [3][2600/3268] Elapsed 10m 37s (remain 2m 43s) Loss: 0.0187(0.0171) Grad: 8766.2100  LR: 0.00000820  \n","Epoch: [3][2700/3268] Elapsed 11m 1s (remain 2m 18s) Loss: 0.0157(0.0171) Grad: 4733.7036  LR: 0.00000801  \n","Epoch: [3][2800/3268] Elapsed 11m 25s (remain 1m 54s) Loss: 0.0170(0.0170) Grad: 5023.5503  LR: 0.00000782  \n","Epoch: [3][2900/3268] Elapsed 11m 49s (remain 1m 29s) Loss: 0.0122(0.0170) Grad: 10586.2363  LR: 0.00000763  \n","Epoch: [3][3000/3268] Elapsed 12m 12s (remain 1m 5s) Loss: 0.0174(0.0170) Grad: 15704.7676  LR: 0.00000744  \n","Epoch: [3][3100/3268] Elapsed 12m 37s (remain 0m 40s) Loss: 0.0167(0.0170) Grad: 9383.0977  LR: 0.00000726  \n","Epoch: [3][3200/3268] Elapsed 13m 1s (remain 0m 16s) Loss: 0.0205(0.0170) Grad: 16916.9766  LR: 0.00000707  \n","Epoch: [3][3267/3268] Elapsed 13m 17s (remain 0m 0s) Loss: 0.0145(0.0170) Grad: 7255.4238  LR: 0.00000695  \n","EVAL: [0/379] Elapsed 0m 0s (remain 2m 8s) Loss: 0.0199(0.0199) \n","EVAL: [100/379] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0217(0.0189) \n","EVAL: [200/379] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0225(0.0206) \n","EVAL: [300/379] Elapsed 0m 31s (remain 0m 8s) Loss: 0.0241(0.0201) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.0170  avg_val_loss: 0.0203  time: 837s\n","Epoch 3 - Score: 0.8273\n","Epoch 3 - Save Best Score: 0.8273 Model\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [378/379] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0424(0.0203) \n","Epoch: [4][0/3268] Elapsed 0m 0s (remain 32m 11s) Loss: 0.0162(0.0162) Grad: 1468.2745  LR: 0.00000695  \n","Epoch: [4][100/3268] Elapsed 0m 25s (remain 13m 8s) Loss: 0.0250(0.0168) Grad: 5214.3228  LR: 0.00000676  \n","Epoch: [4][200/3268] Elapsed 0m 49s (remain 12m 39s) Loss: 0.0186(0.0167) Grad: 2392.3616  LR: 0.00000658  \n","Epoch: [4][300/3268] Elapsed 1m 13s (remain 12m 6s) Loss: 0.0126(0.0166) Grad: 3514.0481  LR: 0.00000640  \n","Epoch: [4][400/3268] Elapsed 1m 37s (remain 11m 37s) Loss: 0.0159(0.0165) Grad: 4968.2681  LR: 0.00000622  \n","Epoch: [4][500/3268] Elapsed 2m 2s (remain 11m 15s) Loss: 0.0172(0.0166) Grad: 2546.2056  LR: 0.00000605  \n","Epoch: [4][600/3268] Elapsed 2m 26s (remain 10m 51s) Loss: 0.0199(0.0165) Grad: 3249.6807  LR: 0.00000587  \n","Epoch: [4][700/3268] Elapsed 2m 51s (remain 10m 26s) Loss: 0.0203(0.0165) Grad: 4491.2402  LR: 0.00000569  \n","Epoch: [4][800/3268] Elapsed 3m 16s (remain 10m 3s) Loss: 0.0154(0.0165) Grad: 1658.5394  LR: 0.00000552  \n","Epoch: [4][900/3268] Elapsed 3m 41s (remain 9m 41s) Loss: 0.0148(0.0165) Grad: 3703.5337  LR: 0.00000535  \n","Epoch: [4][1000/3268] Elapsed 4m 5s (remain 9m 16s) Loss: 0.0208(0.0165) Grad: 2454.8472  LR: 0.00000518  \n","Epoch: [4][1100/3268] Elapsed 4m 30s (remain 8m 52s) Loss: 0.0157(0.0165) Grad: 3259.1833  LR: 0.00000501  \n","Epoch: [4][1200/3268] Elapsed 4m 55s (remain 8m 27s) Loss: 0.0172(0.0166) Grad: 3605.3420  LR: 0.00000485  \n","Epoch: [4][1300/3268] Elapsed 5m 19s (remain 8m 3s) Loss: 0.0215(0.0165) Grad: 8934.9570  LR: 0.00000468  \n","Epoch: [4][1400/3268] Elapsed 5m 44s (remain 7m 39s) Loss: 0.0110(0.0165) Grad: 4465.6133  LR: 0.00000452  \n","Epoch: [4][1500/3268] Elapsed 6m 9s (remain 7m 15s) Loss: 0.0148(0.0165) Grad: 3163.7314  LR: 0.00000436  \n","Epoch: [4][1600/3268] Elapsed 6m 34s (remain 6m 50s) Loss: 0.0192(0.0166) Grad: 5178.6313  LR: 0.00000420  \n","Epoch: [4][1700/3268] Elapsed 6m 58s (remain 6m 25s) Loss: 0.0193(0.0166) Grad: 8370.6641  LR: 0.00000404  \n","Epoch: [4][1800/3268] Elapsed 7m 24s (remain 6m 1s) Loss: 0.0182(0.0166) Grad: 828.0897  LR: 0.00000389  \n","Epoch: [4][1900/3268] Elapsed 7m 49s (remain 5m 37s) Loss: 0.0155(0.0166) Grad: 2897.6172  LR: 0.00000374  \n","Epoch: [4][2000/3268] Elapsed 8m 14s (remain 5m 12s) Loss: 0.0171(0.0166) Grad: 4624.7114  LR: 0.00000359  \n","Epoch: [4][2100/3268] Elapsed 8m 38s (remain 4m 48s) Loss: 0.0120(0.0166) Grad: 4592.0889  LR: 0.00000344  \n","Epoch: [4][2200/3268] Elapsed 9m 3s (remain 4m 23s) Loss: 0.0189(0.0166) Grad: 20273.6582  LR: 0.00000330  \n","Epoch: [4][2300/3268] Elapsed 9m 28s (remain 3m 58s) Loss: 0.0136(0.0166) Grad: 3507.8623  LR: 0.00000316  \n","Epoch: [4][2400/3268] Elapsed 9m 53s (remain 3m 34s) Loss: 0.0160(0.0166) Grad: 18253.5117  LR: 0.00000302  \n","Epoch: [4][2500/3268] Elapsed 10m 17s (remain 3m 9s) Loss: 0.0201(0.0166) Grad: 5001.8955  LR: 0.00000288  \n","Epoch: [4][2600/3268] Elapsed 10m 42s (remain 2m 44s) Loss: 0.0152(0.0166) Grad: 5825.7612  LR: 0.00000275  \n","Epoch: [4][2700/3268] Elapsed 11m 6s (remain 2m 19s) Loss: 0.0179(0.0166) Grad: 12534.4678  LR: 0.00000262  \n","Epoch: [4][2800/3268] Elapsed 11m 31s (remain 1m 55s) Loss: 0.0173(0.0166) Grad: 5033.8945  LR: 0.00000249  \n","Epoch: [4][2900/3268] Elapsed 11m 56s (remain 1m 30s) Loss: 0.0200(0.0166) Grad: 12403.9189  LR: 0.00000236  \n","Epoch: [4][3000/3268] Elapsed 12m 21s (remain 1m 5s) Loss: 0.0176(0.0166) Grad: 9479.3438  LR: 0.00000224  \n","Epoch: [4][3100/3268] Elapsed 12m 45s (remain 0m 41s) Loss: 0.0187(0.0166) Grad: 4846.3652  LR: 0.00000212  \n","Epoch: [4][3200/3268] Elapsed 13m 9s (remain 0m 16s) Loss: 0.0175(0.0166) Grad: 8423.2441  LR: 0.00000200  \n","Epoch: [4][3267/3268] Elapsed 13m 26s (remain 0m 0s) Loss: 0.0153(0.0166) Grad: 9929.3018  LR: 0.00000192  \n","EVAL: [0/379] Elapsed 0m 0s (remain 2m 13s) Loss: 0.0206(0.0206) \n","EVAL: [100/379] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0213(0.0189) \n","EVAL: [200/379] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0228(0.0205) \n","EVAL: [300/379] Elapsed 0m 32s (remain 0m 8s) Loss: 0.0240(0.0201) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.0166  avg_val_loss: 0.0203  time: 847s\n","Epoch 4 - Score: 0.8244\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [378/379] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0473(0.0203) \n","Epoch: [5][0/3268] Elapsed 0m 0s (remain 32m 2s) Loss: 0.0136(0.0136) Grad: 2334.4336  LR: 0.00000192  \n","Epoch: [5][100/3268] Elapsed 0m 25s (remain 13m 8s) Loss: 0.0170(0.0167) Grad: 1600.6018  LR: 0.00000181  \n","Epoch: [5][200/3268] Elapsed 0m 49s (remain 12m 38s) Loss: 0.0183(0.0160) Grad: 1159.0171  LR: 0.00000170  \n","Epoch: [5][300/3268] Elapsed 1m 14s (remain 12m 16s) Loss: 0.0186(0.0161) Grad: 5539.7583  LR: 0.00000160  \n","Epoch: [5][400/3268] Elapsed 1m 40s (remain 11m 55s) Loss: 0.0187(0.0162) Grad: 8967.8760  LR: 0.00000149  \n","Epoch: [5][500/3268] Elapsed 2m 4s (remain 11m 29s) Loss: 0.0157(0.0164) Grad: 1301.4779  LR: 0.00000139  \n","Epoch: [5][600/3268] Elapsed 2m 29s (remain 11m 3s) Loss: 0.0238(0.0164) Grad: 3652.0967  LR: 0.00000130  \n","Epoch: [5][700/3268] Elapsed 2m 54s (remain 10m 37s) Loss: 0.0160(0.0163) Grad: 2278.4287  LR: 0.00000120  \n","Epoch: [5][800/3268] Elapsed 3m 19s (remain 10m 13s) Loss: 0.0185(0.0163) Grad: 4236.1880  LR: 0.00000111  \n","Epoch: [5][900/3268] Elapsed 3m 44s (remain 9m 49s) Loss: 0.0186(0.0163) Grad: 1622.6503  LR: 0.00000103  \n","Epoch: [5][1000/3268] Elapsed 4m 9s (remain 9m 24s) Loss: 0.0172(0.0163) Grad: 286.3727  LR: 0.00000094  \n","Epoch: [5][1100/3268] Elapsed 4m 33s (remain 8m 58s) Loss: 0.0143(0.0163) Grad: 5330.7148  LR: 0.00000086  \n","Epoch: [5][1200/3268] Elapsed 4m 58s (remain 8m 34s) Loss: 0.0141(0.0163) Grad: 3408.7300  LR: 0.00000079  \n","Epoch: [5][1300/3268] Elapsed 5m 23s (remain 8m 9s) Loss: 0.0108(0.0163) Grad: 1757.5831  LR: 0.00000071  \n","Epoch: [5][1400/3268] Elapsed 5m 49s (remain 7m 45s) Loss: 0.0214(0.0163) Grad: 14803.0527  LR: 0.00000064  \n","Epoch: [5][1500/3268] Elapsed 6m 14s (remain 7m 20s) Loss: 0.0112(0.0163) Grad: 1073.1521  LR: 0.00000058  \n","Epoch: [5][1600/3268] Elapsed 6m 38s (remain 6m 55s) Loss: 0.0195(0.0163) Grad: 6578.1982  LR: 0.00000051  \n","Epoch: [5][1700/3268] Elapsed 7m 3s (remain 6m 30s) Loss: 0.0119(0.0163) Grad: 4147.0610  LR: 0.00000045  \n","Epoch: [5][1800/3268] Elapsed 7m 28s (remain 6m 5s) Loss: 0.0191(0.0163) Grad: 5536.3252  LR: 0.00000040  \n","Epoch: [5][1900/3268] Elapsed 7m 53s (remain 5m 40s) Loss: 0.0194(0.0163) Grad: 1113.6437  LR: 0.00000035  \n","Epoch: [5][2000/3268] Elapsed 8m 18s (remain 5m 15s) Loss: 0.0202(0.0163) Grad: 11403.4893  LR: 0.00000030  \n","Epoch: [5][2100/3268] Elapsed 8m 42s (remain 4m 50s) Loss: 0.0122(0.0163) Grad: 1964.3995  LR: 0.00000025  \n","Epoch: [5][2200/3268] Elapsed 9m 6s (remain 4m 25s) Loss: 0.0185(0.0163) Grad: 12967.2725  LR: 0.00000021  \n","Epoch: [5][2300/3268] Elapsed 9m 31s (remain 4m 0s) Loss: 0.0199(0.0163) Grad: 1222.0691  LR: 0.00000017  \n","Epoch: [5][2400/3268] Elapsed 9m 55s (remain 3m 34s) Loss: 0.0104(0.0163) Grad: 9164.3730  LR: 0.00000014  \n","Epoch: [5][2500/3268] Elapsed 10m 19s (remain 3m 9s) Loss: 0.0155(0.0163) Grad: 6350.4033  LR: 0.00000011  \n","Epoch: [5][2600/3268] Elapsed 10m 43s (remain 2m 45s) Loss: 0.0146(0.0163) Grad: 11280.4951  LR: 0.00000008  \n","Epoch: [5][2700/3268] Elapsed 11m 7s (remain 2m 20s) Loss: 0.0160(0.0163) Grad: 2242.5686  LR: 0.00000006  \n","Epoch: [5][2800/3268] Elapsed 11m 31s (remain 1m 55s) Loss: 0.0199(0.0164) Grad: 4743.2002  LR: 0.00000004  \n","Epoch: [5][2900/3268] Elapsed 11m 55s (remain 1m 30s) Loss: 0.0234(0.0164) Grad: 19605.5859  LR: 0.00000003  \n","Epoch: [5][3000/3268] Elapsed 12m 20s (remain 1m 5s) Loss: 0.0138(0.0164) Grad: 1908.8313  LR: 0.00000001  \n","Epoch: [5][3100/3268] Elapsed 12m 44s (remain 0m 41s) Loss: 0.0144(0.0164) Grad: 3484.1890  LR: 0.00000001  \n","Epoch: [5][3200/3268] Elapsed 13m 8s (remain 0m 16s) Loss: 0.0158(0.0164) Grad: 1916.9091  LR: 0.00000000  \n","Epoch: [5][3267/3268] Elapsed 13m 24s (remain 0m 0s) Loss: 0.0206(0.0164) Grad: 16956.2910  LR: 0.00000000  \n","EVAL: [0/379] Elapsed 0m 0s (remain 2m 9s) Loss: 0.0208(0.0208) \n","EVAL: [100/379] Elapsed 0m 10s (remain 0m 29s) Loss: 0.0214(0.0191) \n","EVAL: [200/379] Elapsed 0m 21s (remain 0m 18s) Loss: 0.0241(0.0209) \n","EVAL: [300/379] Elapsed 0m 31s (remain 0m 8s) Loss: 0.0245(0.0204) \n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 - avg_train_loss: 0.0164  avg_val_loss: 0.0206  time: 845s\n","Epoch 5 - Score: 0.8243\n"]},{"name":"stdout","output_type":"stream","text":["EVAL: [378/379] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0505(0.0206) \n"]},{"name":"stderr","output_type":"stream","text":["========== fold: 4 result ==========\n","Score: 0.8273\n","========== CV ==========\n","Score: 0.8260\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"369c8e5b28ab47f2ae3e1969442290e6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▅▃▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▂▁▅▅█</td></tr><tr><td>[fold0] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold0] loss</td><td>▆▄▅▅▂▃▆▇▅▅▆▅▃▇▃▆▇▇█▃▅▂█▄▅▃▃▆▅▄▅▄▄▁▆▇▄▁▅▃</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▁▄█▇▆</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▄▃▂▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▁▂▄▇█</td></tr><tr><td>[fold1] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold1] loss</td><td>▅█▃▄▄▄▃▄▆▃▄▅▅▄▄▇▃▄▆▂▄▅▄▅▅▁▅▄▄▄▅▄▄▂▄▁▄▅▂▃</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>▁▆█▆▆</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▄▃▂▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▁▇▅██</td></tr><tr><td>[fold2] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold2] loss</td><td>▄▇█▅▆▇▄▆▅█▃▅▄▇▅▆▅▂▅▆▅▅▇▄▂▁▃▃▅▆█▄▆▆▅▇▆▄▅▃</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>▆▂█▁▂</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▄▃▂▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>▂▁▅▅█</td></tr><tr><td>[fold3] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold3] loss</td><td>▆▃▇▄▇▅█▇▃▄█▅▅▄▄▇▆▃▄▇▅▅▆▄▄▇▅▆▇▅▁▄▄▄▆▅▅▄▄▄</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>▁█▅▆▂</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▅▃▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▁▆▆▆█</td></tr><tr><td>[fold4] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold4] loss</td><td>▆▅█▅▇▆▆▆▆▅▃▃▆▇▃▄▅▅▄▃▆▄▅█▆▁▄▅▁▆▅▃▃▃▄▆▆▇▅▂</td></tr><tr><td>[fold4] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold4] score</td><td>▁▅█▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.01639</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.02035</td></tr><tr><td>[fold0] epoch</td><td>5</td></tr><tr><td>[fold0] loss</td><td>0.01824</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.82269</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.01646</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.0198</td></tr><tr><td>[fold1] epoch</td><td>5</td></tr><tr><td>[fold1] loss</td><td>0.01353</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.82901</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.01639</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.02035</td></tr><tr><td>[fold2] epoch</td><td>5</td></tr><tr><td>[fold2] loss</td><td>0.0213</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.83065</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.01641</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.02072</td></tr><tr><td>[fold3] epoch</td><td>5</td></tr><tr><td>[fold3] loss</td><td>0.0185</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.81559</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.01636</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.02061</td></tr><tr><td>[fold4] epoch</td><td>5</td></tr><tr><td>[fold4] loss</td><td>0.02064</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] score</td><td>0.8243</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">microsoft/deberta-v3-large</strong>: <a href=\"https://wandb.ai/bluehills/PPPM-focal_loss/runs/2wfm1kp2\" target=\"_blank\">https://wandb.ai/bluehills/PPPM-focal_loss/runs/2wfm1kp2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220509_145116-2wfm1kp2/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["if __name__ == '__main__':\n","    \n","    def get_result(oof_df):\n","        labels = oof_df['score'].values\n","        preds = oof_df['pred'].values\n","        score = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:<.4f}')\n","    \n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                _oof_df = train_loop(train, fold)\n","                oof_df = pd.concat([oof_df, _oof_df])\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        oof_df = oof_df.reset_index(drop=True)\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n","        \n","    if CFG.wandb:\n","        wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tL7g_rhfN1sr"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"PPPM training with focal loss.ipynb","provenance":[],"authorship_tag":"ABX9TyPwus5dzLpebWi9EAqrhsDe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0988def69b634add88eff82a97a5363a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35eb7c4af6024a379cd5dc0d9759b284","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d01362ab47974359ad7734e2194e65a6","value":52}},"35eb7c4af6024a379cd5dc0d9759b284":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e8f1beb29f4e3d9b939b37f7416eed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5523755f4e834f218fbe84e837b84ac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee8b843344a94acdadc3795d3a2b7daf","IPY_MODEL_0988def69b634add88eff82a97a5363a","IPY_MODEL_6c81baee294343449d6f2b78a28f3f36"],"layout":"IPY_MODEL_a10fb752a0444556a965d9949cea130f"}},"63c3036f107e463b892869fd87ba6136":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c81baee294343449d6f2b78a28f3f36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb251b5019294289a8f1e3534edc47ad","placeholder":"​","style":"IPY_MODEL_ad7fe65a41fe486c971a6859e83db9df","value":" 52.0/52.0 [00:00&lt;00:00, 1.90kB/s]"}},"a10fb752a0444556a965d9949cea130f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad7fe65a41fe486c971a6859e83db9df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d01362ab47974359ad7734e2194e65a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee8b843344a94acdadc3795d3a2b7daf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63c3036f107e463b892869fd87ba6136","placeholder":"​","style":"IPY_MODEL_37e8f1beb29f4e3d9b939b37f7416eed","value":"Downloading: 100%"}},"fb251b5019294289a8f1e3534edc47ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}